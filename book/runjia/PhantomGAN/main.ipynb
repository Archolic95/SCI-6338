{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Target model definitions that adverserial examples are attempting to 'fool':\n",
    "\n",
    "ref: https://arxiv.org/pdf/1801.02610.pdf\n",
    "\n",
    "Source Code on attack perfomance on MINST Challenge and Cifar10:\n",
    "\n",
    "https://github.com/ctargon/AdvGAN-tf\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os, sys\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from generator import generator\n",
    "from discriminator import discriminator\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import cifar100\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "import pickle\n",
    "#from target_models import Target as target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target:\n",
    "    def __init__(self, lr=0.001, epochs=20, n_input=32, n_classes = 20, batch_size = 64,restore =0):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.n_input = n_input\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.restore = restore\n",
    "\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "    # randomly shuffle a dataset \n",
    "    def shuffle(self, X, Y):\n",
    "        rands = random.sample(range(X.shape[0]),X.shape[0])\n",
    "        return X[rands], Y[rands]\n",
    "\n",
    "    # get the next batch based on x, y, and the iteration (based on batch_size)\n",
    "    def next_batch(self, X, Y, i, batch_size):\n",
    "        idx = i * batch_size\n",
    "        idx_n = i * batch_size + batch_size\n",
    "        return X[idx:idx_n], Y[idx:idx_n]\n",
    "    \n",
    "    \n",
    "\n",
    "    # USAGE:\n",
    "    # - encoder network for vae\n",
    "    # PARAMS:\n",
    "    #x: input data sample\n",
    "    #h_hidden: LIST of num. neurons per hidden layer\n",
    "    def ModelC(self, x):\n",
    "        with tf.variable_scope('ModelC', reuse=tf.AUTO_REUSE):\n",
    "            #input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "            conv1 = tf.layers.Conv2D(\n",
    "                                inputs=x,\n",
    "                                filters=32,\n",
    "                                kernel_size=3,\n",
    "                                padding=\"same\",\n",
    "                                activation=tf.nn.relu)\n",
    "            \n",
    "            conv2 = tf.layers.Conv2D(\n",
    "                                inputs=conv1,\n",
    "                                filters=32,\n",
    "                                kernel_size=3,\n",
    "                                padding=\"same\",\n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "            conv3 = tf.layers.Conv2D(\n",
    "                                inputs=pool1,\n",
    "                                filters=64,\n",
    "                                kernel_size=3,\n",
    "                                padding=\"same\",\n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "            conv4 = tf.layers.Conv2D(\n",
    "                                inputs=conv3,\n",
    "                                filters=64,\n",
    "                                kernel_size=3,\n",
    "                                padding=\"same\",\n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "\n",
    "            pool2_flatten = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "            fc1 = tf.layers.dense(inputs=pool2_flatten, units=200, activation=tf.nn.relu)\n",
    "\n",
    "            fc2 = tf.layers.dense(inputs=fc1, units=200, activation=tf.nn.relu)\n",
    "\n",
    "            logits = tf.layers.dense(inputs=fc2, units=self.n_classes, activation=None)\n",
    "\n",
    "            probs = tf.nn.softmax(logits)\n",
    "\n",
    "            return logits, probs\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, Y, X_test, Y_test):\n",
    "        # define placeholders for input data\n",
    "        x = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]])\n",
    "        y = tf.placeholder(tf.float32, [None, self.n_classes])\n",
    "\n",
    "        # define compute graph\n",
    "        logits, _ = self.ModelC(x)\n",
    "\n",
    "        # define cost\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(cost)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        total_batch = int(X.shape[0] / self.batch_size)\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            avg_cost = 0.\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = self.next_batch(X, Y, i, self.batch_size)\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "                avg_cost += c / total_batch\n",
    "\n",
    "            print(\"Epoch:\", '%04d' % (epoch), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        accs = []\n",
    "\n",
    "        total_test_batch = int(X_test.shape[0] / self.batch_size)\n",
    "        for i in range(total_test_batch):\n",
    "            batch_x, batch_y = self.next_batch(X_test, Y_test, i, self.batch_size)\n",
    "            #batch_x = dataset.train.permute(batch_x, idxs)\n",
    "            accs.append(accuracy.eval({x: batch_x, y: batch_y}, session=sess))\n",
    "\n",
    "        print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
    "\n",
    "        saver.save(sess, \"./weights/target_model/model.ckpt\")\n",
    "        sess.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
      "169009152/169001437 [==============================] - 18s 0us/step\n",
      "WARNING:tensorflow:From <ipython-input-2-5eea9d7484ca>:39: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From E:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-2-5eea9d7484ca>:48: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Development_Components\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002216AA32908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002216AA32908>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002216AA32908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002216AA32908>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From <ipython-input-2-5eea9d7484ca>:68: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216E2677B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216E2677B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216E2677B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216E2677B8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216A714128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002216A714128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch: 0001 cost= 2.280498375\n",
      "Epoch: 0002 cost= 1.806261430\n",
      "Epoch: 0003 cost= 1.550376623\n",
      "Epoch: 0004 cost= 1.345803491\n",
      "Epoch: 0005 cost= 1.188000354\n",
      "Epoch: 0006 cost= 1.047539537\n",
      "Epoch: 0007 cost= 0.930110429\n",
      "Epoch: 0008 cost= 0.809871260\n",
      "Epoch: 0009 cost= 0.714320168\n",
      "Epoch: 0010 cost= 0.639876491\n",
      "Epoch: 0011 cost= 0.562229088\n",
      "Epoch: 0012 cost= 0.494857576\n",
      "Epoch: 0013 cost= 0.426911887\n",
      "Epoch: 0014 cost= 0.392769039\n",
      "Epoch: 0015 cost= 0.337231469\n",
      "Epoch: 0016 cost= 0.310490681\n",
      "Epoch: 0017 cost= 0.280972122\n",
      "Epoch: 0018 cost= 0.257783506\n",
      "Epoch: 0019 cost= 0.226621521\n",
      "Epoch: 0020 cost= 0.214531852\n",
      "accuracy of test set: 0.47325721153846156\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "num_classes=20\n",
    "batch_size = 128\n",
    "restore = 0\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "(X,y), (X_test,y_test) = cifar100.load_data(label_mode = 'coarse')#cifar10.load_data()\n",
    "X = np.divide(X, 255.0)\n",
    "X_test = np.divide(X_test, 255.0)\n",
    "X = X.reshape(X.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "y = to_categorical(y, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "cnn = Target(epochs = 20)\n",
    "cnn.train(X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle a dataset \n",
    "def shuffle(X, Y):\n",
    "    rands = random.sample(range(X.shape[0]),X.shape[0])\n",
    "    return X[rands], Y[rands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the next batch based on x, y, and the iteration (based on batch_size)\n",
    "def next_batch(X, Y, i, batch_size):\n",
    "    idx = i * batch_size\n",
    "    idx_n = i * batch_size + batch_size\n",
    "    return X[idx:idx_n], Y[idx:idx_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_loss(preds, labels, is_targeted):\n",
    "    real = tf.reduce_sum(labels * preds, 1)\n",
    "    other = tf.reduce_max((1 - labels) * preds - (labels * 10000), 1)\n",
    "    if is_targeted:\n",
    "        return tf.reduce_sum(tf.maximum(0.0, other - real))\n",
    "    return tf.reduce_sum(tf.maximum(0.0, real - other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function to influence the perturbation to be as close to 0 as possible\n",
    "def perturb_loss(preds, thresh=0.3):\n",
    "    zeros = tf.zeros((tf.shape(preds)[0]))\n",
    "    return tf.reduce_mean(tf.maximum(zeros, tf.norm(tf.reshape(preds, (tf.shape(preds)[0], -1)), axis=1) - thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that defines ops, graphs, and training procedure for AdvGAN framework\n",
    "def AdvGAN(X, y, X_test, y_test, n_classes, epochs, batch_size, target=-1):\n",
    "    people = []\n",
    "    # placeholder definitions\n",
    "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
    "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "    #-----------------------------------------------------------------------------------\n",
    "    # MODEL DEFINITIONS\n",
    "    is_targeted = False\n",
    "    if target in range(0, y.shape[-1]):\n",
    "        is_targeted = True\n",
    "\n",
    "    # gather target model\n",
    "    f = Target(n_classes, batch_size,restore = 0) #target_model()\n",
    "\n",
    "    thresh = 0.3\n",
    "\n",
    "    # generate perturbation, add to original input image(s)\n",
    "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
    "    x_perturbed = perturb + x_pl\n",
    "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
    "\n",
    "    # pass real and perturbed image to discriminator and the target model\n",
    "    d_real_logits, d_real_probs = discriminator(x_pl, is_training)\n",
    "    d_fake_logits, d_fake_probs = discriminator(x_perturbed, is_training)\n",
    "\n",
    "    # pass real and perturbed images to the model we are trying to fool\n",
    "    f_real_logits, f_real_probs = f.ModelC(x_pl)\n",
    "    f_fake_logits, f_fake_probs = f.ModelC(x_perturbed)\n",
    "\n",
    "\n",
    "    # generate labels for discriminator (optionally smooth labels for stability)\n",
    "    smooth = 0.0\n",
    "    d_labels_real = tf.ones_like(d_real_probs) * (1 - smooth)\n",
    "    d_labels_fake = tf.zeros_like(d_fake_probs)\n",
    "\n",
    "    #-----------------------------------------------------------------------------------\n",
    "    # LOSS DEFINITIONS\n",
    "    # discriminator loss\n",
    "    d_loss_real = tf.losses.mean_squared_error(predictions=d_real_probs, labels=d_labels_real)\n",
    "    d_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=d_labels_fake)\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    # generator loss\n",
    "    g_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=tf.ones_like(d_fake_probs))\n",
    "\n",
    "    # perturbation loss (minimize overall perturbation)\n",
    "    l_perturb = perturb_loss(perturb, thresh)\n",
    "\n",
    "    # adversarial loss (encourage misclassification)\n",
    "    l_adv = adv_loss(f_fake_probs, t, is_targeted)\n",
    "\n",
    "    # weights for generator loss function\n",
    "    alpha = 1.0\n",
    "    beta = 5.0\n",
    "    g_loss = l_adv + alpha*g_loss_fake + beta*l_perturb \n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # gather variables for training/restoring\n",
    "    t_vars = tf.trainable_variables()\n",
    "    f_vars = [var for var in t_vars if 'ModelC' in var.name]\n",
    "    d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
    "\n",
    "    # define optimizers for discriminator and generator\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        d_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    # create saver objects for the target model, generator, and discriminator\n",
    "    saver = tf.train.Saver(f_vars)\n",
    "    g_saver = tf.train.Saver(g_vars)\n",
    "    d_saver = tf.train.Saver(d_vars)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # load the pretrained target model\n",
    "    try:\n",
    "        saver.restore(sess, \"./weights/target_model/model.ckpt\")\n",
    "    except:\n",
    "        print(\"make sure to train the target model first...\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    total_batches = int(X.shape[0] / batch_size)\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "        loss_D_sum = 0.0\n",
    "        loss_G_fake_sum = 0.0\n",
    "        loss_perturb_sum = 0.0\n",
    "        loss_adv_sum = 0.0\n",
    "\n",
    "        for i in range(total_batches):\n",
    "\n",
    "            batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
    "\n",
    "            # if targeted, create one hot vectors of the target\n",
    "            if is_targeted:\n",
    "                targets = np.full((batch_y.shape[0],), target)\n",
    "                batch_y = np.eye(y.shape[-1])[targets]\n",
    "\n",
    "            # train the discriminator first n times\n",
    "            for _ in range(1):\n",
    "                _, loss_D_batch = sess.run([d_opt, d_loss], feed_dict={x_pl: batch_x, \\\n",
    "                                                                       is_training: True})\n",
    "\n",
    "            # train the generator n times\n",
    "            for _ in range(1):\n",
    "                _, loss_G_fake_batch, loss_adv_batch, loss_perturb_batch = \\\n",
    "                                    sess.run([g_opt, g_loss_fake, l_adv, l_perturb], \\\n",
    "                                                feed_dict={x_pl: batch_x, \\\n",
    "                                                           t: batch_y, \\\n",
    "                                                           is_training: True})\n",
    "            loss_D_sum += loss_D_batch\n",
    "            loss_G_fake_sum += loss_G_fake_batch\n",
    "            loss_perturb_sum += loss_perturb_batch\n",
    "            loss_adv_sum += loss_adv_batch\n",
    "\n",
    "        #person = x_pert[31]\n",
    "        #people.append(person)\n",
    "        print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f, \\\n",
    "                \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
    "                (epoch + 1, loss_D_sum/total_batches, loss_G_fake_sum/total_batches,\n",
    "                loss_perturb_sum/total_batches, loss_adv_sum/total_batches))\n",
    "\n",
    "        #if epoch % 10 == 0:\n",
    "        g_saver.save(sess, \"weights/generator/gen.ckpt\")\n",
    "        d_saver.save(sess, \"weights/discriminator/disc.ckpt\")\n",
    "\n",
    "    # evaluate the test set\n",
    "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accs = []\n",
    "    total_batches_test = int(X_test.shape[0] / batch_size)\n",
    "    for i in range(total_batches_test):\n",
    "        batch_x, batch_y = next_batch(X_test, y_test, i, batch_size)\n",
    "        acc, x_pert = sess.run([accuracy, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
    "        accs.append(acc)\n",
    "\n",
    "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
    "\n",
    "    # plot some images and their perturbed counterparts\n",
    "    f, axarr = plt.subplots(2,2)\n",
    "    axarr[0,0].imshow(np.squeeze(batch_x[2]), cmap='Greys_r')\n",
    "    axarr[0,1].imshow(np.squeeze(x_pert[2]), cmap='Greys_r')\n",
    "    axarr[1,0].imshow(np.squeeze(batch_x[5]), cmap='Greys_r')\n",
    "    axarr[1,1].imshow(np.squeeze(x_pert[5]), cmap='Greys_r')\n",
    "    plt.show()\n",
    "\n",
    "    print('finished training, saving weights')\n",
    "    g_saver.save(sess, \"weights/generator/gen.ckpt\")\n",
    "    d_saver.save(sess, \"weights/discriminator/disc.ckpt\")\n",
    "    return batch_x, x_pert, g_saver, d_saver#, people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315539860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315539860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315539860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315539860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3155861D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3155861D0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3155861D0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3155861D0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315578780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315578780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315578780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315578780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F31594DF60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F31594DF60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F31594DF60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F31594DF60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F31594DF60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F31594DF60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F31594DF60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F31594DF60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F310832320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F310832320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F310832320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F310832320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F310832320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F310832320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F310832320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F310832320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F310832320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F310832320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F310832320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F310832320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F310832320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F310832320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F310832320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F310832320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315593FD0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F3153FC940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F3153FC940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F3153FC940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F3153FC940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3153FC940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3153FC940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3153FC940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3153FC940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F3153FC940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F3153FC940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F3153FC940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F3153FC940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315FDEC50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315FDEC50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315FDEC50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F315FDEC50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315FDEC50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315FDEC50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315FDEC50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000001F315FDEC50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F315FDEC50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F315FDEC50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F315FDEC50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F315FDEC50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F315F91A90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F315F91A90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F315F91A90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F315F91A90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F316085320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F316085320>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F316085320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x000001F316085320>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F316388E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F3164454E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F3164454E0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F3164454E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F3164454E0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F3164454E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F3164454E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F3164454E0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F3164454E0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3161CE320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3161CE320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3161CE320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3161CE320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3161CE320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3161CE320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3161CE320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3161CE320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3154B67F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3154B67F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3154B67F0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001F3154B67F0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F3163AC898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F3163AC898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F3163AC898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000001F3163AC898>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F3163AC898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F3163AC898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F3163AC898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001F3163AC898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'inputs')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-33fe6030e646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpert_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_saver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_saver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdvGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'batch_fi'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-56a4aed6f6bd>\u001b[0m in \u001b[0;36mAdvGAN\u001b[1;34m(X, y, X_test, y_test, n_classes, epochs, batch_size, target)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# pass real and perturbed images to the model we are trying to fool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mf_real_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_real_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mf_fake_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_fake_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_perturbed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-68a6fd733ab5>\u001b[0m in \u001b[0;36mModelC\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m                                 \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                                 activation=tf.nn.relu)\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             conv2 = tf.layers.Conv2D(\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         name=name, **kwargs)\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0mkernel_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     super(Layer, self).__init__(trainable=trainable, name=name, dtype=dtype,\n\u001b[1;32m--> 203\u001b[1;33m                                 **kwargs)\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_in_keras_style_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     }\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# Validate optional keyword arguments.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;31m# Mutable properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[1;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[0;32m    598\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'inputs')"
     ]
    }
   ],
   "source": [
    "#train Generative Adversarial Network\n",
    "tf.reset_default_graph()\n",
    "# read in mnist data\n",
    "#(X,y), (X_test,y_test) = mnist.load_data()\n",
    "(X, y), (X_test, y_test) = cifar100.load_data(label_mode = 'coarse')\n",
    "X = np.divide(X, 255.0)\n",
    "X_test = np.divide(X_test, 255.0)\n",
    "X = X.reshape(X.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "y = to_categorical(y, num_classes=20)\n",
    "y_test = to_categorical(y_test, num_classes=20)\n",
    "\n",
    "for epoch in range(4,40):\n",
    "    batch_x, pert_x, gen_saver, d_saver = AdvGAN(X, y, X_test, y_test, 20,batch_size=128, epochs=5, target=-1)\n",
    "    with open('batch_fi'+str(i)+'.pkl','wb') as f:\n",
    "        pickle.dump(batch_x, f)\n",
    "    with open('pert_fi'+str(i)+'.pkl','wb') as f:\n",
    "        pickle.dump(pert_x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3374367b0267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdif_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpert_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdif_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0maxarr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Greys_r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_x' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAK8UAAAJDCAYAAAAe89iPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzXQejn953X8ffn1xiEgquwFbadiA7TTW1KDs508bjgobWH9iYJiGjFUkEP4mVP3a2nnKWirrB8UdDgLT0s8WRRxCVOWHbZCKWTRszMCpug7kVImvLdQ6ZxdjLJTL/5P/vbmTweMNCZ+c0/X5598+L7W/u+DwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPp9O5HwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA407nfgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAjjud+wEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOO507gcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4LjTuR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgONO534AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI473e8Da63fWGv9wVrr9z7g79da65+utW6stX53rfVXLv4xH136drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvg+f0wN8ZpuZL3/I3//1mfns7V/fmJl//tEf62NlG30r22hb2Ubb0jb6VrbRtrKNtpVttC1to29lG20r22hb2kbfyjbaVrbRtrSNvpVttK1so21lG21L2+hb2UbbyjbalrbRt7KNtpVttC1to29lG20r22hb2Ubb0jb6VrbRtrKNtqVt9K1so21lG21L2+hb2UbbyjbaVrbRtrSNvpVttK1so21pG30r22hb2Ubb0jb6VrbRtrKNtpVttC1to29lG20r22hb2kbfyjbaVrbRtrSNvpVttK1so21lG21L2+hb2UbbyjbalrbRt7KNtpVttC1to29lG20r22hb2kbfyjbaVrbRtrKNtqVt9K1so21lG21L2+hb2UbbyjbalrbRt7KNtpVttK1so21pG30r22hb2Ubb0jb6VrbRtrKNtqVt9K1so21lG20r22hb2kbfyjbaVrbRtrSNvpVttK1so21pG30r22hb2UbbyjbalrbRt7KNtpVttC1to29lG20r22hb2kbfyjbaVrbRtrKNtqVt9K1so21lG21L2+hb2UbbyjbalrbRt7KNtpVttK1so21pG30r22hb2Ubb0jb6VrbRtrKNtqVt9H2onO73gX3f/9PM/O8P+cjXZuZf7+/6rZn5s2utX7ioB3zU6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/o+fE4X8DM+MzOv3/H7m7f/jIuhb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6fsnzGMX8DPWPf5sv+cH1/rGzHxjZuaTn/zk1c997nMX8J9/+H3hC1+YGzduzLVr1/5Yt5dffvnNmXnpHv/kfX21vbeLaDuj771o27ILHbfb0bbzQW1ve3tmnrvrz7T9Kdjcjl3o2IWWXejYhY62LbvQcbsdbTvexVo2t2MXOnahZRc6dqGjbcsudNxuR9uOd7GW2+1o27ELLe9iHbvQ0bZlFzput6Ntx7tYy+12tO3YhZZ3sY5d6Gjbsgsdt9vRtuNdrOV2O9p27ELLu1jHLnS0bdmFjtvtaNvxLtZyux1tO3ah5V2sYxc62rbsQsftdrRt2YWO2+1o2/EdrWVzO3ahYxdadqFjFzratuxCx+12tO14F2vZ3I5d6NiFll3o2IWOti270HG7HW073sVaNrdjFzp2oWUXOnaho23LLnTcbkfbjnexls3t2IWOXWjZhY5d6Gjbsgsdt9vRtuNdrOV2O9p27ELLu1jHLnS0bdmFjtvtaNvxLtZyux1tO3ah5V2sYxc62rbsws/eyy+//Oa+75869I/3fb/vr5n5izPzex/wd/9yZp694/ffn5lfuN/PvHr16s67Xnvttf2pp55635/PzPUjfbX9/y667a7ve7Rt2YWO2+1o2/mgtvu+7zPzhrYfjc3t2IWOXWjZhY5d6Gjbsgsdt9vRtuNdrGVzO3ahYxdadqFjFzratuxCx+12tO14F2u53Y62HbvQ8i7WsQsdbVt2oeN2O9p2vIu13G5H245daHkX69iFjrYtu9Bxux1tO97FWm63o23HLrS8i3XsQkfbll3ouN2Oth3vYi2329G2Yxda3sU6dqGjbcsudNxuR9uWXei43Y62Hd/RWja3Yxc6dqFlFzp2oaNtyy503G5H2453sZbN7diFjl1o2YWOXeho27ILHbfb0bbjXaxlczt2oWMXWnahYxc62rbsQsftdrTteBdr2dyOXejYhZZd6NiFjrYtu9Bxux1tO97FWm63o23HLrS8i3XsQkfbll3ouN2Oth3vYi2329G2Yxda3sU6dqGjbcsu/OzNzPX9Pjf6Qb9O89F9d2b+1nrXX52ZP9z3/X9dwM/lXfp2tO1o29K3o21H287/HW1LbrejbccutNxuR9uOti19O9p2tO14F2u53Y62HbvQcrsdbTvatvTtaNvRtuNdrOV2O9p27ELL7Xa07Wjb0rejbUfbjnexltvtaNuxCy2329G2o21L3462HW073sVabrejbccutNxuR9uOti19O9p2tO14F2u53Y62HbvQcrsdbTvatvTtaNvRtqVvR9uOth3f0Vput6Ntxy603G5H2462LX072na07XgXa7ndjrYdu9Byux1tO9q29O1o29G2412s5XY72nbsQsvtdrTtaNvSt6NtR9uOd7GW2+1o27ELLbfb0bajbUvfjrYdbTvexVput6Ntxy603G5H2462LX072na07XgXa7ndjrYdu9Byux1tO9q29P0T5rH7fWCt9e9m5pdn5ufXWjdn5ldn5k/NzOz7/i9m5jdn5iszc2Nm/t/M/J3qYR9Fzz777Hzve9+bN998cy5dujTf/va350c/+tGdH9H3IG072rb07Wjb0bbzYW2/+c1vzsz84cz8cLQ9xO12tO3YhZbb7Wjb0balb0fbjrYd72Itt9vRtmMXWm63o21H25a+HW072na8i7Xcbkfbjl1oud2Oth1tW/p2tO1o2/Eu1nK7HW07dqHldjvadrRt6dvRtqNtx7tYy+12tO3YhZbb7Wjb0balb0fbjrYd72Itt9vRtmMXWm63o21H25a+HW072rb07Wjb0bbjO1rL7Xa07diFltvtaNvRtqVvR9uOth3vYi2329G2YxdabrejbUfblr4dbTvadryLtdxuR9uOXWi53Y62HW1b+na07Wjb8S7WcrsdbTt2oeV2O9p2tG3p29G2o23Hu1jL7Xa07diFltvtaNvRtqVvR9uOth3vYi2329G2YxdabrejbUfblr4Pn7Xv+1n+w9euXduvX79+lv/2w2Kt9fK+79d+2n+n7f0dbTuj7/1o27ILHbfb0bZlFzraduxCy+12tO3YhY62LbvQcbsdbVt2oaNtxy603G5H245d6Gjbsgsdt9vRtmUXOm63o23LLnS07diFjrYtu9Bxux1tW3ah43Y72rbsQkfbjl3oaNuyCx2329G2ZRc6brejbcsudLTt2IWOti270HG7HW1bdqHjdjvatuxCR9uOXeho27ILHbfb0bZlFzput6Ntyy50tO3YhZbb7WjbsQsdbVt2oeN2O9q27EJH245daLndjrYdu9DRtmUXOm63o23LLnS07diFltvtaNuxCx1tW3ah43Y72rbsQkfbjl1oud2Oth270NG2ZRc6brejbcsudNxuR9uWXeho27ELHW1bdqHjdjvatuxCx+12tG3ZhY62HbvQ0bZlFzof5XZPF/0wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPzsnM79AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcdzr3AwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABw3OncDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAcadzPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAx53O/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHHc69wMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcNzp3A8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwHGncz8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMedzv0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABx3OvcDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHDc6dwPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBxp3M/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADHnc79AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcdzr3AwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABw3OncDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAcadzPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAx53O/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHHc69wMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcNzp3A8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwHGncz8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMedzv0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABx3OvcDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHDc6UE+tNb68lrr+2utG2utX7nH3/+FtdZ/XGv99lrrd9daX7n4R300vfjii/Pkk0/OlStX5rnnnnvf32v70ejb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbR9C+75/6K+Z+cTMvDozl2fm8Zn5nZn5/F2f+fWZ+fu3//fnZ+Z/3O/nXr16df+4e+edd/bLly/vr7766v7WW2/tTz/99P7KK6+89/czc13b4z6s78xc393uYdp27EKnarvru++7XSjZhY62LbvQcbsdbVt2oaNtxy50fEdr2YWOXeho27ILHbfb0bZlFzraduxCx3e0ll3o2IWOXWjZhY5d6GjbsgsdbTt2oeNdrGUXOnahYxdadqFjFzratuxCR9uOXWi53Y62HbvQ8R2tZRc6dqGjbcsudLTt2IWW2+1o27ELHd/RWnahYxc62rbsQkfbjl1oud2Oth270PEdrWUXOnaho23LLnTcbkfbll3oaNuxCx3f0Vp2oWMXOtq27ELH7Xa0bdmFjrYdu9DxHa1lFzp2oaNtyy503G5H25Zd6GjbsQsd39FadqFjFzratuxCx+12tG3ZhY62HbvQ8R2tZRc6dqFjF1p2oWMXOtq27EJH245d6HgXa9mFjl3o2IWWXejYhY62LbvQ0bZjF87nJ7d75Ndp7u+XZubGvu8/3Pf97Zl5fma+dtdn9pn5M7f/98/NzO8/wM/92HvppZfmypUrc/ny5Xn88cfnmWeemRdeeOHuj2l7kL4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvR9uF0eoDPfGZmXr/j9zdv/9mdfm1m/uZa6+bM/ObM/MMLebpH3K1bt+aJJ5547/eXLl2aW7du3f2xXxttD9G3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072j6cTg/wmXWPP9vv+v2zM7Pt+35pZr4yM/9mrfW+n73W+sZa6/pa6/obb7zx0z/tI2bf7844s9b7cmt7kL4dbTvadi6y7e1/q+8d3G5H2462LX072na0benb0bajbcd3tJbb7Wjb0balb0fbjrYtfTvadrTt+I7WcrsdbTt2oeV2O9p2tG3p29G2o23Hu1jL7Xa07diFltvtaNvRtqVvR9uOti19O9p2tO34jtZyux1tO9q29O1o29G2pW9H2462Hd/RWm63o21H25a+HW072rb07Wjb0bbjO1rL7Xa07Wjb0rejbUfblr4dbTvadnxHa7ndjrYdbVv6drTtaNvSt6NtR9uO72gtt9vRtqNtS9+Oth1tW/p2tO1o2/EdreV2O9p2tG3p29G2o21L3462HW07vqO13G5H245daLndjrYdbVv6drTtaNvxLtZyux1tO3ah5XY72na0benb0baj7cPpni9rd7k5M0/c8ftLM/P7d33m787Mv5+Z2ff9v87Mn56Zn7/7B+37/uv7vl/b9/3apz71qWNP/Ai5dOnSvP766+/9/ubNm/PpT3/67o9pe5C+HW072nYusu3tv9f3Dm63o21H25a+HW072rb07Wjb0bbjO1rL7Xa07Wjb0rejbUfblr4dbTvadnxHa7ndjrYdu9Byux1tO9q29O1o29G2412s5XY72nbsQsvtdrTtaNvSt6NtR9uWvh1tO9p2fEdrud2Oth1tW/p2tO1o29K3o21H247vaC2329G2o21L3462HW1b+na07Wjb8R2t5XY72na0benb0bajbUvfjrYdbTu+o7XcbkfbjrYtfTvadrRt6dvRtqNtx3e0ltvtaNvRtqVvR9uOti19O9p2tO34jtZyux1tO9q29O1o29G2pW9H2462Hd/RWm63o23HLrTcbkfbjrYtfTvadrTteBdrud2Oth270HK7HW072rb07Wjb0fbhdHqAz/y3mfnsWusvrbUen5lnZua7d33mf87MX5uZWWv95Xn3/9g3LvJBH0Vf/OIX5wc/+MG89tpr8/bbb8/zzz8/X/3qV+/+mLYH6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tH06P3e8D+76/s9b6BzPzH2bmEzPzG/u+v7LW+iczc33f9+/OzD+emX+11vpHM7PPzN/e930vH/xR8Nhjj813vvOd+dKXvjQ//vGP5+tf//o89dRT861vfWuuXbv2k49pe9CH9Z2Zn7v9MX0P0LZjFzratuxCx+12tG3ZhY7b7WjbsgsdbTt2oaNtyy503G5H25Zd6LjdjrYtu9DRtmMXOtq27ELH7Xa0bdmFjtvtaNuyCx1tO3aho23LLnTcbkfbll3ouN2Oti270NG2YxdabrejbccudLRt2YWO2+1o27ILHW07dqHldjvaduxCR9uWXei43Y62LbvQ0bZjF1put6Ntxy50tG3ZhY7b7Wjbsgsdt9vRtmUXOtp27EJH25Zd6LjdjrYtu9Bxux1tW3aho23HLnS0bdmFjtvtaNuyCx2329G2ZRc62nbsQkfbll3ouN2Oti270HG7HW1bdqGjbccudLRt2YWO2+1o27ILHbfb0bZlFzraduxCR9uWXei43Y62LbvQcbsdbVt2oaNtxy48nNa5+l+7dm2/fv36Wf7bD4u11sv7vl+7/yf/OG3v72jbGX3vR9uWXei43Y62LbvQ0bZjF1put6Ntxy50tG3ZhY7b7WjbsgsdbTt2oeV2O9p27EJH25Zd6LjdjrYtu9Bxux1tW3aho23HLnS0bdmFjtvtaNuyCx2329G2ZRc62nbsQkfbll3ouN2Oti270HG7HW1bdqGjbccudLRt2YWO2+1o27ILHbfb0bZlFzraduxCR9uWXei43Y62LbvQcbsdbVt2oaNtxy603G5H245d6Gjbsgsdt9vRtmUXOtp27ELL7Xa07diFjrYtu9Bxux1tW3aho23HLrTcbkfbjl3oaNuyCx2329G2ZRc62nbsQsvtdrTt2IWOti270HG7HW1bdqHjdjvatuxCR9uOXeho27ILHbfb0bZlFzput6Ntyy50tO3YhY62LbvQ+Si3e7rohwEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgZ+d07gcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4LjTuR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgONO534AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI47nfsBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADjudO4HAAoQwMUAACAASURBVAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOC407kfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDjTud+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACOO537AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA47nTuBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADguNO5HwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA407nfgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAjjud+wEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOO507gcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4LjTuR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgONO534AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI47nfsBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADjudO4HAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOC407kfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDjTud+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACOO537AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA47nTuBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADguNO5HwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA404P8qG11pfXWt9fa91Ya/3KB3zmb6y1/vta65W11r+92Md8dL344ovz5JNPzpUrV+a5556752e0PUbblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaPsQ2vf9Q3/NzCdm5tWZuTwzj8/M78zM5+/6zGdn5rdn5s/d/v2fv9/PvXr16v5x98477+yXL1/eX3311f2tt97an3766f2VV1557+9n5rq2x1Rtd333ff/wvjNzfbcLh9mFjl1o2YWOXeho27ILHbfb0bbjXaxlczt2oWMXWnahYxc62rbsQsftdrTteBdr2dyOXejYhZZd6NiFjl1o2YWOXeho27G5LZvbsQsdu9CyCx270LELLbvQsQsdbTs2t2VzO3ahYxdadqFjFzp2oWUXOnaho23H5rZsbscudOxCyy507ELHLrTsQscudLTt2NyWze3YhY5daNmFjl3o2IWWXejYhY62LbvQcbsdbTvexVo2t2MXOnahZRc6dqGjbcsudNxuR9uOd7GWze3YhY5daNmFjl3oaNuyCx2329G2412sZXM7dqFjF1p2oWMXOtq27ELH7Xa07XgXa9ncjl3o2IWWXejYhY5daNmFjl3oaNuxuS2b27ELHbvQsgsdu9CxCy270LELHW07Nrdlczt24Xx+crtHfp3m/n5pZm7s+/7Dfd/fnpnnZ+Zrd33m783MP9v3/f/MzOz7/gcP8HM/9l566aW5cuXKXL58eR5//PF55pln5oUXXrj7Y9oeoG1L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na0fTidHuAzn5mZ1+/4/c3bf3anX5yZX1xr/Ze11m+ttb58UQ/4KLt169Y88cQT7/3+0qVLc+vWrbs/pu0B2rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbh9NjD/CZdY8/2+/xcz47M788M5dm5j+v9Ufs1zGonnean+Hn/SxUhCzTxI33yMFCG8NqESw6s6QLCyHeytts4SKVA4bAVAmBhYCLSWOSIo2rKab2kiY2gXiKkGWrxaspZrEXhhlrQiRNMwRShci2+KeILGRZoyO/o3s+jnxdIMixvjl6uffJj/fb/mCt9b+/8ou27a2ZeWtm5uWXX/7GD/u8WevRjDPb9rXc2u7wLNve/9/q+xC329G2YxdabrejbUfblr4dbTvadryLtdxuR9uOXWi53Y62HW1b+na07Wjb8S7WcrsdbTt2oeV2O9p27ELL7Xa07Wjbsbktt9vRtmMXWm63o23HLrTcbkfbjrYdm9tyux1tO3ah5XY72nbsQsvtdrTtaNuxuS2329G2YxdabrejbccutNxuR9uOth2b23K7HW07dqHldjvaduxCy+12tO1o29K3o21H2453sZbb7WjbsQstt9vRtqNtS9+Oth1tO97FWm63o23HLrTcbkfbjrYtfTvadrTteBdrud2Oth270HK7HW072rb07Wjb0bbjXazldjvaduxCy+12tO3YhZbb7Wjb0bZjc1tut6Ntxy603G5H245daLndjrYdbTs2t+V2O9qeT4en+Mztmbn00M8nM/PLx3zm/bXW52utX8zMT+f//x/6K9ZaP1hrna61Tl988cW9z/zcODk5mVu3bj34+fbt2/PSSy89+jFtd3iWbWf0fZTb7WjbsQstt9vRtqNtS9+Oth1tO97FWm63o23HLrTcbkfbjrYtfTvadrTteBdrud2Oth270HK7HW07dqHldjvadrTt2NyW2+1o27ELLbfb0bZjF1put6NtR9uOzW253Y62HbvQcrsdbTt2oeV2O9p2tO3Y3Jbb7WjbsQstt9vRtmMXWm63o21H247NbbndjrYdu9Byux1tO3ah5XY72na0benb0bajbce7WMvtdrTt2IWW2+1o29G2pW9H2462He9iLbfb0bZjF1put6NtR9uWvh1tO9p2vIu13G5H245daLndjrYdbVv6drTtaNvxLtZyux1tO3ah5XY72nbsQsvtdrTtaNuxuS2329G2YxdabrejbccutNxuR9uOth2b23K7HW3Pp8NTfOZvZub3tm17Zdu2izPzxsx88Mhn/vPM/PHMzLZt/2Bm/tHM3HyWD/o8+u53vzs/+9nP5he/+MV89tln8957783rr7/+6Me03UHblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaHs+XTjrA2utL7Zt+97M/GhmXpiZH661Ptm27fszc2Ot9cH9v/tn27b93czcm5l/s9b6X+WDPw8uXLgw77777rz22mtz7969efPNN+fq1avz9ttvz+np6Zcf03YHbVtP6jsz37n/MX13cLsdbVt2oeN2O9q27ELH7Xa07Wjbsrkdt9vRtmUXOm63o23LLnTcbkfbjrYtm9txux1tW3ah43Y72rbsQsftdrTtaNuyuR2329G2ZRc6brejbcsudNxuR9uOti2b23G7HW1bdqHjdjvatuxCx+12tO1o27K5Hbfb0bZlFzput6Ntyy503G5H2462LZvbcbsdbVt2oeN2O9q27ELH7Xa0bdmFjtvtaNvRtmVzO263o23LLnTcbkfbll3ouN2Oth1tWza343Y72rbsQsftdrRt2YWO2+1o29G2ZXM7brejbcsudNxuR9uWXei43Y62HW1bNrfjdjvatuxCx+12tG3ZhY7b7Wjb0bZlcztut6Ntyy503G5H25Zd6LjdjrYdbVs2t+N2z6dtrXWUf/j09HTduHHjKP/2ebFt24/XWqdnf/KrtD3b3rYz+p5F25Zd6LjdjrYtu9DRtmMXWm63o23HLnS0bdmFjtvtaNuyCx1tO3ah5XY72nbsQkfbll3ouN2Oti270HG7HW1bdqGjbccudLRt2YWO2+1o27ILHbfb0bZlFzraduxCR9uWXei43Y62LbvQcbsdbVt2oaNtxy50tG3ZhY7b7Wjbsgsdt9vRtmUXOtp27EJH25Zd6LjdjrYtu9Bxux1tW3aho23HLrTcbkfbjl3oaNuyCx2329G2ZRc62nbsQsvtdrTt2IWOti270HG7HW1bdqGjbccutNxuR9uOXeho27ILHbfb0bZlFzraduxCy+12tO3YhY62LbvQcbsdbVt2oeN2O9q27EJH245d6Gjbsgsdt9vRtmUXOm63o23LLnS07diFjrYtu9D5TW738KwfBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDfnsOxHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA/Q7HfgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA9jsc+wEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2O9w7AcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYL/DsR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgP0Ox34AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPY7HPsBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANjvcOwHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGC/w7EfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAID9Dsd+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD2Oxz7AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADY73DsBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgv8OxHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA/Q7HfgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA9jsc+wEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2O9w7AcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYL/DsR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgP0Ox34AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPY7HPsBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANjvcOwHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGC/w7EfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAID9Dsd+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD2OzzNh7Zt+5Nt2366bdvPt2378yd87s+2bVvbtp0+u0d8vn344Yfz6quvzpUrV+add975tZ/Tdh99O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o+05tNZ64p+ZeWFmPp2ZyzNzcWZ+MjO//5jP/c7M/NXM/PXMnJ71e69fv76+7b744ot1+fLl9emnn667d++ua9eurU8++eTB38/MjaXtbk/q+2Xbpe8u2nbsQqdqu/Rda9mFkl3oaNuyCx2329G2ZRc62nbsQsd3tJZd6NiFjrYtu9Bxux1tW3aho23HLnR8R2vZhY5d6NiFll3o2IWOti270NG2Yxc63sVadqFjFzp2oWUXOnaho23LLnS07diFltvtaNuxCx3f0Vp2oWMXOtq27EJH245daLndjrYdu9DxHa1lFzp2oaNtyy50tO3YhZbb7WjbsQsd39FadqFjFzratuxCx+12tG3ZhY62HbvQ8R2tZRc6dqGjbcsudNxuR9uWXeho27ELHd/RWnahYxc62rbsQsftdrRt2YWOth270PEdrWUXOnaho23LLnTcbkfbll3oaNuxCx3f0Vp2oWMXOnahZRc6dqGjbcsudLTt2IWOd7GWXejYhY5daNmFjl3oaNuyCx1tO3bheB6+3W/65zBn+6OZ+fla6+Za67OZeW9m/vQxn/t3M/PvZ+b/PsXvZGY++uijuXLlyly+fHkuXrw4b7zxxrz//vuP+6i2O+jb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbc+nw1N85ndn5tZDP9++/98e2LbtD2fm0lrrvzzDZ3vu3blzZy5duvTg55OTk7lz585XPqPtfvp2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2/Pp8BSf2R7z39aDv9y2w8z8x5n512f+om17a9u2G9u23fjVr3719E/5nFprfe2/bdvXcmu701l93e5+2nbsQudZtr3/v9X3IXahYxc62rbsQsftdrRt2YWOth270PEdrWUXOnaho23LLnTcbkfbll3oaNuxCx3f0Vp2oWMXOnahZRc6dqGjbcsudLTt2IWOd7GWXejYhY5daNmFjl3oaNuyCx1tO3ah5XY72nbsQsd3tJZd6NiFjrYtu9DRtmMXWm63o23HLnR8R2vZhY5d6GjbsgsdbTt2oeV2O9p27ELHd7SWXejYhY62LbvQcbsdbVt2oaNtxy50fEdr2YWOXeho27ILHbfb0bZlFzraduxCx3e0ll3o2IWOti270HG7HW1bdqGjbccudHxHa9mFjl3oaNuyCx2329G2ZRc62nbsQsd3tJZd6NiFjl1o2YWOXeho27ILHW07dqHjXaxlFzp2oWMXWnahYxc62rbsQkfbjl04nw5P8ZnbM3PpoZ9PZuaXD/38OzPzBzPzl9u2/Y+Z+ccz88G2baeP/qK11g/WWqdrrdMXX3xx/1M/J05OTubWrVsPfr59+/a89NJLD3/khdF2t6fo63Z30rZjFzrPsu2Mvo+yCx270NG2ZRc6brejbcsudLTt2IWO72gtu9CxCx1tW3ah43Y72rbsQkfbjl3o+I7Wsgsdu9CxCy270LELHW1bdqGjbccudLyLtexCxy507ELLLnTsQkfbll3oaNuxCy2329G2Yxc6vqO17ELHLnS0bdmFjrYdu9Byux1tO3ah4ztayy507EJH25Zd6GjbsQstt9vRtmMXOr6jtexCxy50tG3ZhY7b7WjbsgsdbTt2oeM7WssudOxCR9uWXei43Y62LbvQ0bZjFzq+o7XsQscudLRt2YWO2+1o27ILHW07dqHjO1rLLnTsQkfbll3ouN2Oti270NG2Yxc6vqO17ELHLnTsQssudOxCR9uWXeho27ELHe9iLbvQsQsdu9CyCx270NG2ZRc62nbswjm11nrin5m5MDM3Z+aVmbk4Mz+ZmatP+PxfzszpWb/3+vXr69vu888/X6+88sq6efPmunv37rp27dr6+OOPH/z9zNxY2u72pL6Ptl36fiPaduxCp2q79F1r2YWSXeho27ILHbfb0bZlFzraduxCx3e0ll3o2IWOti270HG7HW1bdqGjbccudHxHa9mFjl3o2IWWXejYhY62LbvQ0bZjFzrexVp2oWMXOnahZRc6dqGjbcsudLTt2IWW2+1o27ELHd/RWnahYxc62rbsQkfbjl1oud2Oth270PEdrWUXOnaho23LLnS07diFltvtaNuxCx3f0Vp2oWMXOtq27ELH7Xa0bdmFjrYdu9DxHa1lFzp2oaNtyy503G5H25Zd6GjbsQsd39FadqFjFzratuxCx+12tG3ZhY62HbvQ8R2tZRc6dqGjbcsudNxuR9uWXeho27ELHd/RWnahYxc6dqFlFzp2oaNtyy50tO3YhY53sZZd6NiFjl1o2YWOXeho27ILHW07duF4Hne7T/vnwpxhrfXFtm3fm5kfzcwLM/PDtdYn27Z9//4//MFZv4PHu3Dhwrz77rvz2muvzb179+bNN9+cq1evzttvvz2np6fHfrxz70l9Z+Y7x36+80zbjl3oaNuyCx2329G2ZRc6brejbcsudLTt2IWOti270HG7HW1bdqHjdjvatuxCR9uOXeho27ILHbfb0bZlFzput6Ntyy50tO3YhY62LbvQcbsdbVt2oeN2O9q27EJH245daLndjrYdu9DRtmUXOm63o23LLnS07diFltvtaNuxCx1tW3ah43Y72rbsQkfbjl1oud2Oth270NG2ZRc6brejbcsudNxuR9uWXeho27ELHW1bdqHjdjvatuxCx+12tG3ZhY62HbvQ0bZlFzput6Ntyy503G5H25Zd6GjbsQsdbVt2oeN2O9q27ELH7Xa0bdmFjrYdu9DRtmUXOm63o23LLnTcbkfbll3oaNuxCx1tW3ah43Y72rbsQsftvvX+fAAAIABJREFUdrRt2YWOth27cD5ta62j/MOnp6frxo0bR/m3z4tt23681vrG/79H27PtbTuj71m0bdmFjtvtaNuyCx1tO3ah5XY72nbsQkfbll3ouN2Oti270NG2YxdabrejbccudLRt2YWO2+1o27ILHbfb0bZlFzraduxCR9uWXei43Y62LbvQcbsdbVt2oaNtxy50tG3ZhY7b7Wjbsgsdt9vRtmUXOtp27EJH25Zd6LjdjrYtu9Bxux1tW3aho23HLnS0bdmFjtvtaNuyCx2329G2ZRc62nbsQsvtdrTt2IWOti270HG7HW1bdqGjbccutNxuR9uOXeho27ILHbfb0bZlFzraduxCy+12tO3YhY62LbvQcbsdbVt2oaNtxy603G5H245d6Gjbsgsdt9vRtmUXOm63o23LLnS07diFjrYtu9Bxux1tW3ah43Y72rbsQkfbjl3oaNuyC53f5HYPz/phAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPjtORz7AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADY73DsBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgv8OxHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA/Q7HfgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA9jsc+wEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2O9w7AcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYL/DsR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgP0Ox34AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPY7HPsBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANjvcOwHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGC/w7EfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAID9Dsd+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD2Oxz7AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADY73DsBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgv8OxHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA/Q7HfgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA9jsc+wEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2O9w7AcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYL/DsR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgP0Ox34AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPY7HPsBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANjvcOwHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGC/w9N8aNu2P9m27afbtv1827Y/f8zf/6tt2/5u27a/3bbtv23b9g+f/aM+nz788MN59dVX58qVK/POO+987e+13U/blr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaHsOrbWe+GdmXpiZT2fm8sxcnJmfzMzvP/KZP56Zv3f///0vZ+Yvzvq9169fX992X3zxxbp8+fL69NNP1927d9e1a9fWJ5988uDvZ+aGtvtUbZe+a60n952ZG8su7GYXOnahZRc6dqGjbcsudNxuR9uOd7GWze3YhY5daNmFjl3oaNuyCx2329G2412sZXM7dqFjF1p2oWMXOnahZRc6dqGjbcfmtmxuxy507ELLLnTsQscutOxCxy50tO3Y3JbN7diFjl1o2YWOXejYhZZd6NiFjrYdm9uyuR270LELLbvQsQsdu9CyCx270NG2Y3NbNrdjFzp2oWUXOnahYxdadqFjFzratuxCx+12tO14F2vZ3I5d6NiFll3o2IWOti270HG7HW073sVaNrdjFzp2oWUXOnaho23LLnTcbkfbjnexls3t2IWOXWjZhY5d6Gjbsgsdt9vRtuNdrGVzO3ahYxdadqFjFzp2oWUXOnaho23H5rZsbscudOxCyy507ELHLrTsQscudLTt2NyWze3YheP58nb3/DnM2f5oZn6+1rq51vpsZt6bmT99+ANrrf++1vo/93/865k5eYrf+6330UcfzZUrV+by5ctz8eLFeeONN+b999//yme03Ufblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaHs+HZ7iM787M7ce+vn2/f/26/yLmfmvv8lDfVvcuXNnLl269ODnk5OTuXPnzpP+J9o+JW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rej7fl04Sk+sz3mv63HfnDb/vnMnM7MP/k1f//WzLw1M/Pyyy8/5SM+v9b6esZte1xubb+pZ9n2/mf0fYjb7WjbsQstt9vRtqNtS9+Oth1tO97FWm63o23HLrTcbkfbjrYtfTvadrTteBdrud2Oth270HK7HW07dqHldjvadrTt2NyW2+1o27ELLbfb0bZjF1put6NtR9uOzW253Y62HbvQcrsdbTt2oeV2O9p2tO3Y3Jbb7WjbsQstt9vRtmMXWm63o21H247NbbndjrYdu9Byux1tO3ah5XY72na0benb0bajbce7WMvtdrTt2IWW2+1o29G2pW9H2462He9iLbfb0bZjF1put6NtR9uWvh1tO9p2vIu13G5H245daLndjrYdbVv6drTtaNvxLtZyux1tO3ah5XY72nbsQsvtdrTtaNuxuS2329G2YxdabrejbccutNxuR9uOth2b23K7HW3Pp8NTfOb2zFx66OeTmfnlox/atu2fzsy/nZnX11p3H/eL1lo/WGudrrVOX3zxxT3P+1w5OTmZW7duPfj59u3b89JLL33tc9p+c8+y7Yy+j3K7HW07dqHldjvadrRt6dvRtqNtx7tYy+12tO3YhZbb7Wjb0balb0fbjrYd72Itt9vRtmMXWm63o23HLrTcbkfbjrYdm9tyux1tO3ah5XY72nbsQsvtdrTtaNuxuS2329G2YxdabrejbccutNxuR9uOth2b23K7HW07dqHldjvaduxCy+12tO1o27G5Lbfb0bZjF1put6Ntxy603G5H2462LX072na07XgXa7ndjrYdu9Byux1tO9q29O1o29G2412s5XY72nbsQsvtdrTtaNvSt6NtR9uOd7GW2+1o27ELLbfb0bajbUvfjrYdbTvexVput6Ntxy603G5H245daLndjrYdbTs2t+V2O9p27ELL7Xa07diFltvtaNvRtmNzW263o+05tdZ64p+ZuTAzN2fmlZm5ODM/mZmrj3zmD2fm05n5vbN+35d/rl+/vr7tPv/88/XKK6+smzdvrrt3765r166tjz/++MHfz8wNbfep2i5911pP7jszN5Zd2M0udOxCyy507EJH25Zd6LjdjrYd72Itm9uxCx270LILHbvQ0bZlFzput6Ntx7tYy+Z27ELHLrTsQscudOxCyy507EJH247Nbdncjl3o2IWWXejYhY5daNmFjl3oaNuxuS2b27ELHbvQsgsdu9CxCy270LELHW07Nrdlczt2oWMXWnahYxc6dqFlFzp2oaNtx+a2bG7HLnTsQssudOxCxy607ELHLnS0bdmFjtvtaNvxLtayuR270LELLbvQsQsdbVt2oeN2O9p2vIu1bG7HLnTsQssudOxCR9uWXei43Y62He9iLZvbsQsdu9CyCx270NG2ZRc6brejbce7WMvmduxCxy607ELHLnTsQssudOxCR9uOzW3Z3I5d6NiFll3o2IWOXWjZhY5d6Gjbsbktm9uxC8fz5e3u+XNhzrDW+mLbtu/NzI9m5oWZ+eFa65Nt275//x/+YGb+w8z8/Zn5T9u2zcz8z7XW62f97m+7CxcuzLvvvjuvvfba3Lt3b9588825evXqvP3223N6evrlx7TdQdvWk/rOzHfuf0zfHdxuR9uWXei43Y62LbvQcbsdbTvatmxux+12tG3ZhY7b7Wjbsgsdt9vRtqNty+Z23G5H25Zd6LjdjrYtu9Bxux1tO9q2bG7H7Xa0bdmFjtvtaNuyCx2329G2o23L5nbcbkfbll3ouN2Oti270HG7HW072rZsbsftdrRt2YWO2+1o27ILHbfb0bajbcvmdtxuR9uWXei43Y62LbvQcbsdbVt2oeN2O9p2tG3Z3I7b7Wjbsgsdt9vRtmUXOm63o21H25bN7bjdjrYtu9Bxux1tW3ah43Y72na0bdncjtvtaNuyCx2329G2ZRc6brejbUfbls3tuN2Oti270HG7HW1bdqHjdjvadrRt2dyO2+1o27ILHbfb0bZlFzput6NtR9uWze243fNpW2sd5R8+PT1dN27cOMq/fV5s2/bjtdbp2Z/8Km3PtrftjL5n0bZlFzput6Ntyy50tO3YhZbb7WjbsQsdbVt2oeN2O9q27EJH245daLndjrYdu9DRtmUXOm63o23LLnTcbkfbll3oaNuxCx1tW3ah43Y72rbsQsftdrRt2YWOth270NG2ZRc6brejbcsudNxuR9uWXeho27ELHW1bdqHjdjvatuxCx+12tG3ZhY62HbvQ0bZlFzput6Ntyy503G5H25Zd6GjbsQstt9vRtmMXOtq27ELH7Xa0bdmFjrYdu9Byux1tO3aho23LLnTcbkfbll3oaNuxCy2329G2Yxc62rbsQsftdrRt2YWOth270HK7HW07dqGjbcsudNxuR9uWXei43Y62LbvQ0bZjFzratuxCx+12tG3ZhY7b7WjbsgsdbTt2oaNtyy50fpPbPTzrhwEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgt+dw7AcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYL/DsR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgP0Ox34AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPY7HPsBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANjvcOwHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGC/w7EfAP4f+3UQattiHvb9W8eqUjBp0iYdGEvgCBmZ2HhgCeNMSiGDuh5Eg6YgD5qkNAgR2nlCBsGZ1OPQUpdSs50OElNPrEJrQ0lNRrHyBLUrUVyebFpLNSSKqSYF24Ldwbvndp3rd8/d3uf8z15rvd8PLrx779Fd3/r788feAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwPXubj0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANe7u/UAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFzv7tYDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHC9u1sPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMD17m49AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADXu7v1AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABc7+7WAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwvbtbDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADA9e5uPQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA17u79QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAXO/u1gMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcL27Ww8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwPXubj0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANe7u/UAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFzv7tYDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHC9u1sPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMD17i75oWVZfnJZlt9aluX9ZVn+9of8/Z9aluUXX/39ry/L8gPPPehR/cqv/Mp85jOfmU9/+tPzsz/7s3/s77V9Gn072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0baj7Q6dz+dHf83M98zMN2bmUzPz8Zn5jZn5i2/8zN+amZ979d9fmJlffNe/+9nPfvb8Uffd7373/KlPfer8jW984/wHf/AH5x/90R89f/3rX3/99zPznrbXe6zvzLx3trtX07bjLnSqtmd9z+ezu1ByFzrattyFjt3taNtyFzradtyFju9oLXeh4y50tG25Cx2729G25S50tO24Cx3f0VruQsdd6LgLLXeh4y50tG25Cx1tO+5Cx2exlrvQcRc67kLLXei4Cx1tW+5CR9uOu9Cyux1tO+5Cx3e0lrvQcRc62rbchY62HXehZXc72nbchY7vaC13oeMudLRtuQsdbTvuQsvudrTtuAsd39Fa7kLHXeho23IXOna3o23LXeho23EXOr6jtdyFjrvQ0bblLnTsbkfblrvQ0bbjLnR8R2u5Cx13oaNty13o2N2Oti13oaNtx13o+I7Wchc67kJH25a70LG7HW1b7kJH24670PEdreUudNyFjrvQchc67kJH25a70NG24y50fBZruQsdd6HjLrTchY670NG25S50tO24C7dzv7vX/Lqbd/vxmXn/fD7/9vl8/sOZ+ccz8/k3fubzM/MLr/77l2bmLy/Lslzwb3+kfeUrX5lPf/rT86lPfWo+/vGPzxe+8IX55V/+5Td/TNsr6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1t9+nugp/5/pn53dXvv/nqzz70Z87n83dn5jsz8+eeY8Aj+9a3vjWf/OQnX//+E5/4xHzrW99688e0vZK+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0XaflvP5/PgPLMt/ODP/3vl8/puvfv8fzcyPn8/n/2z1M19/9TPffPX7b7z6mX/1xr/1xZn54qvf/sjMfO25XuQZ/PmZ+fYLP/PfnJl/Y2b+z1e//7dm5nvn1f+TzMxnZub/mv23ndle38+cz+c/bXevpm3HXeg8W9tXf7flvlvbXXfhadyFjrYtd6HzUdldbVtb6+suPI22HXeh4ztay13ouAsdbVvuQuejsrvatrbW1114Gm077kLHd7SWu9BxFzruQstd6HxU7oK2ra31dReeRtuOu9DxWazlLnTchY670HIXOh+Vu6Bta2t93YWn0bbjLrTsbkfbjrvQ8R2t5S503IWOti13oaNtx11o2d2Oth13oeM7Wstd6LgLHW1b7kJH24670LK7HW077kLHd7SWu9BxFzrattyFzkdld7Vtba2vu/A02nbchY7vaC13oeMudLRtuQudj8ruatvaWl934Wm07bgLHd/RWu5Cx13oaNtyFzofld3VtrW1vu7C02jbcRc6vqO13IWOu9DRtuUudD4qu6tta2t93YWn0bbjLnR8R2u5Cx13oeMutNyFzkflLmjb2lpfd+FptO24Cx2fxVruQsdd6LgLLXeh81G5C9q2ttbXXXgabTvuwu185nw+/+mr/pfn8/nRXzPzl2bmV1e//zsz83fe+JlfnZm/9Oq/PzYfxFne8e++965nv+SvW8zzrrYz894R2m6x7/08R+ir7Uen7f1MR2h7i5mqtlvsu7XddRe6tvczHaHtLWbS9nZ93YWu7f1M2mq7t77ugrZ7bHs/0xHa3mKmqu0W+25td92Fru39TEdoe4uZtL1dX3eha3s/k7ba7q2vu6DtHtvez3SEtreYqWq7xb5b2113oWt7P9MR2t5iJnfhdn3dha7t/Uzaaru3vu6Ctntsez/TEdreYqaq7Rb7bm133YWu7f1MR2h7i5nchdv1dRe6tvczaavt3vq6C9ruse39TEdou8W+dlfbPba9n+kIbW8xU9V2i323trvuQtf2fqYjtL3FTNrerq+7oO0e297PdIS2W+xrd7XdY9v7mY7Q9hYzVW232Hdru+sudG3vZzpC21vMpO3t+roL2u6x7f1MR2i7xb52V9s9tr0HN+cAAAAgAElEQVSf6QhtbzFT1XaLfbe2u+5C1/Z+piO0vcVM2t6ur7vQtb2fSVtt99bXXdB2j23vZzpC21vMVLXdYt+t7a670LW9n+kIbW8xk7a36+sudG3vZ9JW2731dRe03WPb+5mO0PYWM1Vtt9h3a7vrLnRt72c6QttbzKTt7fq6C13b+5m01XZvfd0FbffY9n6mI7S9xUxV2y323druugtd2/uZjtD2FjNpe7u+7kLX9n4mbbXdW193Qds9tr2f6QhtbzFT1XaLfbe2u+5C1/Z+piO0vcVM7sLt+roLXdv7mbTVdm993QVt99j2fqYjtL3FTFXbLfbd2u66C13b+5mO0PYWM7kLt+vrLnRt72fSVtu99XUXtN1j2/uZjtB2izM9ZZ67ebd/PjM/uCzLX1iW5eMz84WZ+fIbP/Plmfnrr/77r87MPzm/moxHadvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9ru0Mfe9QPn8/m7y7L8pzPzqzPzPTPz8+fz+evLsvz9mXnvfD5/eWb+25n575ZleX9mfn8++D8+7/Cutq9+TNsrvaPvn3n1Y/peQduOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu9Cyux1tO+5CR9uWu9Cxux1tW+5CR9uOu9Cyux1tO+5CR9uWu9Cxux1tW+5CR9uOu9Cyux1tO+5CR9uWu9Cxux1tW+5Cx+52tG25Cx1tO+5CR9uWu9Cxux1tW+5Cx+52tG25Cx1tO+5CR9uWu9Cxux1tW+5Cx+52tG25Cx1tO+5CR9uWu9Cxux1tW+5Cx+52tG25Cx1tO+5CR9uWu9Cxux1tW+5Cx+52tG25Cx1tO+5CR9uWu9Cxux1tW+5Cx+52tG25Cx1tO+7CTp3P55v8mpkv3urZe5jnKTMd6V22OM+R3mVr8xzpXbY205HeZYvzHOldtjbPkd5lazMd6V22Ns/W3mOLM7kL25xpa++ytXmO1HaLM7kL25znSO+ytZmO9C5bnOdI77K1eY70Llub6UjvsrV5tvYeW5zJXdjmTFt7l63Nc6S2W5zJXdjmPEd6l63NdKR32eI8R3qXrc1zpHfZ2kxHepctznOkd9naPEd6l63NtLV32do8R2q7xZnchW3Oc6R32dpMR3qXLc5zpHfZ2jxHepetzXSkd9niPEd6l63Nc6R32dpMW3uXrc1zpLZbnMld2OY8R3qXrc10pHfZ4jxHepetzXOkd9naTEd6ly3Oc6R32do8R3qXrc10pHfZ2jxbe48tzuQubHOeI73L1mY60rtscZ4jvcvW5jnSu2xtpiO9yxbnOdK7bG2eI73L1mY60rtsbZ6tvccWZ3IXtjnPkd5lazMd6V22OM+R3mVr8xzpXbY205HeZYvzHOldtjbPkd5lazMd6V22Ns/W3mOLM7kL25xpa++ytXmO1HaLM7kL25znSO+ytZmO9C5bnOdI77K1eY70Llub6UjvsrV5tvYeW5zJXdjmTFt7l63Nc6S2W5zJXdjmPEd6l63NdKR32eI8R3qXrc1zpHfZ2kxHepetzbO199jiTO7CNmfa2rtsbZ4jtd3iTO7CNuc50rtsbaYjvcsW5znSu2xtniO9y9ZmOtK7bG2erb3HFmdyF7Y509beZWvzHKntFmdyF7Y5z5HeZWszHeldtjjPkd5la/Mc6V22NtOR3mWL8xzpXbY2z5HeZWszbe1dtjbPkdpucSZ3YZvzHOldtjbTkd5li/Mc6V22Ns+R3mVrMx3pXbY4z5HeZWvzHOldtjbT1t5la/Mcqe0WZ3IXtjnPkd5lazMd6V22OM/y6h8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYIfubj0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANe7qx+wLMtPLsvyW8uyvL8sy9/+kL//U8uy/OKrv//1ZVl+4Mbz/I1lWf7lsiz/66tffzOe5+eXZfkXy7J87S1/vyzL8g9ezfuby7L82J/gXbQ9SNsLZ9L3Stp2tG3p29G2o21rS32f0vbCd7G7B9ldbTvatvTtaNvRtqVvR9uOtq0t9X1K2wvfxe4eZHe17Wjb0rejbUfblr4dbTvatrbU9yltL3wXu3uQ3dW2o21L3462HW1b+na07Wjb2lLfp7S98F3s7kF2V9uOti19O9p2tG3p29G2o21rS32f0vbCd7G7B9ldbTvatvTtaNvRtqVvR9uOtq0t9X1K2wvfxe4eZHe1bW2pr7Z291LadrRt6dvRtqNta0t9n9L2wnexuwfZXW1bW+rrLtjdS2nb0balb0fbjrYtfTvadrRtbanvU9pe+C529yC7q21H25a+HW072rb07Wjb0ba1pb5PaXvhu9jdg+yuth1tW/p2tO1o29K3o21H29aW+j6l7YXvYncPsrvadrRt6dvRtqNtS9+Oth1tW1vq+5S2F76L3T3I7mrb0balb0fbjrYtfTvadrRtbanvU9pe+C529yC7q21H25a+HW07R2r7qPP5nP2ame+ZmW/MzKdm5uMz8xsz8xff+Jm/NTM/9+q/vzAzv3jjef7GzPwXZZc3nvfvzMyPzczX3vL3PzUz/9PMLDPzEzPz69p+tNrqa3e11VZfu6uttnvse23bLfbdWtsj7a622u6xrb52V1tt9bW72mq7x77Xtt1i3621PdLuaqvtHtvqa3e11VZfu6uttnvse23bLfbdWtsj7a622u6xrb52V1tt9bW72mq7x77Xtt1i3621PdLuaqvtHtvqa3e11VZfu6uttnvse23bLfbdWtsj7a622u6xrb52V1tt9bW72mq7x77Xtt1i3621PdLuausuaKuvttoeoa2+dldbbffQ99q2W+y7tbZH2l1t3YVL2m6x79baHml3tdV2j231tbvaaquv3dVW2z32vbbtFvture2RdldbbffYVl+7q622+tpdbbXdY99r226x79baHml3tdV2j231tbvaaquv3dVW2z32vbbtFvture2RdldbbffYVl+7q622+tpdbbXdY99r226x79baHml3tdV2j231tbvaaquv3dVW2z32vbbtFvture2RdldbbffYVl+7q+3t277r1920fnxm3j+fz799Pp//cGb+8cx8/o2f+fzM/MKr//6lmfnLy7IsN5znRZ3P5386M7//yI98fmb+4fkD/2xm/uyyLN832r7TgdrOhTO9qAP11Vbbtb20nQtnelEH6quttmt7aTsXzvRintB2Znt9N9V25lC7q622a3tpOxfO9KIO1Fdbbdf20nYunOlFHaivttqu7aXtXDjTi/EdrXWg3dVW27W9tJ0LZ3pRB+qrrbZre2k7F870og7UV1tt1/bSdi6c6cX4jtY60O5qq+3aXtrOhTO9qAP11Vbbtb20nQtnelEH6quttmt7aTsXzvRifEdrHWh3tdV2bS9t58KZXtSB+mqr7dpe2s6FM72oA/XVVtu1vbSdC2d6Mb6jtQ60u9pqu7aXtnPhTC/qQH211XZtL23nwple1IH6aqvt2l7azoUzvRjf0VoH2l1t3YU1bZ/gQH211XZtL23nwple1IH6aqvt2l7azoUzvRjf0VoH2l1t3YU1d+EJDrS72mq7tpe2c+FML+pAfbXVdm0vbefCmV7Ugfpqq+3aXtrOhTO9GN/RWgfaXW21XdtL27lwphd1oL7aaru2l7Zz4Uwv6kB9tdV2bS9t58KZXozvaK0D7a622q7tpe1cONOLOlBfbbVd20vbuXCmF3Wgvtpqu7aXtnPhTC/Gd7TWgXZXW23X9tJ2LpzpRR2or7baru2l7Vw404s6UF9ttV3bS9u5cKYX4zta60C7q622a3tpOxfO9KIO1Fdbbdf20nYunOlFHaivttqu7aXtXDjTi/EdrXWg3dVW27W9tJ0LZ3pRB+qrrbZrj31eeKu75xrwLb5/Zn539ftvvvqzD/2Z8/n83Zn5zsz8uRvOMzPzHyzL8pvLsvzSsiyfjGa51Ntm1vbp9tL2wfMemWlG3+ec803aPt+cb9L2Ovra3TVtO3tp++B5j8w0s52+j827tb57azuzn93VVtu1vbR98LxHZprR9znnfJO2zzfnm7S9jr52d03bzl7aPnjeIzPNbKev72itveyuttqu7aXtg+c9MtOMvs8555u0fb4536TtdfS1u2vadvbS9sHzHplpZjt9fUdr7WV3tdV2bS9tHzzvkZlm9H3OOd+k7fPN+SZtr6Ov3V3TtrOXtg+e98hMM9vp6ztaay+7q622a3tp++B5j8w0o+9zzvkmbZ9vzjdpex197e6atp29tH3wvEdmmtlOX9/RWnvZXW21XdtL2wfPe2SmGX2fc843aft8c75J2+voa3fXtO3spe2D5z0y08x2+vqO1trL7mrrLqxp29pLX221XdtL2wfPe2SmGX2fc843aft8c75J2+vsra/vaK297K627sKau9Day+5qq+3aXto+eN4jM83o+5xzvknb55vzTdpeR1+7u6ZtZy9tHzzvkZlmttPXd7TWXnZXW23X9tL2wfMemWlG3+ec803aPt+cb9L2Ovra3TVtO3tp++B5j8w0s52+vqO19rK72mq7tpe2D573yEwz+j7nnG/S9vnmfJO219HX7q5p29lL2wfPe2Smme309R2ttZfd1Vbbtb20ffC8R2aa0fc553yTts8355u0vY6+dndN285e2j543iMzzWynr+9orb3srrbaru2l7YPnPTLTjL7POeebtH2+Od+k7XX0tbtr2nb20vbB8x6ZaWY7fX1Ha+1ld7XVdm0vbR8875GZZvR9zjnfpO3zzfmmPbR91F02zgeWD/mz8xU/81wuedb/MDM/cD6ff3Rm/ueZ+YVolku9bWZtn24vbS99nr7X0Vbbtb20vfR5+l5HW23X9tL20udtqe9j826t797azuxnd7XVdm0vbS99nr7X0Vbbtb20vfR5+l5HW23X9tL20udtqa/vaK297K622q7tpe2lz9P3Otpqu7aXtpc+T9/raKvt2l7aXvq8LfX1Ha21l93VVtu1vbS99Hn6Xkdbbdf20vbS5+l7HW21XdtL20uft6W+vqO19rK72mq7tpe2lz5P3+toq+3aXtpe+jx9r6Ottmt7aXvp87bU13e01l52V1tt1/bS9tLn6XsdbbVd20vbS5+n73W01XZtL20vfd6W+vqO1trL7mrrLqxp29pLX221XdtL20ufp+91tNV2bS9tL33elvr6jtbay+5q6y6suQutveyuttqu7aXtpc/T9zraaru2l7aXPk/f62ir7dpe2l76vC319R2ttZfd1Vbbtb20vfR5+l5HW23X9tL20ufpex1ttV3bS9tLn7elvr6jtfayu9pqu7aXtpc+T9/raKvt2l7aXvo8fa+jrbZre2l76fO21Nd3tNZedldbbdf20vbS5+l7HW21XdtL20ufp+91tNV2bS9tL33elvr6jtbay+5qq+3aXtpe+jx9r6Ottmt7aXvp8/S9jrbaru2l7aXP21Jf39Fae9ldbbVd20vbS5+n73W01Xbtqj53wSBr35yZT65+/4mZ+b/f9jPLsnxsZv7MzPz+reY5n8//6nw+/8Gr3/43M/PZaJZLvW1mbZ9uL20fPO9tM+n77HO+pu2zz/matk+ir91d07azl7YPnve2mTbW97F5t9Z3b21n9rO72mq7tpe2D573tpn0ffY5X9P22ed8Tdsn0dfurmnb2UvbB89720wb6+s7Wmsvu6uttmt7afvgeW+bSd9nn/M1bZ99zte0fRJ97e6atp29tH3wvLfNtLG+vqO19rK72mq7tpe2D573tpn0ffY5X9P22ed8Tdsn0dfurmnb2UvbB89720wb6+s7Wmsvu6uttmt7afvgeW+bSd9nn/M1bZ99zte0fRJ97e6atp29tH3wvLfNtLG+vqO19rK72mq7tpe2D573tpn0ffY5X9P22ed8Tdsn0dfurmnb2UvbB89720wb6+s7Wmsvu6utu7CmbWsvfbXVdm0vbR88720z6fvsc76m7bPP+Zq2T7K3vr6jtfayu9q6C2vuQmsvu6uttmt7afvgeW+bSd9nn/M1bZ99zte0fRJ97e6atp29tH3wvLfNtLG+vqO19rK72mq7tpe2D573tpn0ffY5X9P22ed8Tdsn0dfurmnb2UvbB89720wb6+s7Wmsvu6uttmt7afvgeW+bSd9nn/M1bZ99zte0fRJ97e6atp29tH3wvLfNtLG+vqO19rK72mq7tpe2D573tpn0ffY5X9P22ed8Tdsn0dfurmnb2UvbB89720wb6+s7Wmsvu6uttmt7afvgeW+bSd9nn/M1bZ99zte0fRJ97e6atp29tH3wvLfNtLG+vqO19rK72mq7tpe2D573tpn0ffY5X9P22ed8bUdtH3WXjfOBfz4zP7gsy19YluXjM/OFmfnyGz/z5Zn566/++6/OzD85n8/nW82zLMv3rX77V2bmf49mudSXZ+avLR/4iZn5zvl8/r3R9jnspe1cMpO+V9NW27W9tJ1LZtL3atpqu7aXtnPJTBvr+7a2M9vru7e2M/vZXW21XdtL27lkJn2vpq22a3tpO5fMpO/VtNV2bS9t55KZNtbXd7TWXnZXW23X9tJ2LplJ36tpq+3aXtrOJTPpezVttV3bS9u5ZKaN9fUdrbWX3dVW27W9tJ1LZtL3atpqu7aXtnPJTPpeTVtt1/bSdi6ZaWN9fUdr7WV3tdV2bS9t55KZ9L2attqu7aXtXDKTvlfTVtu1vbSdS2baWF/f0Vp72V1ttV3bS9u5ZCZ9r6attmt7aTuXzKTv1bTVdm0vbeeSmTbW13e01l52V1t3YU3b1l76aqvt2l7aziUz6Xs1bbVd20vbuWSmjfX1Ha21l93V1l1Ycxdae9ldbbVd20vbuWQmfa+mrbZre2k7l8yk79W01XZtL23nkpk21td3tNZedldbbdf20nYumUnfq2mr7dpe2s4lM+l7NW21XdtL27lkpo319R2ttZfd1Vbbtb20nUtm0vdq2mq7tpe2c8lM+l5NW23X9tJ2LplpY319R2vtZXe11XZtL23nkpn0vZq22q7tpe1cMpO+V9NW27W9tJ1LZtpYX9/RWnvZXW21XdtL27lkJn2vpq22a3tpO5fMpO/VtNV2bS9t55KZNtbXd7TWXnZXW23X9tJ2LplJ36tpq+3aY58X3u58Pqe/ZuanZub/mJlvzMzfffVnf39m/sqr//7XZ+a/n5n3Z+YrM/OpG8/zn8/M12fmN2bmf5mZH4rn+Ucz83sz80cz882Z+U9m5ksz86VXf7/MzH/5at7/bWY+p+1Hr62+dldbbfW1u9pqu7e+T2m7xb5banu03dVW2z221dfuaqutvnZXW2331vcpbbfYd0ttj7a72mq7x7b62l1ttdXX7mqr7d76PqXtFvtuqe3RdldbbffYVl+7q622+tpdbbXdW9+ntN1i3y21PdruaqvtHtvqa3e11VZfu6uttnvr+5S2W+y7pbZH211ttd1jW33trrba6mt3tdV2b32f0naLfbfU9mi7q233Llvrq63d1VbbI7fV1+5qq+3W+z6l7Rb7bqnt0XZX2+5dttbXXbC72mp75Lb62l1ttdXX7mqr7d76PqXtFvtuqe3RdldbbffYVl+7q622+tpdbbXdW9+ntN1i3y21PdruaqvtHtvqa3e11VZfu6uttnvr+5S2W+y7pbZH211ttd1jW33trrba6mt3tdV2b32f0naLfbfU9mi7q622e2yrr93VVlt97a622u6t71PabrHvltoebXe11XaPbfW1u9revu1jv5ZX/2MAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAduju1gMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcL27Ww8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwPXubj0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANe7u/UAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFzv7tYDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHC9u1sPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMD17m49AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADXu7v1AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABc7+7WAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwvbtbDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADA9e5uPQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA17u79QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAXO/uXT+wLMvPL8vyL5Zl+dpb/n5ZluUfLMvy/rIsv7ksy489/5jHpW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0bem7P3cX/MxpZn7ykb//92fmB1/9+uLM/FdPH+sj5TT6Vk6jbeU02pZOo2/lNNpWTqNt5TTalk6jb+U02lZOo23pNPpWTqNt5TTalk6jb+U02lZOo23lNNqWTqNv5TTaVk6jbek0+lZOo23lNNqWTqNv5TTaVk6jbeU02pZOo2/lNNpWTqNt6TT6Vk6jbeU02pZOo2/lNNpWTqNt5TTalk6jb+U02lZOo23pNPpWTqNt5TTalk6jb+U02lZOo23lNNqWTqNv5TTaVk6jbek0+lZOo23lNNqWTqNv5TTaVk6jbeU02pZOo2/lNNpWTqNt6TT6Vk6jbeU02pZOo2/lNNpWTqNt6TT6Vk6jbeU02lZOo23pNPpWTqNt5TTalk6jb+U02lZOo23pNPpWTqNt5TTaVk6jbek0+lZOo23lNNqWTqNv5TTaVk6jbek0+lZOo23lNNpWTqNt6TT6Vk6jbeU02pZOo2/lNNpWTqNt6TT6Vk6jbeU02lZOo23pNPpWTqNt5TTalk6jb+U02lZOo23pNPpWTqNt5TTaVk6jbek0+lZOo23lNNqWTqNv5TTaVk6jbek0+lZOo23lNNpWTqNt6TT6Vk6jbeU02pZOo2/lNNpWTqNt6TT67srdu37gfD7/05n5/Ud+5PMz8w/PH/hnM/Nnl2X5vuca8Oj07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfffn7hn+je+fmd9d/f6br/6M56FvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3puzEfe4Z/Y/mQPzt/6A8uyx6aOYwAACAASURBVBdn5oszM9/7vd/72R/6oR96hsfv34/8yI/M+++/P5/73OcedPvqV7/67Zn5yof8T/5YX20/3HO0ndH3w2jbchc6drejbedtbV/5w5n52Tf+TNs/ATe34y503IWWu9BxFzrattyFjt3taNvxWazl5nbchY670HIXOu5CR9uWu9Cxux1tOz6LtexuR9uOu9DyWazjLnS0bbkLHbvb0bbjs1jL7na07bgLLZ/FOu5CR9uWu9Cxux1tOz6LtexuR9uOu9DyWazjLnS0bbkLHbvb0bbjs1jL7na07bgLLZ/FOu5CR9uWu9Cxux1tW+5Cx+52tO34jtZyczvuQsddaLkLHXeho23LXejY3Y62HZ/FWm5ux13ouAstd6HjLnS0bbkLHbvb0bbjs1jLze24Cx13oeUudNyFjrYtd6FjdzvadnwWa7m5HXeh4y603IWOu9DRtuUudOxuR9uOz2Itu9vRtuMutHwW67gLHW1b7kLH7na07fgs1rK7HW077kLLZ7GOu9DRtuUuvLyvfvWr3z6fz//2Vf/j8/n8zl8z8wMz87W3/N1/PTM/vfr9b83M973r3/zsZz975gO/8zu/c/7hH/7hP/bnM/PeNX21/f89d9uzvq9p23IXOna3o23nbW3P5/N5Zv6ltk/j5nbchY670HIXOu5CR9uWu9Cxux1tOz6LtdzcjrvQcRda7kLHXeho23IXOna3o23HZ7GW3e1o23EXWj6LddyFjrYtd6FjdzvadnwWa9ndjrYdd6Hls1jHXeho23IXOna3o23HZ7GW3e1o23EXWj6LddyFjrYtd6FjdzvadnwWa9ndjrYdd6Hls1jHXeho23IXOna3o23LXejY3Y62Hd/RWm5ux13ouAstd6HjLnS0bbkLHbvb0bbjs1jLze24Cx13oeUudNyFjrYtd6FjdzvadnwWa7m5HXeh4y603IWOu9DRtuUudOxuR9uOz2ItN7fjLnTchZa70HEXOtq23IWO3e1o2/FZrGV3O9p23IWWz2Idd6Gjbctd6NjdjrYdn8VadrejbcddaPks1nEXOtq23IWXNzPvnd+xo2/7dTdP9+WZ+WvLB35iZr5zPp9/7xn+XT6gb0fbjrYtfTvadrTt/D+jbcnudrTtuAstu9vRtqNtS9+Oth1tOz6LtexuR9uOu9Cyux1tO9q29O1o29G247NYy+52tO24Cy2729G2o21L3462HW07Pou17G5H24670LK7HW072rb07Wjb0bbjs1jL7na07bgLLbvb0bajbUvfjrYdbTs+i7XsbkfbjrvQsrsdbTvatvTtaNvRtqVvR9uOth3f0Vp2t6Ntx11o2d2Oth1tW/p2tO1o2/FZrGV3O9p23IWW3e1o29G2pW9H2462HZ/FWna3o23HXWjZ3Y62HW1b+na07Wjb8VmsZXc72nbchZbd7Wjb0balb0fbjrYdn8VadrejbcddaNndjrYdbVv6drTtaNvxWaxldzvadtyFlt3taNvRtqXvxnzsXT+wLMs/mpl/d2b+/LIs35yZvzcz/9rMzPl8/rmZ+R9n5qdm5v2Z+X9n5j+uhj2in/7pn55f+7Vfm29/+9vziU98Yn7mZ35m/uiP/mj9I/peSduOti19O9p2tO081vZLX/rSzMx3Zua3R9ur2N2Oth13oWV3O9p2tG3p29G2o23HZ7GW3e1o23EXWna3o21H25a+HW072nZ8FmvZ3Y62HXehZXc72na0benb0bajbcdnsZbd7WjbcRdadrejbUfblr4dbTvadnwWa9ndjrYdd6FldzvadrRt6dvRtqNtx2exlt3taNtxF1p2t6NtR9uWvh1tO9q29O1o29G24ztay+52tO24Cy2729G2o21L3462HW07Pou17G5H24670LK7HW072rb07Wjb0bbjs1jL7na07bgLLbvb0bajbUvfjrYdbTs+i7XsbkfbjrvQsrsdbTvatvTtaNvRtuOzWMvudrTtuAstu9vRtqNtS9+Oth1tOz6LtexuR9uOu9Cyux1tO9q29N2f5Xw+3+TBn/vc587vvffeTZ69F8uyfPV8Pn/uT/q/0/bdrm07o++7aNtyFzp2t6Nty13oaNtxF1p2t6Ntx13oaNtyFzp2t6Nty13oaNtxF1p2t6Ntx13oaNtyFzp2t6Nty13o2N2Oti13oaNtx13oaNtyFzp2t6Nty13o2N2Oti13oaNtx13oaNtyFzp2t6Nty13o2N2Oti13oaNtx13oaNtyFzp2t6Nty13o2N2Oti13oaNtx13oaNtyFzp2t6Nty13o2N2Oti13oaNtx11o2d2Oth13oaNty13o2N2Oti13oaNtx11o2d2Oth13oaNty13o2N2Oti13oaNtx11o2d2Oth13oaNty13o2N2Oti13oaNtx11o2d2Oth13oaNty13o2N2Oti13oWN3O9q23IWOth13oaNty13o2N2Oti13oWN3O9q23IWOth13oaNty13oPGV37557GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABezt2tBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgene3HgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA693degAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAArnd36wEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAuN7drQcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4Hp3tx4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgOvd3XoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAK53d+sBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALje3a0HAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOB6d7ceAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDr3d16AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACud3frAQAAAAAAAAAAAAAAAAAAAAAAAAAAgP+P/ToI/Tu/8zr+/v4mBC9rEXY8zGYK+ydtsCO55D89rwhO2cP0IhLBg4xYENaDiNDTsKyX4Hm8VFgEQYIn04NkT4IiLtksS4UslJl0xEl6sIgeTczw8bCZIf03m6Tf5rk/knk8IND/5Dv/+fLsmxe/HwAAAAAAAAAAAAAAAAAAAAAAAAAAAADsdzj2CwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACw3+HYLwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAfodjvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+x2O/QIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7Hc49gsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsN/h2C8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwH6HY78AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPsdjv0CAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOx3OPYLAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALDf4dgvAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMB+hxd5aNu272zb9uNt2z7Ztu37T/n7r2/b9h+3bfuTbdv+27Ztv/3yX/X1dPPmzbl06dJcvHhxrl279gt/r+2vRt+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o+wpaaz3zz8y8MTN3Z+ZkZs7PzI9m5ltnnvnBzPyjx//7WzPz35/3e69cubK+6h49erROTk7W3bt314MHD9bly5fXnTt3vvz7mbmt7X7P6jszt5fb3U3bjl3oVG2Xvmstu1CyCx1tW3ah43Y72rbsQkfbjl3o+I7Wsgsdu9DRtmUXOm63o23LLnS07diFju9oLbvQsQsdu9CyCx270NG2ZRc62nbsQsdnsZZd6NiFjl1o2YWOXeho27ILHW07dqHldjvaduxCx3e0ll3o2IWOti270NG2YxdabrejbccudHxHa9mFjl3oaNuyCx1tO3ah5XY72nbsQsd3tJZd6NiFjrYtu9Bxux1tW3aho23HLnR8R2vZhY5d6Gjbsgsdt9vRtmUXOtp27ELHd7SWXejYhY62LbvQcbsdbVt2oaNtxy50fEdr2YWOXeho27ILHbfb0bZlFzraduxCx3e0ll3o2IWOXWjZhY5d6GjbsgsdbTt2oeOzWMsudOxCxy607ELHLnS0bdmFjrYdu3A8X9zunj+Heb5vz8wna62frLUezsz1mfnumWfWzPzlx//7azPz0xf4vV95t27dmosXL87JycmcP39+rl69Ojdu3Dj7mLY76dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tX02HF3jmN2bmsyd+vvf4nz3pd2fm723bdm9m/sPM/OOX8navufv378/bb7/95c8XLlyY+/fvn33sd0fbXfTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOtq+mwws8sz3ln60zP//dmfnXa60LM/PbM/Nvtm37hd+9bdv3tm27vW3b7Z/97Ge//Nu+ZtY6m3Fm234ht7Y76dvRtqNt52W2ffzv6vsEt9vRtqNtS9+Oth1tW/p2tO1o2/EdreV2O9p2tG3p29G2o21L3462HW07vqO13G5H245daLndjrYdbVv6drTtaNvxWazldjvaduxCy+12tO1o29K3o21H25a+HW072nZ8R2u53Y62HW1b+na07Wjb0rejbUfbju9oLbfb0bajbUvfjrYdbVv6drTtaNvxHa3ldjvadrRt6dvRtqNtS9+Oth1tO76jtdxuR9uOti19O9p2tG3p29G2o23Hd7SW2+1o29G2pW9H2462LX072na07fiO1nK7HW072rb07Wjb0balb0fbjrYd39FabrejbccutNxuR9uOti19O9p2tO34LNZyux1tO3ah5XY72na0benb0baj7avpqR/Wzrg3M28/8fOFmfnpmWf+wcz8u5mZtdZ/nZm/NDO/fvYXrbV+sNY6XWudvvnmm/ve+DVy4cKF+eyzz778+d69e/PWW2+dfUzbnfTtaNvRtvMy2z7+e32f4HY72na0benb0bajbUvfjrYdbTu+o7XcbkfbjrYtfTvadrRt6dvRtqNtx3e0ltvtaNuxCy2329G2o21L3462HW07Pou13G5H245daLndjrYdbVv6drTtaNvSt6NtR9uO72gtt9vRtqNtS9+Oth1tW/p2tO1o2/EdreV2O9p2tG3p29G2o21L3462HW07vqO13G5H2462LX072na0benb0bajbcd3tJbb7Wjb0balb0fbjrYtfTvadrTt+I7WcrsdbTvatvTtaNvRtqVvR9uOth3f0Vput6NtR9uWvh1tO9q29O1o29G24ztay+12tO3YhZbb7Wjb0balb0fbjrYdn8VabrejbccutNxuR9uOti19O9p2tH01HV7gmT+amW9s2/ab27adn5mrM/PDM8/8j5n5mzMz27b9tfmz/2N/9jJf9HX07rvvzscffzyffvrpPHz4cK5fvz7vv//+2ce03UnfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaPtqOve8B9Zaj7Zt+52Z+YOZeWNmfn+tdWfbtt+bmdtrrR/OzD+dmX+1bds/mZk1M39/rbXKF38dnDt3bj766KN577335vPPP58PPvhg3nnnnfnwww/n9PT0i8e03elZfWfma48f03cHbTt2oaNtyy503G5H25Zd6LjdjrYtu9DRtmMXOtq27ELH7Xa0bdmFjtvtaNuyCx1tO3aho23LLnTcbkfbll3ouN2Oti270NG2Yxc62rbsQsftdrRt2YWO2+1o27ILHW07dqHldjvaduxCR9uWXei43Y62LbvQ0bZjF1put6Ntxy50tG3ZhY7b7WjbsgsdbTt2oeV2O9p27EJH25Zd6LjdjrYtu9Bxux1tW3aho23HLnS0bdmFjtvtaNuyCx2329G2ZRc62nbsQkfbll3ouN2Oti270HG7HW1bdqGjbccudLRt2YWO2+1o27ILHbfb0bZlFzraduxCR9uWXei43Y62LbvQcbsdbVt2oaNtxy50tG3ZhY7b7Wjbsgsdt9vRtmUXOtp27MKraTtW/9PT03X79u2j/LdfFdu2/fFa6/T5T/48bZ9vb9sZfZ9H25Zd6LjdjrYtu9DRtmMXWm63o23HLnS0bdmFjtvtaNuyCx1tO3ah5XY72nbsQkfbll3ouN2Oti270HG7HW1bdqGjbccudLRt2YWO2+1o27ILHbfb0bZlFzraduxCR9uWXei43Y62LbvQcbsdbVt2oaNtxy50tG3ZhY7b7Wjbsgsdt9vRtmUXOtp27EJH25Zd6LjdjrYtu9Bxux1tW3aho23HLrTcbkfbjl3oaNuyCx2329G2ZRc62nbsQsvtdrTt2IWOti270HG7HW1bdqGjbccutNxuR9uOXeho27ILHbfb0bZlFzraduxCy+12tO3YhY62LbvQcbsdbVt2oeN2O9q27EJH245d6Gjbsgsdt9vRtmUXOm63o23LLnS07diFjrYtu9D5VW738LJfBgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIC/OIdjvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+x2O/QIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7Hc49gsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsN/h2C8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwH6HY78AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPsdjv0CAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOx3OPYLAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALDf4dgvAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMB+h2O/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD7HY79AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADsdzj2CwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACw3+HYLwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAfodjvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+x2O/QIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7Hc49gsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsN/h2C8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwH6HY78AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPsdjv0CAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOx3OPYLAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALDf4dgvAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMB+h2O/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD7HY79AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADsd3iRh7Zt+862bT/etu2Tbdu+/+c883e2bfvTbdvubNv2b1/ua76+bt68OZcuXZqLFy/OtWvXnvqMtvto29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW1fQWutZ/6ZmTdm5u7MnMzM+Zn50cx868wz35iZP5mZv/L457/6vN975cqV9VX36NGjdXJysu7evbsePHiwLl++vO7cufPl38/MbW33qdoufddaz+47M7eXXdjNLnTsQssudOxCR9uWXei43Y62HZ/FWja3Yxc6dqFlFzp2oaNtyy503G5H247PYi2b27ELHbvQsgsdu9CxCy270LELHW07Nrdlczt2oWMXWnahYxc6dqFlFzp2oaNtx+a2bG7HLnTsQssudOxCxy607ELHLnS07djcls3t2IWOXWjZhY5d6NiFll3o2IWOth2b27K5HbvQsQstu9CxCx270LILHbvQ0bZlFzput6Ntx2exls3t2IWOXWjZhY5d6Gjbsgsdt9vRtuOzWMvmduxCxy607ELHLnS0bdmFjtvtaNvxWaxlczt2oWMXWnahYxc62rbsQsftdrTt+CzWsrkdu9CxCy270LELHbvQsgsdu9DRtmNzWza3Yxc6dqFlFzp2oWMXWnahYxc62nZsbsvmduzC8Xxxu3v+HOb5vj0zn6y1frLWejgz12fmu2ee+Ycz8y/XWv97Zmat9T9f4Pd+5d26dWsuXrw4Jycnc/78+bl69ercuHHj7GPa7qBtS9+Oth1tW/p2tO1o29K3cgBAxQAAIABJREFUo21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tH01HV7gmd+Ymc+e+Pne43/2pG/OzDe3bfsv27b94bZt33lZL/g6u3///rz99ttf/nzhwoW5f//+2ce03UHblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaPtqOvcCz2xP+WfrKb/nGzPzWzNzYWb+87Ztf32t9X9+7hdt2/dm5nszM1//+td/6Zd93ax1NuPMtv1Cbm13eJltH/+7+j7B7Xa07diFltvtaNvRtqVvR9uOth2fxVput6Ntxy603G5H2462LX072na07fgs1nK7HW07dqHldjvaduxCy+12tO1o27G5Lbfb0bZjF1put6Ntxy603G5H2462HZvbcrsdbTt2oeV2O9p27ELL7Xa07Wjbsbktt9vRtmMXWm63o23HLrTcbkfbjrYdm9tyux1tO3ah5XY72nbsQsvtdrTtaNvSt6NtR9uOz2Itt9vRtmMXWm63o21H25a+HW072nZ8Fmu53Y62HbvQcrsdbTvatvTtaNvRtuOzWMvtdrTt2IWW2+1o29G2pW9H2462HZ/FWm63o23HLrTcbkfbjl1oud2Oth1tOza35XY72nbsQsvtdrTt2IWW2+1o29G2Y3Nbbrej7avp8ALP3JuZt5/4+cLM/PQpz9xYa/2/tdanM/Pj+bP/o3/OWusHa63Ttdbpm2++ufedXxsXLlyYzz777Muf7927N2+99dbZx7Td4WW2ndH3LLfb0bZjF1put6NtR9uWvh1tO9p2fBZrud2Oth270HK7HW072rb07Wjb0bbjs1jL7Xa07diFltvtaNuxCy2329G2o23H5rbcbkfbjl1oud2Oth270HK7HW072nZsbsvtdrTt2IWW2+1o27ELLbfb0bajbcfmttxuR9uOXWi53Y62HbvQcrsdbTvadmxuy+12tO3YhZbb7WjbsQstt9vRtqNtS9+Oth1tOz6LtdxuR9uOXWi53Y62HW1b+na07Wjb8Vms5XY72nbsQsvtdrTtaNvSt6NtR9uOz2Itt9vRtmMXWm63o21H25a+HW072nZ8Fmu53Y62HbvQcrsdbTt2oeV2O9p2tO3Y3Jbb7WjbsQstt9vRtmMXWm63o21H247NbbndjravpsMLPPNHM/ONbdt+c9u28zNzdWZ+eOaZfz8zf2NmZtu2X5+Zb87MT17mi76O3n333fn444/n008/nYcPH87169fn/fffP/uYtjto29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW1fTeee98Ba69G2bb8zM38wM2/MzO+vte5s2/Z7M3N7rfXDx3/3t7Zt+9OZ+Xxm/tla63+VL/46OHfu3Hz00Ufz3nvvzeeffz4ffPDBvPPOO/Phhx/O6enpF49pu4O2rWf1nZmvPX5M3x3cbkfbll3ouN2Oti270HG7HW072rZsbsftdrRt2YWO2+1o27ILHbfb0bajbcvmdtxuR9uWXei43Y62LbvQcbsdbTvatmxux+12tG3ZhY7b7Wjbsgsdt9vRtqNty+Z23G5H25Zd6LjdjrYtu9Bxux1tO9q2bG7H7Xa0bdmFjtvtaNuyCx2329G2o23L5nbcbkfbll3ouN2Oti270HG7HW1bdqHjdjvadrRt2dyO2+1o27ILHbfb0bZlFzput6NtR9uWze243Y62LbvQcbsdbVt2oeN2O9p2tG3Z3I7b7Wjbsgsdt9vRtmUXOm63o21H25bN7bjdjrYtu9Bxux1tW3ah43Y72na0bdncjtvtaNuyCx2329G2ZRc6brejbUfbls3tuN1X07bWOsp/+PT0dN2+ffso/+1XxbZtf7zWOn3+kz9P2+fb23ZG3+fRtmUXOm63o23LLnS07diFltvtaNuxCx1tW3ah43Y72rbsQkfbjl1oud2Oth270NG2ZRc6brejbcsudNxuR9uWXeho27ELHW1bdqHjdjvatuxCx+12tG3ZhY62HbvQ0bZlFzput6Ntyy503G5H25Zd6GjbsQsdbVt2oeN2O9q27ELH7Xa0bdmFjrYdu9DRtmUXOm63o23LLnTcbkfbll3oaNuxCy2329G2Yxc62rbsQsftdrRt2YWOth270HK7HW07dqGjbcsudNxuR9uWXeho27ELLbfb0bZjFzratuxCx+12tG3ZhY62HbvQcrsdbTt2oaNtyy503G5H25Zd6LjdjrYtu9DRtmMXOtq27ELH7Xa0bdmFjtvtaNuyCx1tO3aho23LLnR+lds9vOyXAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOAvzuHYLwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAfodjvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+x2O/QIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7Hc49gsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsN/h2C8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwH6HY78AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPsdjv0CAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOx3OPYLAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALDf4dgvAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMB+h2O/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD7HY79AgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADsdzj2CwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACw3+HYLwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAfodjvwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+x2O/QIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA7Hc49gsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsN/h2C8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwH6HY78AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPsdjv0CAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOx3OPYLAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALDf4dgvAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMB+h2O/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD7HV7koW3bvrNt24+3bftk27bvP+O5v71t29q27fTlveLr7ebNm3Pp0qW5ePHiXLt27c99Ttt99O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462r6C11jP/zMwbM3N3Zk5m5vzM/GhmvvWU535tZv7TzPzhzJw+7/deuXJlfdU9evRonZycrLt3764HDx6sy5cvrzt37nz59zNze2m727P6ftF26buLth270KnaLn3XWnahZBc62rbsQsftdrRt2YWOth270PEdrWUXOnaho23LLnTcbkfbll3oaNuxCx3f0Vp2oWMXOnahZRc6dqGjbcsudLTt2IWOz2Itu9CxCx270LILHbvQ0bZlFzraduxCy+12tO3YhY7vaC270LELHW1bdqGjbccutNxuR9uOXej4jtayCx270NG2ZRc62nbsQsvtdrTt2IWO72gtu9CxCx1tW3ah43Y72rbsQkfbjl3o+I7Wsgsdu9DRtmUXOm63o23LLnS07diFju9oLbvQsQsdbVt2oeN2O9q27EJH245d6PiO1rILHbvQ0bZlFzput6Ntyy50tO3YhY7vaC270LELHbvQsgsdu9DRtmUXOtp27ELHZ7GWXejYhY5daNmFjl3oaNuyCx1tO3bheJ683V/2z2Ge79sz88la6ydrrYczc31mvvuU5/75zPyLmfm/L/A7mZlbt27NxYsX5+TkZM6fPz9Xr16dGzduPO1RbXfQt6NtR9uOti19O9p2tG3p29G2o21L3462HW07/5/9+gn1Pb/vOv7+/nKZghqLf7JJ78TOZcpApwzUuRS7ElGYrNqNwiwUJZaA0m50kyJk0VXAhSBZSMGAuGnQTYJgslC7tPEWTJkRCplBO3O7MCq4EW4z4eui5wyTm+m9N797n/l9zvc+HnDgnnt+c+7n+8wnL34/bVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW1vptMTvOanZua9j3z//tXffWjbtp+fmRf3ff93z/Bsh3f//v158cUXP/z+9u3bc//+/R94jbbn07ejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTva3kynJ3jN9jF/t3/4w207zcw/m5l//NhftG2f37bt3rZt97773e8++SkPat/3H/q7bfuh3Nqe6XF93d3zaduxC51n2fbqv9X3I+xCxy50tG3ZhY6729G2ZRc62nbsQsdntJZd6NiFjrYtu9BxdzvatuxCR9uOXej4jNayCx270LELLbvQsQsdbVt2oaNtxy50vBdr2YWOXejYhZZd6NiFjrYtu9DRtmMXWu5uR9uOXej4jNayCx270NG2ZRc62nbsQsvd7WjbsQsdn9FadqFjFzratuxCR9uOXWi5ux1tO3ah4zNayy507EJH25Zd6Li7HW1bdqGjbccudHxGa9mFjl3oaNuyCx13t6Ntyy50tO3YhY7PaC270LELHW1bdqHj7na0bdmFjrYdu9DxGa1lFzp2oaNtyy503N2Oti270NG2Yxc6PqO17ELHLnTsQssudOxCR9uWXeho27ELHe/FWnahYxc6dqFlFzp2oaNtyy50tO3YhZvp9ASveX9mXvzI97dn5g8/8v0nZ+bnZua3t2377zPzV2bm69u23X34F+37/pv7vt/d9/3upz71qfNPfRC3b9+e995778Pv33///fn0pz/90Zd8YrQ92xP0dXfPpG3HLnSeZdsZfR9mFzp2oaNtyy503N2Oti270NG2Yxc6PqO17ELHLnS0bdmFjrvb0bZlFzraduxCx2e0ll3o2IWOXWjZhY5d6GjbsgsdbTt2oeO9WMsudOxCxy607ELHLnS0bdmFjrYdu9BydzvaduxCx2e0ll3o2IWOti270NG2Yxda7m5H245d6PiM1rILHbvQ0bZlFzraduxCy93taNuxCx2f0Vp2oWMXOtq27ELH3e1o27ILHW07dqHjM1rLLnTsQkfbll3ouLsdbVt2oaNtxy50fEZr2YWOXeho27ILHXe3o23LLnS07diFjs9oLbvQsQsdbVt2oePudrRt2YWOth270PEZrWUXOnahYxdadqFjFzratuxCR9uOXeh4L9ayCx270LELLbvQsQsdbVt2oaNtxy7cUPu+P/JrZm7NzLsz89LMvDAz356ZVx/x+t+embuP+72vv/76/rz73ve+t7/00kv7u+++uz948GB/7bXX9rfeeuvDn8/MvV3bsz2q78Ntd31/JNp27EKnarvru++7XSjZhY62LbvQcXc72rbsQkfbjl3o+IzWsgsdu9DRtmUXOu5uR9uWXeho27ELHZ/RWnahYxc6dqFlFzp2oaNtyy50tO3YhY73Yi270LELHbvQsgsdu9DRtmUXOtp27ELL3e1o27ELHZ/RWnahYxc62rbsQkfbjl1oubsdbTt2oeMzWssudOxCR9uWXeho27ELLXe3o23HLnR8RmvZhY5d6Gjbsgsdd7ejbcsudLTt2IWOz2gtu9CxCx1tW3ah4+52tG3ZhY62HbvQ8RmtZRc6dqGjbcsudNzdjrYtu9DRtmMXOj6jtexCxy50tG3ZhY6729G2ZRc62nbsQsdntJZd6NiFjl1o2YWOXeho27ILHW07dqHjvVjLLnTsQscutOxCxy50tG3ZhY62HbtwOR93d5/069Y8xr7vH2zb9qsz882Z+cTMfGXf97e3bfuNq3/464/7HXy8W7duzZe//OV544035vvf//587nOfm1dffXW++MUvzt27dy99vBvvUX1n5icvfb6bTNuOXeho27ILHXe3o23LLnTc3Y62LbvQ0bZjFzratuxCx93taNuyCx13t6Ntyy50tO3YhY62LbvQcXc72rbsQsfd7WjbsgsdbTt2oaNtyy503N2Oti270HF3O9q27EJH245daLm7HW07dqGjbcsudNzdjrYtu9DRtmMXWu5uR9uOXeho27ILHXe3o23LLnS07diFlrvb0bZjFzratuxCx93taNuyCx13t6Ntyy50tO3YhY62LbvQcXc72rbsQsfd7WjbsgsdbTt2oaNtyy503N2Oti270HF3O9q27EJH245d6Gjbsgsdd7ejbcsudNzdjrYtu9DRtmMXOtq27ELH3e1o27ILHXe3o23LLnS07diFjrYtu9BxdzvatuxCx93taNuyCx1tO3bhZtr2fb/IP3z37t393r17F/m3b4pt23533/cf+f892j7euW1n9H0cbVt2oePudrRt2YWOth270HJ3O9p27EJH25Zd6Li7HW1bdqGjbccutNzdjrYdu9DRtmUXOu5uR9uWXei4ux1tW3aho23HLnS0bdmFjrvb0bZlFzrubkfbll3oaNuxCx1tW3ah4+52tG3ZhY6729G2ZRc62nbsQkfbll3ouLsdbVt2oePudrRt2YWOth270NG2ZRc67m5H25Zd6Li7HW1bdqGjbccutNzdjrYdu9DRtmUXOu5uR9uWXeho27ELLXe3o23HLnS0bdmFjrvb0bZlFzraduxCy93taNuxCx1tW3ah4+52tG3ZhY62HbvQcnc72nbsQkfbll3ouLsdbVt2oePudrRt2YWOth270NG2ZRc67m5H25Zd6Li7HW1bdqGjbccudLRt2YXO09zd07M+DAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/PqdLHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA850ufQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzne69AEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAON/p0gcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4HynSx8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgPOdLn0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM53uvQBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADjf6dIHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOB8p0sfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDznS59AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADOd7r0AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA43+nSBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgfKdLHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA850ufQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzne69AEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAON/p0gcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4HynSx8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgPOdLn0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM53uvQBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADjf6dIHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOB8p0sfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDznS59AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADOd3qSF23b9tlt235/27bvbNv2hY/5+T/atu2/bdv2e9u2/Ydt2/7Ssz/qMX3jG9+YV155ZV5++eX50pe+9EM/1/Z82rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+VzHQfAAAgAElEQVSOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbG2jf90d+zcwnZuadmbkzMy/MzLdn5mcfes1fm5k/dfXnfzAzX33c73399df3590HH3yw37lzZ3/nnXf2Bw8e7K+99tr+9ttvf/jzmbmn7Xmqtru++74/uu/M3NvtwtnsQscutOxCxy50tG3ZhY6729G2471Yy+Z27ELHLrTsQscudLRt2YWOu9vRtuO9WMvmduxCxy607ELHLnTsQssudOxCR9uOzW3Z3I5d6NiFll3o2IWOXWjZhY5d6Gjbsbktm9uxCx270LILHbvQsQstu9CxCx1tOza3ZXM7dqFjF1p2oWMXOnahZRc6dqGjbcfmtmxuxy507ELLLnTsQscutOxCxy50tG3ZhY6729G2471Yy+Z27ELHLrTsQscudLRt2YWOu9vRtuO9WMvmduxCxy607ELHLnS0bdmFjrvb0bbjvVjL5nbsQscutOxCxy50tG3ZhY6729G2471Yy+Z27ELHLrTsQscudOxCyy507EJH247Nbdncjl3o2IWWXejYhY5daNmFjl3oaNuxuS2b27ELl3N9d8/5Os3j/cLMfGff93f3ff+jmfmtmfnlj75g3/f/tO/7/7v69j/PzO0n+L3PvW9961vz8ssvz507d+aFF16YN998c772ta/9wGu0PY+2LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0fZmOj3Ba35qZt77yPfvX/3dn+Tvz8y/f5pDPS/u378/L7744off3759e+7fv/+o/0TbJ6RtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tL2Zbj3Ba7aP+bv9Y1+4bX97Zu7OzF/9E37++Zn5/MzMZz7zmSc84nHt+w9n3LaPy63tj+pZtr16jb4f4e52tO3YhZa729G2o21L3462HW073ou13N2Oth270HJ3O9p2tG3p29G2o23He7GWu9vRtmMXWu5uR9uOXWi5ux1tO9p2bG7L3e1o27ELLXe3o23HLrTc3Y62HW07Nrfl7na07diFlrvb0bZjF1rubkfbjrYdm9tydzvaduxCy93taNuxCy13t6NtR9uOzW25ux1tO3ah5e52tO3YhZa729G2o21L3462HW073ou13N2Oth270HJ3O9p2tG3p29G2o23He7GWu9vRtmMXWu5uR9uOti19O9p2tO14L9ZydzvaduxCy93taNvRtqVvR9uOth3vxVrubkfbjl1oubsdbTt2oeXudrTtaNuxuS13t6Ntxy603N2Oth270HJ3O9p2tO3Y3Ja729H2Zjo9wWven5kXP/L97Zn5w4dftG3b35iZfzIzv7Tv+4OP+0X7vv/mvu93932/+6lPfeqc8x7K7du357333vvw+/fff38+/elP/9DrtP3RPcu2M/o+zN3taNuxCy13t6NtR9uWvh1tO9p2vBdrubsdbTt2oeXudrTtaNvSt6NtR9uO92Itd7ejbccutNzdjrYdu9BydzvadrTt2NyWu9vRtmMXWu5uR9uOXWi5ux1tO9p2bG7L3e1o27ELLXe3o23HLrTc3Y62HW07Nrfl7na07diFlrvb0bZjF1rubkfbjrYdm9tydzvaduxCy93taNuxCy13t6NtR9uWvh1tO9p2vBdrubsdbTt2oeXudrTtaNvSt6NtR9uO92Itd7ejbccutNzdjrYdbVv6drTtaNvxXqzl7na07diFlrvb0bajbUvfjrYdbTvei7Xc3Y62HbvQcnc72nbsQsvd7Wjb0bZjc1vubkfbjl1oubsdbTt2oeXudrTtaNuxuS13t6PtDbXv+yO/ZubWzLw7My/NzAsz8+2ZefWh1/z8zLwzMz/zuN93/fX666/vz7vvfe97+0svvbS/++67+4MHD/bXXnttf+uttz78+czc0/Y8Vdtd333fH913Zu7tduFsdqFjF1p2oWMXOtq27ELH3e1o2/FerGVzO3ahYxdadqFjFzratuxCx93taNvxXqxlczt2oWMXWnahYxc6dqFlFzp2oaNtx+a2bG7HLnTsQssudOxCxy607ELHLnS07djcls3t2IWOXWjZhY5d6NiFll3o2IWOth2b27K5HbvQsQstu9CxCx270LILHbvQ0bZjc1s2t2MXOnahZRc6dqFjF1p2oWMXOtq27ELH3e1o2/FerGVzO3ahYxdadqFjFzratuxCx93taNvxXqxlczt2oWMXWnahYxc62rbsQsfd7Wjb8V6sZXM7dqFjF1p2oWMXOtq27ELH3e1o2/FerGVzO3ahYxdadqFjFzp2oWUXOnaho23H5rZsbscudOxCyy507ELHLrTsQscudLTt2NyWze3Yhcu5vrvnfN2ax9j3/YNt2351Zr45M5+Yma/s+/72tm2/cfUPf31m/unM/JmZ+Tfbts3M/MG+77/0uN/9vLt169Z8+ctfnjfeeGO+//3vz+c+97l59dVX54tf/OLcvXv3+mXankHb1qP6zsxPXr1M3zO4ux1tW3ah4+52tG3ZhY6729G2o23L5nbc3Y62LbvQcXc72rbsQsfd7Wjb0bZlczvubkfbll3ouLsdbVt2oePudrTtaNuyuR13t6Ntyy503N2Oti270HF3O9p2tG3Z3I6729G2ZRc67m5H25Zd6Li7HW072rZsbsfd7Wjbsgsdd7ejbcsudNzdjrYdbVs2t+PudrRt2YWOu9vRtmUXOu5uR9uWXei4ux1tO9q2bG7H3e1o27ILHXe3o23LLnTc3Y62HW1bNrfj7na0bdmFjrvb0bZlFzrubkfbjrYtm9txdzvatuxCx93taNuyCx13t6NtR9uWze24ux1tW3ah4+52tG3ZhY6729G2o23L5nbc3Y62LbvQcXc72rbsQsfd7Wjb0bZlczvu7s207ft+kX/47t27+7179y7yb98U27b97r7vdx//yh+k7eOd23ZG38fRtmUXOu5uR9uWXeho27ELLXe3o23HLnS0bdmFjrvb0bZlFzraduxCy93taNuxCx1tW3ah4+52tG3ZhY6729G2ZRc62nbsQkfbll3ouLsdbVt2oePudrRt2YWOth270NG2ZRc67m5H25Zd6Li7HW1bdqGjbccudLRt2YWOu9vRtmUXOu5uR9uWXeho27ELHW1bdqHj7na0bdmFjrvb0bZlFzraduxCy93taNuxCx1tW3ah4+52tG3ZhY62HbvQcnc72nbsQkfbll3ouLsdbVt2oaNtxy603N2Oth270NG2ZRc67m5H25Zd6GjbsQstd7ejbccudLRt2YWOu9vRtmUXOu5uR9uWXeho27ELHW1bdqHj7na0bdmFjrvb0bZlFzraduxCR9uWXeg8zd09PevDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPDjc7r0AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA43+nSBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgfKdLHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA850ufQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzne69AEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAON/p0gcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4HynSx8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgPOdLn0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM53uvQBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADjf6dIHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOB8p0sfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDznS59AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADOd7r0AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA43+nSBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgfKdLHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA850ufQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzne69AEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAON/p0gcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4HynSx8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgPOdLn0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM53uvQBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADjf6dIHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOB8pyd50bZtn9227fe3bfvOtm1f+Jif/8S2bV+9+vnvbNv208/6oEf1jW98Y1555ZV5+eWX50tf+tIP/Vzbp6NvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tL2B9n1/5NfMfGJm3pmZOzPzwsx8e2Z+9qHX/MOZ+RdXf35zZr76uN/7+uuv78+7Dz74YL9z587+zjvv7A8ePNhfe+21/e233/7w5zNzT9vzParvzNzb3d2zaduxC52q7a7vvu92oWQXOtq27ELH3e1o27ILHW07dqHjM1rLLnTsQkfbll3ouLsdbVt2oaNtxy50fEZr2YWOXejYhZZd6NiFjrYtu9DRtmMXOt6LtexCxy507ELLLnTsQkfbll3oaNuxCy13t6Ntxy50fEZr2YWOXeho27ILHW07dqHl7na07diFjs9oLbvQsQsdbVt2oaNtxy603N2Oth270PEZrWUXOnaho23LLnTc3Y62LbvQ0bZjFzo+o7XsQscudLRt2YWOu9vRtmUXOtp27ELHZ7SWXejYhY62LbvQcXc72rbsQkfbjl3o+IzWsgsdu9DRtmUXOu5uR9uWXeho27ELHZ/RWnahYxc6dqFlFzp2oaNtyy50tO3YhY73Yi270LELHbvQsgsdu9DRtmUXOtp27MLlXN/dc75O83i/MDPf2ff93X3f/2hmfmtmfvmh1/zyzPyrqz//25n569u2bU/wu59r3/rWt+bll1+eO3fuzAsvvDBvvvnmfO1rX3v4ZdqeSd+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1oezOdnuA1PzUz733k+/ev/u5jX7Pv+wcz839n5i88iwMe2f379+fFF1/88Pvbt2/P/fv3H36ZtmfSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9reTNu+749+wbb9rZl5Y9/3X7n6/u/MzC/s+/5rH3nN21evef/q+3euXvO/H/pdn5+Zz199+3Mz89azepBn4C/OzP/6Mf+bf25m/uzM/I+r7//8zPzpufo/ycy8MjN/MDe/7cx6fV/Z9/2T7u7ZtO3Yhc4za3v1s5X7rnZ37cLTsQsdbVt2ofO83F1tW6v1tQtPR9uOXej4jNayCx270NG2ZRc6z8vd1ba1Wl+78HS07diFjs9oLbvQsQsdu9CyC53nZRe0ba3W1y48HW07dqHjvVjLLnTsQscutOxC53nZBW1bq/W1C09H245daLm7HW07dqHjM1rLLnTsQkfbll3oaNuxCy13t6Ntxy50fEZr2YWOXeho27ILHW07dqHl7na07diFjs9oLbvQsQsdbVt2ofO83F1tW6v1tQtPR9uOXej4jNayCx270NG2ZRc6z8vd1ba1Wl+78HS07diFjs9oLbvQsQsdbVt2ofO83F1tW6v1tQtPR9uOXej4jNayCx270NG2ZRc6z8vd1ba1Wl+78HS07diFjs9oLbvQsQsdu9CyC53nZRe0ba3W1y48HW07dqHjvVjLLnTsQscutOxC53nZBW1bq/W1C09H245duJxX9n3/5Fn/5b7vj/yamV+cmW9+5Ptfn5lff+g135yZX7z686354zjbY37vvcf92z/Or0uc53FtZ+beEdqu2Pf6PEfoq+3z0/b6TEdoe4kzVW1X7Lva3bULXdvrMx2h7SXOpO3l+tqFru31mbTV9qb1tQva3sS212c6QttLnKlqu2Lf1e6uXejaXp/pCG0vcSZtL9fXLnRtr8+krbY3ra9d0PYmtr0+0xHaXuJMVdsV+652d+1C1/b6TEdoe4kz2YXL9bULXdvrM2mr7U3raxe0vYltr890hLaXOFPVdsW+q91du9C1vT7TEdpe4kx24XJ97ULX9vpM2mp70/raBW1vYtvrMx2h7Yp93V1tb2Lb6zMdoe0lzlS1XbHvanfXLnRtr890hLaXOJO2l+trF7S9iW2vz3SEtiv2dXe1vYltr890hLaXOFPVdsW+q91du9C1vT7TEdpe4kzaXq6vXdD2Jra9PtMR2q7Y193V9ia2vT7TEdpe4kxV2xX7rnZ37ULX9vpMR2h7iTNpe7m+dqFre30mbbW9aX3tgrY3se31mY7Q9hJnqtqu2He1u2sXurbXZzpC20ucSdvL9bULXdvrM2mr7U3raxe0vYltr890hLaXOFPVdsW+q91du9C1vT7TEdpe4kzaXq6vXejaXp9JW21vWl+7oO1NbHt9piO0vcSZqrYr9l3t7tqFru31mY7Q9hJn0vZyfe1C1/b6TNpqe9P62gVtb2Lb6zMdoe0lzlS1XbHvanfXLnRtr890hLaXOJNduFxfu9C1vT6TttretL52Qdub2Pb6TEdoe4kzVW1X7Lva3bULXdvrMx2h7SXOZBcu19cudG2vz6Sttjetr13Q9ia2vT7TEdqueKanOc9pHu+/zMzPbNv20rZtL8zMmzPz9Yde8/WZ+btXf/6bM/Mf96uT8UjatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOtjfQrce9YN/3D7Zt+9WZ+ebMfGJmvrLv+9vbtv3GzNzb9/3rM/MvZ+Zfb9v2nZn5P/PH/+PzGI9re/Uybc/0mL4/efUyfc+gbccudLRt2YWOu9vRtmUXOu5uR9uWXeho27ELHW1bdqHj7na0bdmFjrvb0bZlFzraduxCR9uWXei4ux1tW3ah4+52tG3ZhY62HbvQ0bZlFzrubkfbll3ouLsdbVt2oaNtxy603N2Oth270NG2ZRc67m5H25Zd6GjbsQstd7ejbccudLRt2YWOu9vRtmUXOtp27ELL3e1o27ELHW1bdqHj7na0bdmFjrvb0bZlFzraduxCR9uWXei4ux1tW3ah4+52tG3ZhY62HbvQ0bZlFzrubkfbll3ouLsdbVt2oaNtxy50tG3ZhY6729G2ZRc67m5H25Zd6GjbsQsdbVt2oePudrRt2YWOu9vRtmUXOtp27EJH25Zd6Li7HW1bdqHj7na0bdmFjrYdu3BD7ft+ka+Z+fyl/u2bcJ6nOdORnmXF8xzpWVY7z5GeZbUzHelZVjzPkZ5ltfMc6VlWO9ORnmW186z2HCueyS6seabVnmW18xyp7YpnsgtrnudIz7LamY70LCue50jPstp5jvQsq53pSM+y2nlWe44Vz2QX1jzTas+y2nmO1HbFM9mFNc9zpGdZ7UxHepYVz3OkZ1ntPEd6ltXOdKRnWfE8R3qW1c5zpGdZ7UyrPctq5zlS2xXPZBfWPM+RnmW1Mx3pWVY8z5GeZbXzHOlZVjvTkZ5lxfMc6VlWO8+RnmW1M632LKud50htVzyTXVjzPEd6ltXOdKRnWfE8R3qW1c5zpGdZ7UxHepYVz3OkZ1ntPEd6ltXOdKRnWe08qz3HimeyC2ue50jPstqZjvQsK57nSM+y2nmO9CyrnelIz7LieY70LKud50jPstqZjvQsq51ntedY8Ux2Yc3zHOlZVjvTkZ5lxfMc6VlWO8+RnmW1Mx3pWVY8z5GeZbXzHOlZVjvTkZ5ltfOs9hwrnskurHmm1Z5ltfMcqe2KZ7ILa57nSM+y2pmO9CwrnudIz7LaeY70LKud6UjPstp5VnuOFc9kF9Y802rPstp5jtR2xTPZhTXPc6RnWe1MR3qWFc9zpGdZ7TxHepbVznSkZ1ntPKs9x4pnsgtrnmm1Z1ntPEdqu+KZ7MKa5znSs6x2piM9y4rnOdKzrHaeIz3Lamc60rOsdp7VnmPFM9mFNc+02rOsdp4jtV3xTHZhzfMc6VlWO9ORnmXF8xzpWVY7z5GeZbUzHelZVjzPkZ5ltfMc6VlWO9Nqz7LaeY7UdsUz2YU1z3OkZ1ntTEd6lhXPc6RnWe08R3qW1c50pGdZ8TxHepbVznOkZ1ntTKs9y2rnOVLbFc9kF9Y8z5GeZbUzHelZVjzPdvULAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALiBTpc+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADnO9X/wLZtn9227fe3bfvOthjCPUEAACAASURBVG1f+Jif/8S2bV+9+vnvbNv20xc+z9/btu2727b916uvX4nP85Vt2/7ntm1v/Qk/37Zt++dX5/29bdv+8o/wLNoepO0TnknfM2nb0balb0fbjratlfo+TdsnfBZ39yB3V9uOti19O9p2tG3p29G2o21rpb5P0/YJn8XdPcjd1bajbUvfjrYdbVv6drTtaNtaqe/TtH3CZ3F3D3J3te1o29K3o21H25a+HW072rZW6vs0bZ/wWdzdg9xdbTvatvTtaNvRtqVvR9uOtq2V+j5N2yd8Fnf3IHdX2462LX072na0benb0bajbWulvk/T9gmfxd09yN3VtrVSX23d3SelbUfblr6d/89+HYTaup+HfX6/bVUEDHWg8cD2VUkuMnYt40F0HDwKhQycaiAN2oEEJU0dMGppB6WTQsGNM6nHwSFpAmElHURtM7EKqQulMZk0Va4odi1K6iurVFc2VKqpJwXLMquDe3L67a27911ee//2Xv+l54ED95yzdb/3/vTy8n3adrRtXVLfx7Q98b/F7l7J7mrbuqS+7oLdPZW2HW1b+na07Wjb0rejbUfb1iX1fUzbE/9b7O6V7K62HW1b+na07Wjb0rejbUfb1iX1fUzbE/9b7O6V7K62HW1b+na07Wjb0rejbUfb1iX1fUzbE/9b7O6V7K62HW1b+na07Wjb0rejbUfb1iX1fUzbE/9b7O6V7K62HW1b+na07Wjb0rejbUfb1iX1fUzbE/9b7O6V7K62HW1b+na07VxT2wcdj8fs18x838x8dWbenpmPzsyvz8xP3PmZf39m/tbrf/7szPxXLzzPX56ZXy673Hnen5+ZPzszv3nP339qZv67mdlm5mdm5n/W9nurrb52V1tt9bW72mq7Yt9z215i30tre027q622K7bV1+5qq62+dldbbVfse27bS+x7aW2vaXe11XbFtvraXW211dfuaqvtin3PbXuJfS+t7TXtrrbarthWX7urrbb62l1ttV2x77ltL7HvpbW9pt3VVtsV2+prd7XVVl+7q622K/Y9t+0l9r20tte0u9pqu2Jbfe2uttrqa3e11XbFvue2vcS+l9b2mnZXW3dBW3211fYa2uprd7XVdoW+57a9xL6X1vaadldbd+GUtpfY99LaXtPuaqvtim31tbvaaquv3dVW2xX7ntv2EvteWttr2l1ttV2xrb52V1tt9bW72mq7Yt9z215i30tre027q622K7bV1+5qq62+dldbbVfse27bS+x7aW2vaXe11XbFtvraXW211dfuaqvtin3PbXuJfS+t7TXtrrbarthWX7urrbb62l1ttV2x77ltL7HvpbW9pt3VVtsV2+prd7V9+bYf9utmWn9uZt49Ho+/fTwevz0zX5iZz9z5mc/MzN97/c//cGb+wrZt2wvO86yOx+M/mZnfe+BHPjMzf//4vn86M39y27YfGm0/1BW1nRNnelZX1FdbbfdWaTsnzvSsrqivttrurdJ2Tpzp2Tyi7czl9b2otjNXtbvaaru3Sts5caZndUV9tdV2b5W2c+JMz+qK+mqr7d4qbefEmZ6Nb7TWFe2uttrurdJ2TpzpWV1RX2213Vul7Zw407O6or7aaru3Sts5caZn4xutdUW7q622e6u0nRNnelZX1FdbbfdWaTsnzvSsrqivttrurdJ2Tpzp2fhGa13R7mqr7d4qbefEmZ7VFfXVVtu9VdrOiTM9qyvqq622e6u0nRNneja+0VpXtLvaaru3Sts5caZndUV9tdV2b5W2c+JMz+qK+mqr7d4qbefEmZ6Nb7TWFe2utu7CnraPcEV9tdV2b5W2c+JMz+qK+mqr7d4qbefEmZ6Nb7TWFe2utu7CnrvwCFe0u9pqu7dK2zlxpmd1RX211XZvlbZz4kzP6or6aqvt3ipt58SZno1vtNYV7a622u6t0nZOnOlZXVFfbbXdW6XtnDjTs7qivtpqu7dK2zlxpmfjG611RburrbZ7q7SdE2d6VlfUV1tt91ZpOyfO9KyuqK+22u6t0nZOnOnZ+EZrXdHuaqvt3ipt58SZntUV9dVW271V2s6JMz2rK+qrrbZ7q7SdE2d6Nr7RWle0u9pqu7dK2zlxpmd1RX211XZvlbZz4kzP6or6aqvt3ipt58SZno1vtNYV7a622u6t0nZOnOlZXVFfbbXde+h94V43TzXgPX5kZr6++/17r//sA3/meDx+Z2Z+f2b+lRecZ2bm39y27Te2bfuH27Z9LJrlVPfNrO3jrdL21vMemGlG36ec8y5tn27Ou7Q9j752d0/bziptbz3vgZlmLqfvQ/NeWt/V2s6ss7vaaru3Sttbz3tgphl9n3LOu7R9ujnv0vY8+trdPW07q7S99bwHZpq5nL6+0Vqr7K622u6t0vbW8x6YaUbfp5zzLm2fbs67tD2PvnZ3T9vOKm1vPe+BmWYup69vtNYqu6uttnurtL31vAdmmtH3Kee8S9unm/Mubc+jr93d07azSttbz3tgppnL6esbrbXK7mqr7d4qbW8974GZZvR9yjnv0vbp5rxL2/Poa3f3tO2s0vbW8x6YaeZy+vpGa62yu9pqu7dK21vPe2CmGX2fcs67tH26Oe/S9jz62t09bTurtL31vAdmmrmcvr7RWqvsrrbuwp62rVX6aqvt3iptbz3vgZlm9H3KOe/S9unmvEvb86zW1zdaa5Xd1dZd2HMXWqvsrrba7q3S9tbzHphpRt+nnPMubZ9uzru0PY++dndP284qbW8974GZZi6nr2+01iq7q622e6u0vfW8B2aa0fcp57xL26eb8y5tz6Ov3d3TtrNK21vPe2Cmmcvp6xuttcruaqvt3iptbz3vgZlm9H3KOe/S9unmvEvb8+hrd/e07azS9tbzHphp5nL6+kZrrbK72mq7t0rbW897YKYZfZ9yzru0fbo579L2PPra3T1tO6u0vfW8B2aauZy+vtFaq+yuttrurdL21vMemGlG36ec8y5tn27Ou7Q9j752d0/bziptbz3vgZlmLqevb7TWKrurrbZ7q7S99bwHZprR9ynnvEvbp5vzrhXaPugmG+d92wf82fGMn3kqpzzrv52ZP308Hn9qZv6Hmfl70Synum9mbR9vlbanPk/f82ir7d4qbU99nr7n0VbbvVXanvq8S+r70LyX1ne1tjPr7K622u6t0vbU5+l7Hm213Vul7anP0/c82mq7t0rbU593SX19o7VW2V1ttd1bpe2pz9P3PNpqu7dK21Ofp+95tNV2b5W2pz7vkvr6Rmutsrvaaru3SttTn6fvebTVdm+Vtqc+T9/zaKvt3iptT33eJfX1jdZaZXe11XZvlbanPk/f82ir7d4qbU99nr7n0VbbvVXanvq8S+rrG621yu5qq+3eKm1PfZ6+59FW271V2p76PH3Po622e6u0PfV5l9TXN1prld3V1l3Y07a1Sl9ttd1bpe2pz9P3PNpqu7dK21Ofd0l9faO1Vtldbd2FPXehtcruaqvt3iptT32evufRVtu9Vdqe+jx9z6OttnurtD31eZfU1zdaa5Xd1VbbvVXanvo8fc+jrbZ7q7Q99Xn6nkdbbfdWaXvq8y6pr2+01iq7q622e6u0PfV5+p5HW233Vml76vP0PY+22u6t0vbU511SX99orVV2V1tt91Zpe+rz9D2PttrurdL21Ofpex5ttd1bpe2pz7ukvr7RWqvsrrba7q3S9tTn6XsebbXdW6Xtqc/T9zzaaru3SttTn3dJfX2jtVbZXW213Vul7anP0/c82mq7d1afm2CQvfdm5mO73781M79z389s2/aRmfmBmfm9l5rneDz+38fj8Q9e//bvzMwno1lOdd/M2j7eKm1vPe++mfR98jnf0PbJ53xD20fR1+7uadtZpe2t590304X1fWjeS+u7WtuZdXZXW233Vml763n3zaTvk8/5hrZPPucb2j6KvnZ3T9vOKm1vPe++mS6sr2+01iq7q622e6u0vfW8+2bS98nnfEPbJ5/zDW0fRV+7u6dtZ5W2t55330wX1tc3WmuV3dVW271V2t563n0z6fvkc76h7ZPP+Ya2j6Kv3d3TtrNK21vPu2+mC+vrG621yu5qq+3eKm1vPe++mfR98jnf0PbJ53xD20fR1+7uadtZpe2t590304X19Y3WWmV3tdV2b5W2t55330z6Pvmcb2j75HO+oe2j6Gt397TtrNL21vPum+nC+vpGa62yu9q6C3vatlbpq622e6u0vfW8+2bS98nnfEPbJ5/zDW0fZbW+vtFaq+yutu7CnrvQWmV3tdV2b5W2t55330z6Pvmcb2j75HO+oe2j6Gt397TtrNL21vPum+nC+vpGa62yu9pqu7dK21vPu28mfZ98zje0ffI539D2UfS1u3vadlZpe+t59810YX19o7VW2V1ttd1bpe2t5903k75PPucb2j75nG9o+yj62t09bTurtL31vPtmurC+vtFaq+yuttrurdL21vPum0nfJ5/zDW2ffM43tH0Ufe3unradVdreet59M11YX99orVV2V1tt91Zpe+t5982k75PP+Ya2Tz7nG9o+ir52d0/bziptbz3vvpkurK9vtNYqu6uttnurtL31vPtm0vfJ53xD2yef842F2j7oJhvnff9sZn5027Y/s23bR2fmszPzxTs/88WZ+Xde//O/NTP/4/F4PL7UPNu2/dDut5+emf8tmuVUX5yZv7S972dm5vePx+PvjrZPYZW2c8pM+p5NW233Vmk7p8yk79m01XZvlbZzykwX1ve+tjOX13e1tjPr7K622u6t0nZOmUnfs2mr7d4qbeeUmfQ9m7ba7q3Sdk6Z6cL6+kZrrbK72mq7t0rbOWUmfc+mrbZ7q7SdU2bS92zaaru3Sts5ZaYL6+sbrbXK7mqr7d4qbeeUmfQ9m7ba7q3Sdk6ZSd+zaavt3ipt55SZLqyvb7TWKrurrbZ7q7SdU2bS92zaaru3Sts5ZSZ9z6attnurtJ1TZrqwvr7RWqvsrrba7q3Sdk6ZSd+zaavt3ipt55SZ9D2bttrurdJ2Tpnpwvr6Rmutsrvaugt72rZW6auttnurtJ1TZtL3bNpqu7dK2zllpgvr6xuttcruausu7LkLrVV2V1tt91ZpO6fMpO/ZtNV2b5W2c8pM+p5NW233Vmk7p8x0YX19o7VW2V1ttd1bpe2cMpO+Z9NW271V2s4pM+l7Nm213Vul7Zwy04X19Y3WWmV3tdV2b5W2c8pM+p5NW233Vmk7p8yk79m01XZvlbZzykwX1tc3WmuV3dVW271V2s4pM+l7Nm213Vul7Zwyk75n01bbvVXazikzXVhf32itVXZXW233Vmk7p8yk79m01XZvlbZzykz6nk1bbfdWaTunzHRhfX2jtVbZXW213Vul7Zwyk75n01bbvYfeF+53PB7TXzPzqZn532fmqzPzn77+s782M59+/c9/Ymb+m5l5d2a+NDNvv/A8//nMfGVmfn1m/vHM/Hg8zz+Ymd+dmT+cmfdm5q/MzOdn5vOv/36bmb/xet7/dWZeafu911Zfu6uttvraXW21Xa3vY9peYt9Lanttu6uttiu21dfuaqutvnZXW21X6/uYtpfY95LaXtvuaqvtim31tbvaaquv3dVW29X6PqbtJfa9pLbXtrvaartiW33trrba6mt3tdV2tb6PaXuJfS+p7bXtrrbarthWX7urrbb62l1ttV2t72PaXmLfS2p7bburrbYrttXX7mqrrb52V1ttV+v7mLaX2PeS2l7b7mrb/bdcWl9t7a622l5zW33trrbaXnrfx7S9xL6X1Pbadlfb7r/l0vq6C3ZXW22vua2+dldbbfW1u9pqu1rfx7S9xL6X1PbadldbbVdsq6/d1VZbfe2uttqu1vcxbS+x7yW1vbbd1VbbFdvqa3e11VZfu6uttqv1fUzbS+x7SW2vbXe11XbFtvraXW211dfuaqvtan0f0/YS+15S22vbXW21XbGtvnZXW231tbvaarta38e0vcS+l9T22nZXW21XbKuv3dX25ds+9Gt7/T8GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYEE3Lz0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOe7eekBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADjfzUsPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMD5bl56AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADOd/PSAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwvpuXHgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA89289AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAnO/mpQcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4Hw3Lz0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOe7eekBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADjfzUsPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMD5bl56AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADOd/NhP7Bt29/dtu3/2rbtN+/5+23btr++bdu727b9xrZtf/bpx7xe+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr7ruTnhZw4z8xcf+Pt/Y2Z+9PWvn5+Zv/n4sb6nHEbfymG0rRxG29Jh9K0cRtvKYbStHEbb0mH0rRxG28phtC0dRt/KYbStHEbb0mH0rRxG28phtK0cRtvSYfStHEbbymG0LR1G38phtK0cRtvSYfStHEbbymG0rRxG29Jh9K0cRtvKYbQtHUbfymG0rRxG29Jh9K0cRtvKYbStHEbb0mH0rRxG28phtC0dRt/KYbStHEbb0mH0rRxG28phtK0cRtvSYfStHEbbymG0LR1G38phtK0cRtvSYfStHEbbymG0rRxG29Jh9K0cRtvKYbQtHUbfymG0rRxG29Jh9K0cRtvKYbQtHUbfymG0rRxG28phtC0dRt/KYbStHEbb0mH0rRxG28phtC0dRt/KYbStHEbbymG0LR1G38phtK0cRtvSYfStHEbbymG0LR1G38phtK0cRtvKYbQtHUbfymG0rRxG29Jh9K0cRtvKYbQtHUbfymG0rRxG28phtC0dRt/KYbStHEbb0mH0rRxG28phtC0dRt/KYbStHEbbymG0LR1G38phtK0cRtvSYfStHEbbymG0LR1G38phtK0cRtvKYbQtHUbfymG0rRxG29Jh9K0cRtvKYbQtHUbfpdx82A8cj8d/MjO/98CPfGZm/v7xff90Zv7ktm0/9FQDXjt9O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3/XcPMG/40dm5uu737/3+s94Gvp2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+F+YjT/Dv2D7gz44f+IPb9vMz8/MzM9///d//yR//8R9/gsev7yd/8ifn3XffnVevXt3q9uUvf/lbM/OlD/iffFdfbT/YU7Sd0feDaNtyFzp2t6Nt5762r317Zn7pzp9p+8fg5nbchY670HIXOu5CR9uWu9Cxux1tO97FWm5ux13ouAstd6HjLnS0bbkLHbvb0bbjXaxldzvadtyFlnexjrvQ0bblLnTsbkfbjnexlt3taNtxF1rexTruQkfblrvQsbsdbTvexVp2t6Ntx11oeRfruAsdbVvuQsfudrTteBdr2d2Oth13oeVdrOMudLRtuQsdu9vRtuUudOxuR9uOb7SWm9txFzruQstd6LgLHW1b7kLH7na07XgXa7m5HXeh4y603IWOu9DRtuUudOxuR9uOd7GWm9txFzruQstd6LgLHW1b7kLH7na07XgXa7m5HXeh4y603IWOu9DRtuUudOxuR9uOd7GW3e1o23EXWt7FOu5CR9uWu9Cxux1tO97FWna3o23HXWh5F+u4Cx1tW+7C8/vyl7/8rePx+INn/Y+Px+OH/pqZPz0zv3nP3/0XM/O53e//+cz80If9Oz/5yU8eed/Xvva14yc+8Ynv+vOZeeecvtr+/5667VHfN7RtuQsdu9vRtnNf2+PxeJyZb2r7OG5ux13ouAstd6HjLnS0bbkLHbvb0bbjXazl5nbchY670HIXOu5CR9uWu9Cxux1tO97FWna3o23HXWh5F+u4Cx1tW+5Cx+52tO14F2vZ3Y62HXeh5V2s4y50tG25Cx2729G2412sZXc72nbchZZ3sY670NG25S507G5H2453sZbd7WjbcRda3sU67kJH25a70LG7HW1b7kLH7na07fhGa7m5HXeh4y603IWOu9DRtuUudOxuR9uOd7GWm9txFzruQstd6LgLHW1b7kLH7na07XgXa7m5HXeh4y603IWOu9DRtuUudOxuR9uOd7GWm9txFzruQstd6LgLHW1b7kLH7na07XgXa9ndjrYdd6HlXazjLnS0bbkLHbvb0bbjXaxldzvadtyFlnexjrvQ0bblLjy/mXnn+CE7et+vm3m8L87MX9re9zMz8/vH4/F3n+Dfy/v07Wjb0balb0fbjrad/2e0LdndjrYdd6FldzvadrRt6dvRtqNtx7tYy+52tO24Cy2729G2o21L3462HW073sVadrejbcddaNndjrYdbVv6drTtaNvxLtayux1tO+5Cy+52tO1o29K3o21H2453sZbd7WjbcRdadrejbUfblr4dbTvadryLtexuR9uOu9Cyux1tO9q29O1o29G2pW9H2462Hd9oLbvb0bbjLrTsbkfbjrYtfTvadrTteBdr2d2Oth13oWV3O9p2tG3p29G2o23Hu1jL7na07bgLLbvb0bajbUvfjrYdbTvexVp2t6Ntx11o2d2Oth1tW/p2tO1o2/Eu1rK7HW077kLL7na07Wjb0rejbUfbjnexlt3taNtxF1p2t6NtR9uWvhfmIx/2A9u2/YOZ+ddn5k9t2/bezPxnM/Mvzcwcj8e/NTP/aGY+NTPvzsz/OzP/bjXsNfrc5z43v/Zrvzbf+ta35q233ppf/MVfnD/8wz/c/4i+Z9K2o21L3462HW07D7X9/Oc/PzPz+zPz26PtWexuR9uOu9Cyux1tO9q29O1o29G2412sZXc72nbchZbd7Wjb0balb0fbjrYd72Itu9vRtuMutOxuR9uOti19O9p2tO14F2vZ3Y62HXehZXc72na0benb0bajbce7WMvudrTtuAstu9vRtqNtS9+Oth1tO97FWna3o23HXWjZ3Y62HW1b+na07Wjb0rejbUfbjm+0lt3taNtxF1p2t6NtR9uWvh1tO9p2vIu17G5H24670LK7HW072rb07Wjb0bbjXaxldzvadtyFlt3taNvRtqVvR9uOth3vYi2729G24y607G5H2462LX072na07XgXa9ndjrYdd6FldzvadrRt6dvRtqNtx7tYy+52tO24Cy2729G2o21L3/Vsx+PxRR786tWr4zvvvPMiz17Ftm1fPh6Pr/64/zttP9y5bWf0/TDattyFjt3taNtyFzradtyFlt3taNtxFzrattyFjt3taNtyFzradtyFlt3taNtxFzrattyFjt3taNtyFzp2t6Nty13oaNtxFzrattyFjt3taNtyFzp2t6Nty13oaNtxFzrattyFjt3taNtyFzp2t6Nty13o4NUJfQAAIABJREFUaNtxFzrattyFjt3taNtyFzp2t6Nty13oaNtxFzrattyFjt3taNtyFzp2t6Nty13oaNtxF1p2t6Ntx13oaNtyFzp2t6Nty13oaNtxF1p2t6Ntx13oaNtyFzp2t6Nty13oaNtxF1p2t6Ntx13oaNtyFzp2t6Nty13oaNtxF1p2t6Ntx13oaNtyFzp2t6Nty13o2N2Oti13oaNtx13oaNtyFzp2t6Nty13o2N2Oti13oaNtx13oaNtyFzqP2d2bpx4GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgOdz89IDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHC+m5ceAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDz3bz0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACc7+alBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgfDcvPQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA57t56QEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAON/NSw8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwPluXnoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM5389IDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHC+m5ceAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDz3bz0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACc7+alBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgfDcvPQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA57t56QEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAON/NSw8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwPluXnoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM5389IDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHC+m5ceAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDz3bz0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACc7+alBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgfDcvPQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA57t56QEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAON/NKT+0bdtf3Lbtn2/b9u62bf/JB/z9v7pt2z/etu1/2bbtN7Zt+9TTj3qdfvVXf3V+7Md+bD7+8Y/PL/3SL33X32v7OPp2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2wUdj8cHf83M983MV2fm7Zn56Mz8+sz8xJ2f+dsz8++9/uefmJn/48P+vZ/85CeP3+u+853vHN9+++3jV7/61eMf/MEfHH/qp37q+JWvfOXN38/MO9qe76G+M/PO0e6eTduOu9Cp2h71PR6P7kLJXeho23IXOna3o23LXeho23EXOr7RWu5Cx13oaNtyFzp2t6Nty13oaNtxFzq+0VruQsdd6LgLLXeh4y50tG25Cx1tO+5Cx7tYy13ouAsdd6HlLnTchY62LXeho23HXWjZ3Y62HXeh4xut5S503IWOti13oaNtx11o2d2Oth13oeMbreUudNyFjrYtd6GjbcddaNndjrYdd6HjG63lLnTchY62LXehY3c72rbchY62HXeh4xut5S503IWOti13oWN3O9q23IWOth13oeMbreUudNyFjrYtd6FjdzvattyFjrYdd6HjG63lLnTchY62LXehY3c72rbchY62HXeh4xut5S503IWOu9ByFzruQkfblrvQ0bbjLnS8i7XchY670HEXWu5Cx13oaNtyFzradtyFl/MvdvecXzfz4f7czLx7PB5/+3g8fntmvjAzn7nzM8eZ+Zdf//MPzMzvnPDv/Z73pS99aT7+8Y/P22+/PR/96Efns5/97PzKr/zK3R/T9kz6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9s13ZzwMz8yM1/f/f6913+291dn5t/etu29mflHM/MfPsl0V+4b3/jGfOxjH3vz+7feemu+8Y1v3P2xvzrankXfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaLummxN+ZvuAPzve+f3nZuZwPB7fmplPzcx/uW3bd/27t237+W3b3tm27Z1vfvObf/xpr8zxeDfjzLZ9V25tz6RvR9uOtp2nbPv6f6vvjt3taNvRtqVvR9uOti19O9p2tO34RmvZ3Y62HW1b+na07Wjb0rejbUfbjm+0lt3taNtxF1p2t6NtR9uWvh1tO9p2vIu17G5H24670LK7HW072rb07Wjb0balb0fbjrYd32gtu9vRtqNtS9+Oth1tW/p2tO1o2/GN1rK7HW072rb07Wjb0balb0fbjrYd32gtu9vRtqNtS9+Oth1tW/p2tO1o2/GN1rK7HW072rb07Wjb0balb0fbjrYd32gtu9vRtqNtS9+Oth1tW/p2tO1o2/GN1rK7HW072rb07Wjb0balb0fbjrYd32gtu9vRtuMutOxuR9uOti19O9p2tO14F2vZ3Y62HXehZXc72na0benb0baj7Zo+8GXtjvdm5mO73781M79z52f+ysz81zMzx+Pxf5qZPzEzf+ruv+h4PP7t4/H46ng8vvrBH/zB8ya+Im+99dZ8/etff/P79957b374h3/47o9peyZ9O9p2tO08ZdvXf6/vjt3taNvRtqVvR9uOti19O9p2tO34RmvZ3Y62HW1b+na07Wjb0rejbUfbjm+0lt3taNtxF1p2t6NtR9uWvh1tO9p2vIu17G5H24670LK7HW072rb07Wjb0balb0fbjrYd32gtu9vRtqNtS9+Oth1tW/p2tO1o2/GN1rK7HW072rb07Wjb0balb0fbjrYd32gtu9vRtqNtS9+Oth1tW/p2tO1o2/GN1rK7HW072rb07Wjb0balb0fbjrYd32gtu9vRtqNtS9+Oth1tW/p2tO1o2/GN1rK7HW072rb07Wjb0balb0fbjrYd32gtu9vRtuMutOxuR9uOti19O9p2tO14F2vZ3Y62HXehZXc72na0benb0baj7ZpuTviZfzYzP7pt25/Ztu2jM/PZmfninZ/5P2fmL8zMbNv2r837/8d+8ykHvUY//dM/Pb/1W781X/va1+bb3/72fOELX5hPf/rTd39M2zPp29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW3X9JEP+4Hj8fidbdv+g5n572fm+2bm7x6Px69s2/bXZuad4/H4xZn5j2fm72zb9h/NzHFm/vLxeDyWg1+Dj3zkI/PLv/zL87M/+7PzR3/0R/NzP/dz84lPfGJ+4Rd+YV69evUvfkzbMz3Ud2Z+4PWP6XsGbTvuQkfblrvQsbsdbVvuQsfudrRtuQsdbTvuQkfblrvQsbsdbVvuQsfudrRtuQsdbTvuQkfblrvQsbsdbVvuQsfudrRtuQsdbTvuQkfblrvQsbsdbVvuQsfudrRtuQsdbTvuQsvudrTtuAsdbVvuQsfudrRtuQsdbTvuQsvudrTtuAsdbVvuQsfudrRtuQsdbTvuQsvudrTtuAsdbVvuQsfudrRtuQsdu9vRtuUudLTtuAsdbVvuQsfudrRtuQsdu9vRtuUudLTtuAsdbVvuQsfudrRtuQsdu9vRtuUudLTtuAsdbVvuQsfudrRtuQsdu9vRtuUudLTtuAsdbVvuQsfudrRtuQsdu9vRtuUudLTtuAsdbVvuQsfudrRtuQsdu9vRtuUudLTtuAtr2l6q/6tXr47vvPPOizx7Fdu2ffl4PL768J+8TdsPd27bGX0/jLYtd6FjdzvattyFjrYdd6FldzvadtyFjrYtd6FjdzvattyFjrYdd6FldzvadtyFjrYtd6FjdzvattyFjt3taNtyFzradtyFjrYtd6FjdzvattyFjt3taNtyFzradtyFjrYtd6FjdzvattyFjt3taNtyFzradtyFjrYtd6FjdzvattyFjt3taNtyFzradtyFjrYtd6FjdzvattyFjt3taNtyFzradtyFlt3taNtxFzrattyFjt3taNtyFzradtyFlt3taNtxFzrattyFjt3taNtyFzradtyFlt3taNtxFzrattyFjt3taNtyFzradtyFlt3taNtxFzrattyFjt3taNtyFzp2t6Nty13oaNtxFzrattyFjt3taNtyFzp2t6Nty13oaNtxFzrattyFzmN29+aphwEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADg+dy89AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAnO/mpQcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4Hw3Lz0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOe7eekBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADjfzUsPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMD5bl56AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADOd/PSAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwvpuXHgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA89289AAAAAAAAAAAAAAAAAAA/x/7dQz6d37Xcfz9+V7IJiJ4DmdSuZA24ElA8q846uJ1qovIzQd2clKETjfoElxczsXBUW70ikMcBFEEOSNSaQqld1fhEgeL6Giud3wdvIY0jUn6NU9//NPHAzL8L7/+8+Xp2xe/LwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMdtp34AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI7bTv0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABy3nfoBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADhuO/UDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHDcduoHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOC47dQPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBx26kfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDjtlM/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADHbad+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACO2079AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAct536AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4bjv1AwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABw3HbqBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADguO1ZPrTW+tJa61trrffXWl/9Xz7zm2utb6617qy1/uz5PuaL69atW3Pt2rW5evXq3Lx587Gf0fYYbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6PtObTv+xP/zMxLM/PBzFyZmYsz8/WZ+flHPvP5mfmnmfmpz37+maf93hs3buw/7j755JP9ypUr+wcffLDfv39/v379+n7nzp0Hfz8zt7U9pmq767vv+5P7zszt3S4cZhc6dqFlFzp2oaNtyy503G5H247vYi2b27ELHbvQsgsdu9DRtmUXOm63o23Hd7GWze3YhY5daNmFjl3o2IWWXejYhY62HZvbsrkdu9CxCy270LELHbvQsgsdu9DRtmNzWza3Yxc6dqFlFzp2oWMXWnahYxc62nZsbsvmduxCxy607ELHLnTsQssudOxCR9uOzW3Z3I5d6NiFll3o2IWOXWjZhY5d6Gjbsgsdt9vRtuO7WMvmduxCxy607ELHLnS0bdmFjtvtaNvxXaxlczt2oWMXWnahYxc62rbsQsftdrTt+C7Wsrkdu9CxCy270LELHW1bdqHjdjvadnwXa9ncjl3o2IWWXejYhY5daNmFjl3oaNuxuS2b27ELHbvQsgsdu9CxCy270LELHW07Nrdlczt24XS+f7tH/mzzdL80M+/v+/7hvu8fz8w7M/Prj3zmt2bmj/d9/4+ZmX3f/+0Zfu+Pvffee2+uXr06V65cmYsXL84bb7wx77777qMf0/YAbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6Pt+bQ9w2d+dmY+eujnu5/9t4d9YWa+sNb6u7XW36+1vvS8HvBFdu/evbl8+fKDny9dujT37t179GPaHqBtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tD2fLjzDZ9Zj/tv+mN/z+Zn5lZm5NDN/u9b6hX3f//MHftFaX5mZr8zMfO5zn/uRH/ZFs++PZpxZ64dya3vA82z72f9W34e43Y62HbvQcrsdbTvatvTtaNvRtuO7WMvtdrTt2IWW2+1o29G2pW9H2462Hd/FWm63o23HLrTcbkfbjl1oud2Oth1tOza35XY72nbsQsvtdrTt2IWW2+1o29G2Y3NbbrejbccutNxuR9uOXWi53Y62HW07NrfldjvaduxCy+12tO3YhZbb7Wjb0bZjc1tut6Ntxy603G5H245daLndjrYdbVv6drTtaNvxXazldjvaduxCy+12tO1o29K3o21H247vYi2329G2YxdabrejbUfblr4dbTvadnwXa7ndjrYdu9Byux1tO9q29O1o29G247tYy+12tO3YhZbb7WjbsQstt9vRtqNtx+a23G5H245daLndjrYdu9Byux1tO9p2bG7L7Xa0PZ+2Z/jM3Zm5/NDPl2bmXx/zmXf3ff/evu/fmZlvzf/8H/oH7Pv+J/u+n+37fvbyyy8ffeYXxqVLl+ajjz568PPdu3fnlVdeefRj2h7wPNvO6Psot9vRtmMXWm63o21H25a+HW072nZ8F2u53Y62HbvQcrsdbTvatvTtaNvRtuO7WMvtdrTt2IWW2+1o27ELLbfb0bajbcfmttxuR9uOXWi53Y62HbvQcrsdbTvadmxuy+12tO3YhZbb7WjbsQstt9vRtqNtx+a23G5H245daLndjrYdu9Byux1tO9p2bG7L7Xa07diFltvtaNuxCy2329G2o21L3462HW07vou13G5H245daLndjrYdbVv6drTtaNvxXazldjvaduxCy+12tO1o29K3o21H247vYi2329G2YxdabrejbUfblr4dbTvadnwXa7ndjrYdu9Byux1tO3ah5XY72na07djcltvtaNuxCy2329G2YxdabrejbUfbjs1tud2OtufT9gyf+YeZ+fxa69W11sWZeWNmvvbIZ/58Zn51Zmat9dMz84WZ+fB5PuiL6Itf/OJ8+9vfnu985zvz8ccfzzvvvDNf/vKXH/2Ytgdo29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072nb3GXHXAAAgAElEQVS0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW3PpwtP+8C+75+stX57Zv5yZl6amT/d9/3OWuv3Z+b2vu9f++zvfm2t9c2Z+XRmfm/f938vH/xFcOHChXn77bfn9ddfn08//XTefPPNee211+att96as7Oz739M2wO0bT2p78z85Gcf0/cAt9vRtmUXOm63o23LLnTcbkfbjrYtm9txux1tW3ah43Y72rbsQsftdrTtaNuyuR2329G2ZRc6brejbcsudNxuR9uOti2b23G7HW1bdqHjdjvatuxCx+12tO1o27K5Hbfb0bZlFzput6Ntyy503G5H2462LZvbcbsdbVt2oeN2O9q27ELH7Xa07Wjbsrkdt9vRtmUXOm63o23LLnTcbkfbll3ouN2Oth1tWza343Y72rbsQsftdrRt2YWO2+1o29G2ZXM7brejbcsudNxuR9uWXei43Y62HW1bNrfjdjvatuxCx+12tG3ZhY7b7Wjb0bZlcztut6Ntyy503G5H25Zd6LjdjrYdbVs2t+N2O9q27ELH7Xa0bdmFjtvtaNvRtmVzO273fFr7vp/kHz47O9tv3759kn/7vFhr/eO+72dP/+QP0vbpjrad0fdptG3ZhY7b7WjbsgsdbTt2oeV2O9p27EJH25Zd6LjdjrYtu9DRtmMXWm63o23HLnS0bdmFjtvtaNuyCx2329G2ZRc62nbsQkfbll3ouN2Oti270HG7HW1bdqGjbccudLRt2YWO2+1o27ILHbfb0bZlFzraduxCR9uWXei43Y62LbvQcbsdbVt2oaNtxy50tG3ZhY7b7Wjbsgsdt9vRtmUXOtp27ELL7Xa07diFjrYtu9Bxux1tW3aho23HLrTcbkfbjl3oaNuyCx2329G2ZRc62nbsQsvtdrTt2IWOti270HG7HW1bdqGjbccutNxuR9uOXeho27ILHbfb0bZlFzput6Ntyy50tO3YhY62LbvQcbsdbVt2oeN2O9q27EJH245d6Gjbsgud/8vtbs/7YQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD4/7Od+gEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOG479QMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcNx26gcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4Ljt1A8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwHHbqR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgOO2Uz8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMdtp34AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI7bTv0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABy3nfoBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADhuO/UDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHDcduoHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOC47dQPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBx26kfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDjtlM/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADHbad+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACO2079AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAct536AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4bjv1AwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABw3HbqBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADguO3UDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAcdupHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA47ZTPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAx23P8qG11pfWWt9aa72/1vrqEz73G2utfa119vwe8cV269atuXbt2ly9enVu3rz5v35O22P07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrbn0L7vT/wzMy/NzAczc2VmLs7M12fm5x/zuZ+Ymb+Zmb+fmbOn/d4bN27sP+4++eST/cqVK/sHH3yw379/f79+/fp+586dB38/M7d3bQ97Ut/vt931PUTbjl3oVG13ffd9twslu9DRtmUXOm63o23LLnS07diFjne0ll3o2IWOti270HG7HW1bdqGjbccudLyjtexCxy507ELLLnTsQkfbll3oaNuxCx3fxVp2oWMXOnahZRc6dqGjbcsudLTt2IWW2+1o27ELHe9oLbvQsQsdbVt2oaNtxy603G5H245d6HhHa9mFjl3oaNuyCx1tO3ah5XY72nbsQsc7WssudOxCR9uWXei43Y62LbvQ0bZjFzre0Vp2oWMXOtq27ELH7Xa0bdmFjrYdu9DxjtayCx270NG2ZRc6brejbcsudLTt2IWOd7SWXejYhY62LbvQcbsdbVt2oaNtxy50vKO17ELHLnTsQssudOxCR9uWXeho27ELHd/FWnahYxc6dqFlFzp2oaNtyy50tO3YhdN5+HZ/1D/bPN0vzcz7+75/uO/7xzPzzsz8+mM+9wcz84cz81/P8DuZmffee2+uXr06V65cmYsXL84bb7wx77777uM+qu0B+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfb82l7hs/87Mx89NDPdz/7bw+stX5xZi7v+/4Xz/HZXnj37t2by5cvP/j50qVLc+/evR/4jLbH6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tz6ftGT6zHvPf9gd/udY2M380M7/71F+01lfWWrfXWre/+93vPvtTvqD2ff+h/7bWD+XW9qCn9XW7x2nbsQud59n2s/+tvg+xCx270NG2ZRc6brejbcsudLTt2IWOd7SWXejYhY62LbvQcbsdbVt2oaNtxy50vKO17ELHLnTsQssudOxCR9uWXeho27ELHd/FWnahYxc6dqFlFzp2oaNtyy50tO3YhZbb7WjbsQsd72gtu9CxCx1tW3aho23HLrTcbkfbjl3oeEdr2YWOXeho27ILHW07dqHldjvaduxCxztayy507EJH25Zd6LjdjrYtu9DRtmMXOt7RWnahYxc62rbsQsftdrRt2YWOth270PGO1rILHbvQ0bZlFzput6Ntyy50tO3YhY53tJZd6NiFjrYtu9Bxux1tW3aho23HLnS8o7XsQscudOxCyy507EJH25Zd6GjbsQsd38VadqFjFzp2oWUXOnaho23LLnS07diF82l7hs/cnZnLD/18aWb+9aGff2JmfmFm/nqt9S8z88sz87W11tmjv2jf9z/Z9/1s3/ezl19++fhTvyAuXbo0H3300YOf7969O6+88srDH3lptD3sGfq63YO07diFzvNsO6Pvo+xCxy50tG3ZhY7b7WjbsgsdbTt2oeMdrWUXOnaho23LLnTcbkfbll3oaNuxCx3vaC270LELHbvQsgsdu9DRtmUXOtp27ELHd7GWXejYhY5daNmFjl3oaNuyCx1tO3ah5XY72nbsQsc7WssudOxCR9uWXeho27ELLbfb0bZjFzre0Vp2oWMXOtq27EJH245daLndjrYdu9DxjtayCx270NG2ZRc6brejbcsudLTt2IWOd7SWXejYhY62LbvQcbsdbVt2oaNtxy50vKO17ELHLnS0bdmFjtvtaNuyCx1tO3ah4x2tZRc6dqGjbcsudNxuR9uWXeho27ELHe9oLbvQsQsdu9CyCx270NG2ZRc62nbsQsd3sZZd6NiFjl1o2YWOXeho27ILHW07duGc2vf9iX9m5sLMfDgzr87MxZn5+sy89oTP//XMnD3t9964cWP/cfe9731vf/XVV/cPP/xwv3///n79+vX9G9/4xoO/n5nbu7aHPanvo213fX8k2nbsQqdqu+u777tdKNmFjrYtu9Bxux1tW3aho23HLnS8o7XsQscudLRt2YWO2+1o27ILHW07dqHjHa1lFzp2oWMXWnahYxc62rbsQkfbjl3o+C7Wsgsdu9CxCy270LELHW1bdqGjbccutNxuR9uOXeh4R2vZhY5d6GjbsgsdbTt2oeV2O9p27ELHO1rLLnTsQkfbll3oaNuxCy2329G2Yxc63tFadqFjFzratuxCx+12tG3ZhY62HbvQ8Y7Wsgsdu9DRtmUXOm63o23LLnS07diFjne0ll3o2IWOti270HG7HW1bdqGjbccudLyjtexCxy50tG3ZhY7b7WjbsgsdbTt2oeMdrWUXOnahYxdadqFjFzratuxCR9uOXej4LtayCx270LELLbvQsQsdbVt2oaNtxy6czuNu91n/XJin2Pf9k7XWb8/MX87MSzPzp/u+31lr/f5n//DXnvY7eLwLFy7M22+/Pa+//vp8+umn8+abb85rr702b7311pydnZ368c69J/WdmZ889fOdZ9p27EJH25Zd6LjdjrYtu9Bxux1tW3aho23HLnS0bdmFjtvtaNuyCx2329G2ZRc62nbsQkfbll3ouN2Oti270HG7HW1bdqGjbccudLRt2YWO2+1o27ILHbfb0bZlFzraduxCy+12tO3YhY62LbvQcbsdbVt2oaNtxy603G5H245d6Gjbsgsdt9vRtmUXOtp27ELL7Xa07diFjrYtu9Bxux1tW3ah43Y72rbsQkfbjl3oaNuyCx2329G2ZRc6brejbcsudLTt2IWOti270HG7HW1bdqHjdjvatuxCR9uOXeho27ILHbfb0bZlFzput6Ntyy50tO3YhY62LbvQcbsdbVt2oeN2O9q27EJH245d6Gjbsgsdt9vRtmUXOm63o23LLnS07diF82nt+36Sf/js7Gy/ffv2Sf7t82Kt9Y/7vv/I/9+j7dMdbTuj79No27ILHbfb0bZlFzraduxCy+12tO3YhY62LbvQcbsdbVt2oaNtxy603G5H245d6Gjbsgsdt9vRtmUXOm63o23LLnS07diFjrYtu9Bxux1tW3ah43Y72rbsQkfbjl3oaNuyCx2329G2ZRc6brejbcsudLTt2IWOti270HG7HW1bdqHjdjvatuxCR9uOXeho27ILHbfb0bZlFzput6Ntyy50tO3YhZbb7WjbsQsdbVt2oeN2O9q27EJH245daLndjrYdu9DRtmUXOm63o23LLnS07diFltvtaNuxCx1tW3ah43Y72rbsQkfbjl1oud2Oth270NG2ZRc6brejbcsudNxuR9uWXeho27ELHW1bdqHjdjvatuxCx+12tG3ZhY62HbvQ0bZlFzr/l9vdnvfDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPD/Zzv1AwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABw3HbqBwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADguO3UDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAcdupHwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA47ZTPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAx22nfgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAjttO/QAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHLed+gEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOG479QMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAcNx26gcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4Ljt1A8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwHHbqR8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgOO2Uz8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMdtp34AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI7bTv0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABy3nfoBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADhuO/UDAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHDcduoHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOC47dQPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMBx26kfAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDjtlM/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADHbad+AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACO257lQ2utL621vrXWen+t9dXH/P3vrLW+udb657XWX621fu75P+qL6datW3Pt2rW5evXq3Lx584f+XtvjtG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29L3v9uvg1DLEvSw7999M7RDEkdJHC0s9QhNM2KCFBriboS9SQgOyHihWcSBXiSxkc3gGGdvY9BCmwiyCwqYhAiULKLB2sxkYQmCDVlZ4w5knBmCw/RAPN02OLFAm0BLLU4WU68472nq1dV97//uOad+Pyio6rpT5zt/ffp4t6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfbjrYtfTvadrRt6dvRtqNtS9+Oth1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Otju0LMuDv2bmczPz0cy8MzNvzcy3Zuan733mP5iZf/nF7//zmfna6/7d9957b3nTffbZZ8s777yzfPTRR8unn366vPvuu8t3vvOdl38/Mx9qe5mq7aLvsiwP952ZDxd34WLuQsddaLkLHXeho23LXejY3Y62HT+LtdzcjrvQcRda7kLHXeho23IXOna3o23Hz2ItN7fjLnTchZa70HEXOu5Cy13ouAsdbTtubsvN7bgLHXeh5S503IWOu9ByFzruQkfbjpvbcnM77kLHXWi5Cx13oeMutNyFjrvQ0bbj5rbc3I670HEXWu5Cx13ouAstd6HjLnS07bi5LTe34y503IWWu9BxFzruQstd6LgLHW1b7kLH7na07fhZrOXmdtyFjrvQchc67kJH25a70LG7HW07fhZrubkdd6HjLrTchY670NG25S507G5H246fxVpubsdd6LgLLXeh4y50tG25Cx2729G242exlpvbcRc67kLLXei4Cx13oeUudNyFjrYdN7fl5nbchY670HIXOu5Cx11ouQsdd6GjbcfNbbm5HXfhem5395JfN/N6Pzsz312W5XvLsvzezPz6zHxl/YFlWf7+siz/34s//oOZefuMf/eN981vfnO+9KUvzTvvvDNvvfXWfPDBB/P1r3/9zme0vYy2LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0Xafbs74zI/PzPdXf/74xX97lb88M3/3MUO9KT755JP5whe+8PLPb7/99nzyyScP/U+0PZO2LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0XafPn/GZ04/5L8tP/SDp9N/MjPvz8y//4q//+rMfHVm5id+4ifOHPG4luUPZzydflhubf+onrLti8/ou2J3O9p23IWW3e1o29G2pW9H2462HT+LtexuR9uOu9Cyux1tO9q29O1o29G242exlt3taNtxF1p2t6Ntx11o2d2Oth1tO25uy+52tO24Cy2729G24y607G5H2462HTe3ZXc72nbchZbd7WjbcRdadrejbUfbjpvbsrsdbTvuQsvudrTtuAstu9vRtqNtx81t2d2Oth13oWV3O9p23IWW3e1o29G2pW9H2462HT+LtexuR9uOu9Cyux1tO9q29O1o29G242exlt3taNtxF1p2t6NtR9uWvh1tO9p2/CzWsrsdbTvuQsvudrTtaNvSt6NtR9uOn8VadrejbcddaNndjrYdd6FldzvadrTtuLktu9vRtuMutOxuR9uOu9Cyux1tO9p23NyW3e1ou083Z3zm45n5wurPb8/MP73/odPp9B/OzN+amZ9fluXTH/YPLcvy3y7L8v6yLO//6I/+6CXzHsrbb78936FI1uEAACAASURBVP/+91/++eOPP54f+7Ef+0Of0/aP7inbzuh7n93taNtxF1p2t6NtR9uWvh1tO9p2/CzWsrsdbTvuQsvudrTtaNvSt6NtR9uOn8VadrejbcddaNndjrYdd6FldzvadrTtuLktu9vRtuMutOxuR9uOu9Cyux1tO9p23NyW3e1o23EXWna3o23HXWjZ3Y62HW07bm7L7na07bgLLbvb0bbjLrTsbkfbjrYdN7dldzvadtyFlt3taNtxF1p2t6NtR9uWvh1tO9p2/CzWsrsdbTvuQsvudrTtaNvSt6NtR9uOn8VadrejbcddaNndjrYdbVv6drTtaNvxs1jL7na07bgLLbvb0bajbUvfjrYdbTt+FmvZ3Y62HXehZXc72nbchZbd7Wjb0bbj5rbsbkfbjrvQsrsdbTvuQsvudrTtaNtxc1t2t6PtTi3L8uCvmfn8zHxvZr44M2/NzLdm5mfufebfnZmPZuanXvfv3f567733ljfd7//+7y9f/OIXl+9973vLp59+urz77rvLt7/97Zd/PzMfanuZqu2i77IsD/edmQ8Xd+Fi7kLHXWi5Cx13oaNty13o2N2Oth0/i7Xc3I670HEXWu5Cx13oaNtyFzp2t6Ntx89iLTe34y503IWWu9BxFzruQstd6LgLHW07bm7Lze24Cx13oeUudNyFjrvQchc67kJH246b23JzO+5Cx11ouQsdd6HjLrTchY670NG24+a23NyOu9BxF1ruQsdd6LgLLXeh4y50tO24uS03t+MudNyFlrvQcRc67kLLXei4Cx1tW+5Cx+52tO34Wazl5nbchY670HIXOu5CR9uWu9Cxux1tO34Wa7m5HXeh4y603IWOu9DRtuUudOxuR9uOn8Vabm7HXei4Cy13oeMudLRtuQsdu9vRtuNnsZab23EXOu5Cy13ouAsdd6HlLnTchY62HTe35eZ23IWOu9ByFzruQsddaLkLHXeho23HzW25uR134Xpud/eSX5+f11iW5bPT6fTXZ+a3ZuZzM/Ory7J853Q6/dKLB39jZv6rmflXZ+bvnE6nmZl/sizLz7/u337Tff7zn59f+ZVfmZ/7uZ+bP/iDP5hf+IVfmJ/5mZ+ZX/zFX5z333//9mPaXkDb1kN9Z+ZHXnxM3wvY3Y62LXehY3c72rbchY7d7Wjb0bbl5nbsbkfblrvQsbsdbVvuQsfudrTtaNtyczt2t6Nty13o2N2Oti13oWN3O9p2tG25uR2729G25S507G5H25a70LG7HW072rbc3I7d7Wjbchc6drejbctd6NjdjrYdbVtubsfudrRtuQsdu9vRtuUudOxuR9uOti03t2N3O9q23IWO3e1o23IXOna3o23LXejY3Y62HW1bbm7H7na0bbkLHbvb0bblLnTsbkfbjrYtN7djdzvattyFjt3taNtyFzp2t6NtR9uWm9uxux1tW+5Cx+52tG25Cx2729G2o23Lze3Y3Y62LXehY3c72rbchY7d7Wjb0bbl5nbsbkfblrvQsbsdbVvuQsfudrTtaNtyczt2d59Oy7Jc5cHvv//+8uGHH17l2XtxOp3+t2VZ3n/9J+/S9vUubTuj7+to23IXOna3o23LXeho23EXWna3o23HXeho23IXOna3o23LXeho23EXWna3o23HXeho23IXOna3o23LXejY3Y62LXeho23HXeho23IXOna3o23LXejY3Y62LXeho23HXeho23IXOna3o23LXejY3Y62LXeho23HXeho23IXOna3o23LXejY3Y62LXeho23HXeho23IXOna3o23LXejY3Y62LXeho23HXWjZ3Y62HXeho23LXejY3Y62LXeho23HXWjZ3Y62HXeho23LXejY3Y62LXeho23HXWjZ3Y62HXeho23LXejY3Y62LXeho23HXWjZ3Y62HXeho23LXejY3Y62LXehY3c72rbchY62HXeho23LXejY3Y62LXehY3c72rbchY62HXeho23LXeg8ZndvnnoYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJ7PzbUHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOByN9ceAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDL3Vx7AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAud3PtAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC43M21BwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgcjfXHgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAy91cewAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALndz7QEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAuNzNtQcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4HI31x4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgMvdXHsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC53c+0BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALjczbUHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOByN9ceAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDL3Vx7AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAud3PtAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC43M21BwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgcjfXHgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAy91cewAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALndz7QEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAuNzNtQcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4HI31x4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgMvdnPOh0+n0506n0z8+nU7fPZ1Of+OH/P0fO51OX3vx9799Op1+8qkHParf/M3fnC9/+cvzpS99aX75l3/5D/29to+jb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9p2tG3p29G2o21L3462HW1b+na07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462LX072na07Wjb0rejbUfblr4dbTvatvTtaNvRtqNtS9+Oth1tW/p2tO1o29K3o21H2462LX072na0benb0bajbUvfjrYdbTvatvTtaNvRtqVvR9uOti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTdoWVZHvw1M5+bmY9m5p2ZeWtmvjUzP33vM39tZv72i99/MDNfe92/+9577y1vus8++2x55513lo8++mj59NNPl3fffXf5zne+8/LvZ+ZDbS/3UN+Z+XCxuxfTtuMudKq2i77LsrgLJXeho23LXejY3Y62LXeho23HXej4jtZyFzruQkfblrvQsbsdbVvuQkfbjrvQ8R2t5S503IWOu9ByFzruQkfblrvQ0bbjLnT8LNZyFzruQsddaLkLHXeho23LXeho23EXWna3o23HXej4jtZyFzruQkfblrvQ0bbjLrTsbkfbjrvQ8R2t5S503IWOti13oaNtx11o2d2Oth13oeM7Wstd6LgLHW1b7kLH7na0bbkLHW077kLHd7SWu9BxFzrattyFjt3taNtyFzradtyFju9oLXeh4y50tG25Cx2729G25S50tO24Cx3f0VruQsdd6Gjbchc6drejbctd6GjbcRc6vqO13IWOu9BxF1ruQsdd6Gjbchc62nbchY6fxVruQsdd6LgLLXeh4y50tG25Cx1tO+7C9dzu7iW/bub1fnZmvrssy/eWZfm9mfn1mfnKvc98ZWZ+7cXvf2Nm/uzpdDqd8W+/0b75zW/Ol770pXnnnXfmrbfemg8++GC+/vWv3/+YthfSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrTtaNvSt6NtR9uWvh1tO9q29O1o29G2o21L3462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o21H25a+HW072rb07Wjb0balb0fbjrYdbVv6drTtaNvSt6NtR9uWvh1tO9ru080Zn/nxmfn+6s8fv/hvP/Qzy7J8NjO/OzN/4ikGPLJPPvlkvvCFL7z889tvvz2ffPLJ/Y9peyF9O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o+0+nZZlefgDp9N/PDM/tyzLX3nx5/90Zn52WZb/YvWZ77z4zMcv/vzRi8/8i3v/1ldn5qsv/vjvzMy3n+pFnsC/NTP/7zM/89+YmX9tZv7vF3/+N2fmX5kX/08yM1+emX8y+287s72+X16W5Y/b3Ytp23EXOk/W9sXfbbnv1nbXXXgcd6Gjbctd6Lwpu6tta2t93YXH0bbjLnR8R2u5Cx13oaNty13ovCm7q21ra33dhcfRtuMudHxHa7kLHXeh4y603IXOm3IXtG1tra+78DjadtyFjp/FWu5Cx13ouAstd6HzptwFbVtb6+suPI62HXehZXc72nbchY7vaC13oeMudLRtuQsdbTvuQsvudrTtuAsd39Fa7kLHXeho23IXOtp23IWW3e1o23EXOr6jtdyFjrvQ0bblLnTelN3VtrW1vu7C42jbcRc6vqO13IWOu9DRtuUudN6U3dW2tbW+7sLjaNtxFzq+o7XchY670NG25S503pTd1ba1tb7uwuNo23EXOr6jtdyFjrvQ0bblLnTelN3VtrW1vu7C42jbcRc6vqO13IWOu9BxF1ruQudNuQvatrbW1114HG077kLHz2Itd6HjLnTchZa70HlT7oK2ra31dRceR9uOu3A9X16W5Y9f9L9cluXBXzPzZ2bmt1Z//psz8zfvfea3ZubPvPj95+cHcU6v+Xc/fN2zn/PXNeZ5XduZ+fAIbbfY93aeI/TV9s1pezvTEdpeY6aq7Rb7bm133YWu7e1MR2h7jZm0vV5fd6FrezuTttrura+7oO0e297OdIS215iparvFvlvbXXeha3s70xHaXmMmba/X113o2t7OpK22e+vrLmi7x7a3Mx2h7TVmqtpuse/Wdtdd6NreznSEtteYyV24Xl93oWt7O5O22u6tr7ug7R7b3s50hLbXmKlqu8W+W9tdd6FrezvTEdpeYyZ34Xp93YWu7e1M2mq7t77ugrZ7bHs70xHabrGv3dV2j21vZzpC22vMVLXdYt+t7a670LW9nekIba8xk7bX6+suaLvHtrczHaHtFvvaXW332PZ2piO0vcZMVdst9t3a7roLXdvbmY7Q9hozaXu9vu6CtntsezvTEdpusa/d1XaPbW9nOkLba8xUtd1i363trrvQtb2d6QhtrzGTttfr6y50bW9n0lbbvfV1F7TdY9vbmY7Q9hozVW232Hdru+sudG1vZzpC22vMpO31+roLXdvbmbTVdm993QVt99j2dqYjtL3GTFXbLfbd2u66C13b25mO0PYaM2l7vb7uQtf2diZttd1bX3dB2z22vZ3pCG2vMVPVdot9t7a77kLX9namI7S9xkzaXq+vu9C1vZ1JW2331tdd0HaPbW9nOkLba8xUtd1i363trrvQtb2d6QhtrzGTu3C9vu5C1/Z2Jm213Vtfd0HbPba9nekIba8xU9V2i323trvuQtf2dqYjtL3GTO7C9fq6C13b25m01XZvfd0FbffY9namI7Td4kyPmedmXu8fzsxPnU6nL55Op7dm5oOZ+ca9z3xjZv7ii9//hZn5e8uLyXiQti19O9p2tO1o29K3o21H25a+HW072rb07Wjb0bajbUvfjrYdbVv6drTtaNvSt6NtR9uOti19O9p2tG3p29G2o21L3462HW072rb07Wjb0balb0fbjrYtfTvadrRt6dvRtqNtR9uWvh1tO9q29O1o29G2pW9H2462HW1b+na07Wjb0rejbUfblr4dbTvadrRt6dvRtqNtS9+Oth1tW/p2tO1o29G2pW9H2462LX072na0benb0bajbUfblr4dbTvatvTtaNvRtqVvR9uOth1tW/p2tO1o29K3o21H25a+HW072na0benb0bajbUvfjrYdbVv6drTtaNvRtqVvR9uOti19O9p2tG3p29G2o+0Off51H1iW5bPT6fTXZ+a3ZuZzM/Ory7J853Q6/dLMfLgsyzdm5r+fmf/xdDp9d2Z+Z37wf3xe43VtX3xM2wu9pu+PvPiYvhfQtuMudLRtuQsdu9vRtuUudOxuR9uWu9DRtuMudLRtuQsdu9vRtuUudOxuR9uWu9DRtuMudLRtuQsdu9vRtuUudOxuR9uWu9DRtuMudLRtuQsdu9vRtuUudOxuR9uWu9DRtuMutOxuR9uOu9DRtuUudOxuR9uWu9DRtuMutOxuR9uOu9DRtuUudOxuR9uWu9DRtuMutOxuR9uOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu9DRtuUudOxuR9uWu9Cxux1tW+5CR9uOu7BTy7Jc5dfMfPVaz97DPI+Z6UjvssV5jvQuW5vnSO+ytZmO9C5bnOdI77K1eY70Llub6UjvsrV5tvYeW5zJXdjmTFt7l63Nc6S2W5zJXdjmPEd6l63NdKR32eI8R3qXrc1zpHfZ2kxHepetzbO199jiTO7CNmfa2rtsbZ4jtd3iTO7CNuc50rtsbaYjvcsW5znSu2xtniO9y9ZmOtK7bHGeI73L1uY50rtsbaatvcvW5jlS2y3O5C5sc54jvcvWZjrSu2xxniO9y9bmOdK7bG2mI73LFuc50rtsbZ4jvcvWZtrau2xtniO13eJM7sI25znSu2xtpiO9yxbnOdK7bG2eI73L1mY60rtscZ4jvcvW5jnSu2xtpiO9y9bm2dp7bHEmd2Gb8xzpXbY205HeZYvzHOldtjbPkd5lazMd6V22OM+R3mVr8xzpXbY205HeZWvzbO09tjiTu7DNeY70Llub6UjvssV5jvQuW5vnSO+ytZmO9C5bnOdI77K1eY70Llub6UjvsrV5tvYeW5zJXdjmTFt7l63Nc6S2W5zJXdjmPEd6l63NdKR32eI8R3qXrc1zpHfZ2kxHepetzbO199jiTO7CNmfa2rtsbZ4jtd3iTO7CNuc50rtsbaYjvcsW5znSu2xtniO9y9ZmOtK7bG2erb3HFmdyF7Y509beZWvzHKntFmdyF7Y5z5HeZWszHeldtjjPkd5la/Mc6V22NtOR3mVr82ztPbY4k7uwzZm29i5bm+dIbbc4k7uwzXmO9C5bm+lI77LFeY70Llub50jvsrWZjvQuW5znSO+ytXmO9C5bm2lr77K1eY7UdoszuQvbnOdI77K1mY70Lluc50jvsrV5jvQuW5vpSO+yxXmO9C5bm+dI77K1mbb2Llub50httziTu7DNeY70Llub6UjvssV5Ti/+AQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB26ObaAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwuZv6AafT6c+dTqd/fDqdvns6nf7GD/n7P3Y6nb724u9/+3Q6/eSV5/lLp9Pp/zmdTv/7i19/JZ7nV0+n0z8/nU7ffsXfn06n03/9Yt5/dDqd/tQf4V20PUjbM2fS90LadrRt6dvRtqNta0t9H9P2zHexuwfZXW072rb07Wjb0balb0fbjratLfV9TNsz38XuHmR3te1o29K3o21H25a+HW072ra21Pcxbc98F7t7kN3VtqNtS9+Oth1tW/p2tO1o29pS38e0PfNd7O5BdlfbjrYtfTvadrRt6dvRtqNta0t9H9P2zHexuwfZXW072rb07Wjb0balb0fbjratLfV9TNsz38XuHmR3tW1tqa+2dvdc2na0benb0bajbWtLfR/T9sx3sbsH2V1tW1vq6y7Y3XNp29G2pW9H2462LX072na0bW2p72Panvkudvcgu6ttR9uWvh1tO9q29O1o29G2taW+j2l75rvY3YPsrrYdbVv6drTtaNvSt6NtR9vWlvo+pu2Z72J3D7K72na0benb0bajbUvfjrYdbVtb6vuYtme+i909yO5q29G2pW9H2462LX072na0bW2p72Panvkudvcgu6ttR9uWvh1tO0dq+6BlWbJfM/O5mfloZt6Zmbdm5lsz89P3PvPXZuZvv/j9BzPztSvP85dm5lfKLvee9+/NzJ+amW+/4u///Mz83Zk5zcyfnpnf1vbNaquv3dVWW33trrba7rHvpW232HdrbY+0u9pqu8e2+tpdbbXV1+5qq+0e+17adot9t9b2SLurrbZ7bKuv3dVWW33trrba7rHvpW232HdrbY+0u9pqu8e2+tpdbbXV1+5qq+0e+17adot9t9b2SLurrbZ7bKuv3dVWW33trrba7rHvpW232HdrbY+0u9pqu8e2+tpdbbXV1+5qq+0e+17adot9t9b2SLurrbugrb7aanuEtvraXW213UPfS9tuse/W2h5pd7V1F85pu8W+W2t7pN3VVts9ttXX7mqrrb52V1tt99j30rZb7Lu1tkfaXW213WNbfe2uttrqa3e11XaPfS9tu8W+W2t7pN3VVts9ttXX7mqrrb52V1tt99j30rZb7Lu1tkfaXW213WNbfe2uttrqa3e11XaPfS9tu8W+W2t7pN3VVts9ttXX7mqrrb52V1tt99j30rZb7Lu1tkfaXW213WNbfe2uttdv+7pfN9P62Zn57rIs31uW5fdm5tdn5iv3PvOVmfm1F7//jZn5s6fT6XTFeZ7Vsiz/68z8zgMf+crM/A/LD/yDmfnXT6fTnxxtX+tAbefMmZ7Vgfpqq+3aXtrOmTM9qwP11Vbbtb20nTNnejaPaDuzvb6bajtzqN3VVtu1vbSdM2d6Vgfqq622a3tpO2fO9KwO1Fdbbdf20nbOnOnZ+I7WOtDuaqvt2l7azpkzPasD9dVW27W9tJ0zZ3pWB+qrrbZre2k7Z870bHxHax1od7XVdm0vbefMmZ7Vgfpqq+3aXtrOmTM9qwP11Vbbtb20nTNneja+o7UOtLvaaru2l7Zz5kzP6kB9tdV2bS9t58yZntWB+mqr7dpe2s6ZMz0b39FaB9pdbbVd20vbOXOmZ3Wgvtpqu7aXtnPmTM/qQH211XZtL23nzJmeje9orQPtrrbuwpq2j3Cgvtpqu7aXtnPmTM/qQH211XZtL23nzJmeje9orQPtrrbuwpq78AgH2l1ttV3bS9s5c6ZndaC+2mq7tpe2c+ZMz+pAfbXVdm0vbefMmZ6N72itA+2uttqu7aXtnDnTszpQX221XdtL2zlzpmd1oL7aaru2l7Zz5kzPxne01oF2V1tt1/bSds6c6VkdqK+22q7tpe2cOdOzOlBfbbVd20vbOXOmZ+M7WutAu6uttmt7aTtnzvSsDtRXW23X9tJ2zpzpWR2or7baru2l7Zw507PxHa11oN3VVtu1vbSdM2d6Vgfqq622a3tpO2fO9KwO1Fdbbdf20nbOnOnZ+I7WOtDuaqvt2l7azpkzPasD9dVW27WHfl54pZunGvAVfnxmvr/688cv/tsP/cyyLJ/NzO/OzJ+44jwzM//R6XT6R6fT6TdOp9MXolnO9aqZtX28vbS987wHYwPqHwAADNRJREFUZprR9ynnvE/bp5vzPm0vo6/dXdO2s5e2d573wEwz2+n70Lxb67u3tjP72V1ttV3bS9s7z3tgphl9n3LO+7R9ujnv0/Yy+trdNW07e2l753kPzDSznb6+o7X2srvaaru2l7Z3nvfATDP6PuWc92n7dHPep+1l9LW7a9p29tL2zvMemGlmO319R2vtZXe11XZtL23vPO+BmWb0fco579P26ea8T9vL6Gt317Tt7KXtnec9MNPMdvr6jtbay+5qq+3aXtreed4DM83o+5Rz3qft0815n7aX0dfurmnb2UvbO897YKaZ7fT1Ha21l93VVtu1vbS987wHZprR9ynnvE/bp5vzPm0vo6/dXdO2s5e2d573wEwz2+nrO1prL7urrbuwpm1rL3211XZtL23vPO+BmWb0fco579P26ea8T9vL7K2v72itveyutu7CmrvQ2svuaqvt2l7a3nneAzPN6PuUc96n7dPNeZ+2l9HX7q5p29lL2zvPe2Cmme309R2ttZfd1Vbbtb20vfO8B2aa0fcp57xP26eb8z5tL6Ov3V3TtrOXtnee98BMM9vp6ztaay+7q622a3tpe+d5D8w0o+9Tznmftk83533aXkZfu7umbWcvbe8874GZZrbT13e01l52V1tt1/bS9s7zHphpRt+nnPM+bZ9uzvu0vYy+dndN285e2t553gMzzWynr+9orb3srrbaru2l7Z3nPTDTjL5POed92j7dnPdpexl97e6atp29tL3zvAdmmtlOX9/RWnvZXW21XdtL2zvPe2CmGX2fcs77tH26Oe/bQ9sH3WTj/MDph/y35YLPPJVznvU/z8xPLsvy7sz8LzPza9Es53rVzNo+3l7anvs8fS+jrbZre2l77vP0vYy22q7tpe25z9tS34fm3VrfvbWd2c/uaqvt2l7anvs8fS+jrbZre2l77vP0vYy22q7tpe25z9tSX9/RWnvZXW21XdtL23Ofp+9ltNV2bS9tz32evpfRVtu1vbQ993lb6us7Wmsvu6uttmt7aXvu8/S9jLbaru2l7bnP0/cy2mq7tpe25z5vS319R2vtZXe11XZtL23PfZ6+l9FW27W9tD33efpeRltt1/bS9tznbamv72itveyuttqu7aXtuc/T9zLaaru2l7bnPk/fy2ir7dpe2p77vC319R2ttZfd1dZdWNO2tZe+2mq7tpe25z5P38toq+3aXtqe+7wt9fUdrbWX3dXWXVhzF1p72V1ttV3bS9tzn6fvZbTVdm0vbc99nr6X0Vbbtb20Pfd5W+rrO1prL7urrbZre2l77vP0vYy22q7tpe25z9P3Mtpqu7aXtuc+b0t9fUdr7WV3tdV2bS9tz32evpfRVtu1vbQ993n6XkZbbdf20vbc522pr+9orb3srrbaru2l7bnP0/cy2mq7tpe25z5P38toq+3aXtqe+7wt9fUdrbWX3dVW27W9tD33efpeRltt1/bS9tzn6XsZbbVd20vbc5+3pb6+o7X2srvaaru2l7bnPk/fy2ir7dpFfW6CQdY+npkvrP789sz801d95nQ6fX5mfmRmfuda8yzL8i+WZfn0xR//u5l5L5rlXK+aWdvH20vbO8971Uz6PvmcL2n75HO+pO2j6Gt317Tt7KXtnee9aqaN9X1o3q313Vvbmf3srrbaru2l7Z3nvWomfZ98zpe0ffI5X9L2UfS1u2vadvbS9s7zXjXTxvr6jtbay+5qq+3aXtreed6rZtL3yed8Sdsnn/MlbR9FX7u7pm1nL23vPO9VM22sr+9orb3srrbaru2l7Z3nvWomfZ98zpe0ffI5X9L2UfS1u2vadvbS9s7zXjXTxvr6jtbay+5qq+3aXtreed6rZtL3yed8Sdsnn/MlbR9FX7u7pm1nL23vPO9VM22sr+9orb3srrbaru2l7Z3nvWomfZ98zpe0ffI5X9L2UfS1u2vadvbS9s7zXjXTxvr6jtbay+5q6y6sadvaS19ttV3bS9s7z3vVTPo++Zwvafvkc76k7aPsra/vaK297K627sKau9Day+5qq+3aXtreed6rZtL3yed8Sdsnn/MlbR9FX7u7pm1nL23vPO9VM22sr+9orb3srrbaru2l7Z3nvWomfZ98zpe0ffI5X9L2UfS1u2vadvbS9s7zXjXTxvr6jtbay+5qq+3aXtreed6rZtL3yed8Sdsnn/MlbR9FX7u7pm1nL23vPO9VM22sr+9orb3srrbaru2l7Z3nvWomfZ98zpe0ffI5X9L2UfS1u2vadvbS9s7zXjXTxvr6jtbay+5qq+3aXtreed6rZtL3yed8Sdsnn/MlbR9FX7u7pm1nL23vPO9VM22sr+9orb3srrbaru2l7Z3nvWomfZ98zpe0ffI5X9pR2wfdZOP8wD+cmZ86nU5fPJ1Ob83MBzPzjXuf+cbM/MUXv/8LM/P3lmVZrjXP6XT6k6s//vzM/J/RLOf6xsz8Z6cf+NMz87vLsvyz0fYp7KXtnDOTvhfTVtu1vbSdc2bS92Laaru2l7Zzzkwb6/uqtjPb67u3tjP72V1ttV3bS9s5ZyZ9L6attmt7aTvnzKTvxbTVdm0vbeecmTbW13e01l52V1tt1/bSds6ZSd+Laavt2l7azjkz6XsxbbVd20vbOWemjfX1Ha21l93VVtu1vbSdc2bS92Laaru2l7Zzzkz6Xkxbbdf20nbOmWljfX1Ha+1ld7XVdm0vbeecmfS9mLbaru2l7Zwzk74X01bbtb20nXNm2lhf39Fae9ldbbVd20vbOWcmfS+mrbZre2k758yk78W01XZtL23nnJk21td3tNZedldbd2FN29Ze+mqr7dpe2s45M+l7MW21XdtL2zlnpo319R2ttZfd1dZdWHMXWnvZXW21XdtL2zlnJn0vpq22a3tpO+fMpO/FtNV2bS9t55yZNtbXd7TWXnZXW23X9tJ2zplJ34tpq+3aXtrOOTPpezFttV3bS9s5Z6aN9fUdrbWX3dVW27W9tJ1zZtL3Ytpqu7aXtnPOTPpeTFtt1/bSds6ZaWN9fUdr7WV3tdV2bS9t55yZ9L2Yttqu7aXtnDOTvhfTVtu1vbSdc2baWF/f0Vp72V1ttV3bS9s5ZyZ9L6attmt7aTvnzKTvxbTVdm0vbeecmTbW13e01l52V1tt1/bSds6ZSd+Laavt2kM/L7zasizpr5n58zPzf83MRzPzt178t1+amZ9/8ft/aWb+zsx8d2a+OTPvXHme/3JmvjMz35qZvz8z/3Y8z/80M/9sZn5/Zj6emb88M391Zv7qi78/zcx/82Le/2Nm3tf2zWurr93VVlt97a622u6t72PabrHvltoebXe11XaPbfW1u9pqq6/d1VbbvfV9TNst9t1S26Ptrrba7rGtvnZXW231tbvaaru3vo9pu8W+W2p7tN3VVts9ttXX7mqrrb52V1tt99b3MW232HdLbY+2u9pqu8e2+tpdbbXV1+5qq+3e+j6m7Rb7bqnt0XZXW2332FZfu6uttvraXW213Vvfx7TdYt8ttT3a7mrbvcvW+mprd7XV9sht9bW72mq79b6PabvFvltqe7Td1bZ7l631dRfsrrbaHrmtvnZXW231tbvaaru3vo9pu8W+W2p7tN3VVts9ttXX7mqrrb52V1tt99b3MW232HdLbY+2u9pqu8e2+tpdbbXV1+5qq+3e+j6m7Rb7bqnt0XZXW2332FZfu6uttvraXW213Vvfx7TdYt8ttT3a7mqr7R7b6mt3tdVWX7urrbZ76/uYtlvsu6W2R9tdbbXdY1t97a6212/70K/Ti/8xAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADt0c+0BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALjczbUHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAOByN9ceAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIDL3Vx7AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAud3PtAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC43M21BwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADgcjfXHgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAy91cewAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAALvf/A1PYWRj4SwStAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 14400x720 with 384 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(3,128, figsize = (200,10))\n",
    "dif_x = (batch_x-pert_x)*100\n",
    "dif_x\n",
    "for i in range(0,128):\n",
    "    axarr[0,i].imshow(np.squeeze(batch_x[i]), cmap='Greys_r')\n",
    "    axarr[1,i].imshow(np.squeeze(pert_x[i]), cmap='Greys_r')\n",
    "    axarr[2,i].imshow(np.squeeze(dif_x[i]), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('batch_fi.pkl','wb') as f:\n",
    "    pickle.dump(batch_x, f)\n",
    "with open('pert_fi.pkl','wb') as f:\n",
    "    pickle.dump(pert_x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(X, y, num_classes, batch_size=128, thresh=0.3, target=-1):\n",
    "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
    "    t = tf.placeholder(tf.float32, [None, num_classes]) # target placeholder\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "    is_targeted = False\n",
    "    if target in range(0, y.shape[-1]):\n",
    "        is_targeted = True\n",
    "\n",
    "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
    "    x_perturbed = perturb + x_pl\n",
    "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
    "\n",
    "    f = Target()\n",
    "    f_real_logits, f_real_probs = f.ModelC(x_pl)\n",
    "    f_fake_logits, f_fake_probs = f.ModelC(x_perturbed)\n",
    "\n",
    "    t_vars = tf.trainable_variables()\n",
    "    f_vars = [var for var in t_vars if 'ModelC' in var.name]\n",
    "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
    "    #print(f_vars)\n",
    "    #print(g_vars)\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    #f_saver = tf.train.Saver(f_vars)\n",
    "    #g_saver = tf.train.Saver(g_vars)\n",
    "    new_saver = tf.train.import_meta_graph('weights/generator/gen.ckpt.meta', clear_devices=True)\n",
    "    print(new_saver)\n",
    "    \n",
    "    '''newnew restore methods'''\n",
    "    # create saver objects for the target model, generator, and discriminator\n",
    "    new_saver = tf.train.import_meta_graph('weights/generator/gen.ckpt.meta', clear_devices=True)\n",
    "    #print(new_saver)\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    sess.run(initializer)\n",
    "    new_saver.restore(sess,'weights/generator/gen.ckpt')\n",
    "\n",
    "    rawpert, pert, fake_l, real_l = sess.run([perturb, x_perturbed, f_fake_probs, f_real_probs], \\\n",
    "                                                feed_dict={x_pl: X[:32], \\\n",
    "                                                           is_training: False})\n",
    "    print('LA: ' + str(np.argmax(y[:32], axis=1)))\n",
    "    print('OG: ' + str(np.argmax(real_l, axis=1)))\n",
    "    print('PB: ' + str(np.argmax(fake_l, axis=1)))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accs = []\n",
    "    total_batches_test = int(X.shape[0] / batch_size)\n",
    "    for i in range(total_batches_test):\n",
    "        batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
    "\n",
    "        if is_targeted:\n",
    "            targets = np.full((batch_y.shape[0],), target)\n",
    "            batch_y = np.eye(y.shape[-1])[targets]\n",
    "\n",
    "        acc, fake_l, x_pert = sess.run([accuracy, f_fake_probs, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
    "        accs.append(acc)\n",
    "\n",
    "    #print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
    "\n",
    "    f, axarr = plt.subplots(2,2)\n",
    "    axarr[0,0].imshow(np.squeeze(X[3]), cmap='Greys_r')\n",
    "    axarr[0,1].imshow(np.squeeze(pert[3]), cmap='Greys_r')\n",
    "    axarr[1,0].imshow(np.squeeze(X[4]), cmap='Greys_r')\n",
    "    axarr[1,1].imshow(np.squeeze(pert[4]), cmap='Greys_r')\n",
    "    plt.show()\n",
    "    \n",
    "    return pert, X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_per, X_a = attack(X_test[:10], y_test[:10],num_classes = 20, target=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from weights/generator/gen.ckpt\n"
     ]
    }
   ],
   "source": [
    "gen_sess = tf.Session()\n",
    "\n",
    "'''newnew restore methods'''\n",
    "# create saver objects for the target model, generator, and discriminator\n",
    "gen_saver = tf.train.import_meta_graph('weights/generator/gen.ckpt.meta', clear_devices=True)\n",
    "#print(new_saver)\n",
    "initializer = tf.global_variables_initializer()\n",
    "gen_sess.run(initializer)\n",
    "gen_saver.restore(gen_sess,'weights/generator/gen.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_attack(X,y, batch_size=128, thresh=0.4, target=-1):\n",
    "    x_pl = tf.placeholder(tf.float32, [1, X.shape[0], X.shape[1], X.shape[2]]) # image placeholder\n",
    "    t = tf.placeholder(tf.float32, [None, 20]) # target placeholder\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
    "    x_perturbed = perturb + x_pl\n",
    "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
    "\n",
    "    f = Target()\n",
    "    f_real_logits, f_real_probs = f.ModelC(x_pl)\n",
    "    f_fake_logits, f_fake_probs = f.ModelC(x_perturbed)\n",
    "\n",
    "    t_vars = tf.trainable_variables()\n",
    "    f_vars = [var for var in t_vars if 'ModelC' in var.name]\n",
    "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
    "\n",
    "    rawpert, pert, fake_l, real_l = gen_sess.run([perturb, x_perturbed, f_fake_probs, f_real_probs], \\\n",
    "                                                feed_dict={x_pl: X[np.newaxis,:], \\\n",
    "                                                           is_training: False})\n",
    "    \n",
    "    return pert, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23801A90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23801A90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23801A90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23801A90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B231F4668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B231F4668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B231F4668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B231F4668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23256550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23256550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23256550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23256550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23827940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B236C09E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B236C09E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B236C09E8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B236C09E8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B234B6978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B234B6978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B234B6978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B234B6978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B234B6978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B234B6978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B234B6978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B234B6978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2384A358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2384A358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2384A358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2384A358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B238A69B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B238A69B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B238A69B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B238A69B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B238A69B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B238A69B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B238A69B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B238A69B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B238A69B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B238A69B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B238A69B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B238A69B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B238A69B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B238A69B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B238A69B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B238A69B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23A75BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23A75BA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23A75BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23A75BA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23A75BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23A75BA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23A75BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23A75BA8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23A75BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23A75BA8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23A75BA8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23A75BA8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B238D1B00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B238D1B00>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B238D1B00>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B238D1B00>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23BCB2B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23BCB2B0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23BCB2B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23BCB2B0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'inputs')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-a192e4336830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#add = im[np.newaxis,:]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpure_attack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_per\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m640\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-d13b82e56670>\u001b[0m in \u001b[0;36mpure_attack\u001b[1;34m(X, y, batch_size, thresh, target)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mf_real_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_real_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mf_fake_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_fake_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_perturbed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-68a6fd733ab5>\u001b[0m in \u001b[0;36mModelC\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m                                 \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                                 activation=tf.nn.relu)\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             conv2 = tf.layers.Conv2D(\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         name=name, **kwargs)\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0mkernel_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     super(Layer, self).__init__(trainable=trainable, name=name, dtype=dtype,\n\u001b[1;32m--> 203\u001b[1;33m                                 **kwargs)\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_in_keras_style_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     }\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# Validate optional keyword arguments.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;31m# Mutable properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[1;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[0;32m    598\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'inputs')"
     ]
    }
   ],
   "source": [
    "im = cv2.resize(cv2.imread(\"photo.png\"),(32, 32))\n",
    "#add = im[np.newaxis,:]\n",
    "im.shape\n",
    "X_per, X_a = pure_attack(im, y, target=-1)\n",
    "out = cv2.resize(X_per.reshape(32,32,3), (640,480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B24709278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B24709278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B24709278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B24709278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022AEF09B358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022AEF09B358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022AEF09B358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022AEF09B358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B232A1DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B232A1DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B232A1DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B232A1DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B231A7E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B231A7E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B231A7E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B231A7E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B231A7E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B231A7E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B231A7E48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B231A7E48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B230B12B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B230B12B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B230B12B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B230B12B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B230B12B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B230B12B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B230B12B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B230B12B0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000022B23361EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x0000022B2325B6D8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B2325B6D8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B2325B6D8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B2325B6D8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23454B70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23454B70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23454B70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B23454B70>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B230B12B0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B230B12B0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv2DTranspose.call of <tensorflow.python.layers.convolutional.Conv2DTranspose object at 0x0000022B230B12B0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'inputs')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f5c576e914a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../07_video_test/Assets/Photo/photo.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#print(im.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mpertag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpure_attack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpertag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m640\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../07_video_test/Assets/fromML/photo.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-d13b82e56670>\u001b[0m in \u001b[0;36mpure_attack\u001b[1;34m(X, y, batch_size, thresh, target)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mf_real_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_real_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mf_fake_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_fake_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_perturbed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-68a6fd733ab5>\u001b[0m in \u001b[0;36mModelC\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m                                 \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                                 \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                                 activation=tf.nn.relu)\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             conv2 = tf.layers.Conv2D(\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         name=name, **kwargs)\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0mkernel_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, **kwargs)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     super(Layer, self).__init__(trainable=trainable, name=name, dtype=dtype,\n\u001b[1;32m--> 203\u001b[1;33m                                 **kwargs)\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_in_keras_style_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     }\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# Validate optional keyword arguments.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[0mgeneric_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;31m# Mutable properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Development_Components\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[1;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[0;32m    598\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'inputs')"
     ]
    }
   ],
   "source": [
    "flag = cv2.imread('flag.png')\n",
    "\n",
    "count = 0\n",
    "\n",
    "while flag == None and count <5:\n",
    "    im = cv2.resize(cv2.imread('../07_video_test/Assets/Photo/photo.png'),(32, 32))\n",
    "    #print(im.shape)\n",
    "    pertag, newtag = pure_attack(im, y, target=-1)\n",
    "    out = cv2.resize(pertag.reshape(32,32,3), (640,480))\n",
    "    cv2.imwrite('../07_video_test/Assets/fromML/photo.png', out)\n",
    "    flag = cv2.imread('flag.png')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-830b375610bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpure_attack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_per, X_a = pure_attack(X_test, target=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 480, 640, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = targetim[np.newaxis,:]\n",
    "a = np.tile(x,(5,1)).reshape(5,480,640,3)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 960, 640, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((x,x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
