{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Target model definitions that adverserial examples are attempting to 'fool':\n",
    "\n",
    "ref: https://arxiv.org/pdf/1801.02610.pdf\n",
    "\n",
    "Source Code on attack perfomance on MINST Challenge and Cifar10:\n",
    "\n",
    "https://github.com/ctargon/AdvGAN-tf\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os, sys\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from generator import generator\n",
    "from discriminator import discriminator\n",
    "from keras.datasets import cifar10\n",
    "from keras.datasets import cifar100\n",
    "#from target_models import Target as target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target:\n",
    "    def __init__(self, lr=0.001, epochs=5, n_input=32, n_classes=10, batch_size=16,restore=0):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.n_input = n_input\n",
    "        self.n_classes =  n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.restore = restore\n",
    "\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "    # randomly shuffle a dataset \n",
    "    def shuffle(self, X, Y):\n",
    "        rands = random.sample(range(X.shape[0]),X.shape[0])\n",
    "        return X[rands], Y[rands]\n",
    "\n",
    "    # get the next batch based on x, y, and the iteration (based on batch_size)\n",
    "    def next_batch(self, X, Y, i, batch_size):\n",
    "        idx = i * batch_size\n",
    "        idx_n = i * batch_size + batch_size\n",
    "        return X[idx:idx_n], Y[idx:idx_n]\n",
    "    \n",
    "    \n",
    "\n",
    "    # USAGE:\n",
    "    # - encoder network for vae\n",
    "    # PARAMS:\n",
    "    #x: input data sample\n",
    "    #h_hidden: LIST of num. neurons per hidden layer\n",
    "    def ModelC(self, x):\n",
    "        with tf.variable_scope('ModelC', reuse=tf.AUTO_REUSE):\n",
    "            #input_layer = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "            conv1 = tf.layers.conv2d(\n",
    "                                inputs=x,\n",
    "                                filters=32,\n",
    "                                kernel_size=3,\n",
    "                                padding=\"same\",\n",
    "                                activation=tf.nn.relu)\n",
    "            \n",
    "            conv2 = tf.layers.conv2d(\n",
    "                                inputs=conv1,\n",
    "                                filters=32,\n",
    "                                kernel_size=3,\n",
    "                                padding=\"same\",\n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "            conv3 = tf.layers.conv2d(\n",
    "                                inputs=pool1,\n",
    "                                filters=64,\n",
    "                                kernel_size=3,\n",
    "                                padding=\"same\",\n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "            conv4 = tf.layers.conv2d(\n",
    "                                inputs=conv3,\n",
    "                                filters=64,\n",
    "                                kernel_size=3,\n",
    "                                padding=\"same\",\n",
    "                                activation=tf.nn.relu)\n",
    "\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "\n",
    "            pool2_flatten = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "            fc1 = tf.layers.dense(inputs=pool2_flatten, units=200, activation=tf.nn.relu)\n",
    "\n",
    "            fc2 = tf.layers.dense(inputs=fc1, units=200, activation=tf.nn.relu)\n",
    "\n",
    "            logits = tf.layers.dense(inputs=fc2, units=self.n_classes, activation=None)\n",
    "\n",
    "            probs = tf.nn.softmax(logits)\n",
    "\n",
    "            return logits, probs\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, Y, X_test, Y_test):\n",
    "        # define placeholders for input data\n",
    "        x = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]])\n",
    "        y = tf.placeholder(tf.float32, [None, self.n_classes])\n",
    "\n",
    "        # define compute graph\n",
    "        logits, _ = self.ModelC(x)\n",
    "\n",
    "        # define cost\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "\n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(cost)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        total_batch = int(X.shape[0] / self.batch_size)\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            avg_cost = 0.\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = self.next_batch(X, Y, i, self.batch_size)\n",
    "                \n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "                avg_cost += c / total_batch\n",
    "\n",
    "            print(\"Epoch:\", '%04d' % (epoch), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        accs = []\n",
    "\n",
    "        total_test_batch = int(X_test.shape[0] / self.batch_size)\n",
    "        for i in range(total_test_batch):\n",
    "            batch_x, batch_y = self.next_batch(X_test, Y_test, i, self.batch_size)\n",
    "            #batch_x = dataset.train.permute(batch_x, idxs)\n",
    "            accs.append(accuracy.eval({x: batch_x, y: batch_y}, session=sess))\n",
    "\n",
    "        print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
    "\n",
    "        saver.save(sess, \"./weights/target_model/model.ckpt\")\n",
    "        sess.close() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.376557682\n",
      "Epoch: 0002 cost= 0.900071484\n",
      "Epoch: 0003 cost= 0.697233653\n",
      "Epoch: 0004 cost= 0.552989032\n",
      "Epoch: 0005 cost= 0.445865603\n",
      "accuracy of test set: 0.7129\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "(X,y), (X_test,y_test) = cifar10.load_data()#label_mode = 'coarse'\n",
    "X = np.divide(X, 255.0)\n",
    "X_test = np.divide(X_test, 255.0)\n",
    "X = X.reshape(X.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "y = to_categorical(y, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "cnn = Target()\n",
    "cnn.train(X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle a dataset \n",
    "def shuffle(X, Y):\n",
    "    rands = random.sample(range(X.shape[0]),X.shape[0])\n",
    "    return X[rands], Y[rands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the next batch based on x, y, and the iteration (based on batch_size)\n",
    "def next_batch(X, Y, i, batch_size):\n",
    "    idx = i * batch_size\n",
    "    idx_n = i * batch_size + batch_size\n",
    "    return X[idx:idx_n], Y[idx:idx_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_loss(preds, labels, is_targeted):\n",
    "    real = tf.reduce_sum(labels * preds, 1)\n",
    "    other = tf.reduce_max((1 - labels) * preds - (labels * 10000), 1)\n",
    "    if is_targeted:\n",
    "        return tf.reduce_sum(tf.maximum(0.0, other - real))\n",
    "    return tf.reduce_sum(tf.maximum(0.0, real - other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function to influence the perturbation to be as close to 0 as possible\n",
    "def perturb_loss(preds, thresh=0.3):\n",
    "    zeros = tf.zeros((tf.shape(preds)[0]))\n",
    "    return tf.reduce_mean(tf.maximum(zeros, tf.norm(tf.reshape(preds, (tf.shape(preds)[0], -1)), axis=1) - thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that defines ops, graphs, and training procedure for AdvGAN framework\n",
    "def AdvGAN(X, y, X_test, y_test, epochs=50, batch_size=128, target=-1):\n",
    "    # placeholder definitions\n",
    "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
    "    t = tf.placeholder(tf.float32, [None, y.shape[-1]]) # target placeholder\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "    #-----------------------------------------------------------------------------------\n",
    "    # MODEL DEFINITIONS\n",
    "    is_targeted = False\n",
    "    if target in range(0, y.shape[-1]):\n",
    "        is_targeted = True\n",
    "\n",
    "    # gather target model\n",
    "    f = Target() #target_model()\n",
    "\n",
    "    thresh = 0.3\n",
    "\n",
    "    # generate perturbation, add to original input image(s)\n",
    "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
    "    x_perturbed = perturb + x_pl\n",
    "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
    "\n",
    "    # pass real and perturbed image to discriminator and the target model\n",
    "    d_real_logits, d_real_probs = discriminator(x_pl, is_training)\n",
    "    d_fake_logits, d_fake_probs = discriminator(x_perturbed, is_training)\n",
    "\n",
    "    # pass real and perturbed images to the model we are trying to fool\n",
    "    f_real_logits, f_real_probs = f.ModelC(x_pl)\n",
    "    f_fake_logits, f_fake_probs = f.ModelC(x_perturbed)\n",
    "\n",
    "\n",
    "    # generate labels for discriminator (optionally smooth labels for stability)\n",
    "    smooth = 0.0\n",
    "    d_labels_real = tf.ones_like(d_real_probs) * (1 - smooth)\n",
    "    d_labels_fake = tf.zeros_like(d_fake_probs)\n",
    "\n",
    "    #-----------------------------------------------------------------------------------\n",
    "    # LOSS DEFINITIONS\n",
    "    # discriminator loss\n",
    "    d_loss_real = tf.losses.mean_squared_error(predictions=d_real_probs, labels=d_labels_real)\n",
    "    d_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=d_labels_fake)\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    # generator loss\n",
    "    g_loss_fake = tf.losses.mean_squared_error(predictions=d_fake_probs, labels=tf.ones_like(d_fake_probs))\n",
    "\n",
    "    # perturbation loss (minimize overall perturbation)\n",
    "    l_perturb = perturb_loss(perturb, thresh)\n",
    "\n",
    "    # adversarial loss (encourage misclassification)\n",
    "    l_adv = adv_loss(f_fake_probs, t, is_targeted)\n",
    "\n",
    "    # weights for generator loss function\n",
    "    alpha = 1.0\n",
    "    beta = 5.0\n",
    "    g_loss = l_adv + alpha*g_loss_fake + beta*l_perturb \n",
    "\n",
    "    # ----------------------------------------------------------------------------------\n",
    "    # gather variables for training/restoring\n",
    "    t_vars = tf.trainable_variables()\n",
    "    f_vars = [var for var in t_vars if 'ModelC' in var.name]\n",
    "    d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
    "\n",
    "    # define optimizers for discriminator and generator\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        d_opt = tf.train.AdamOptimizer().minimize(d_loss, var_list=d_vars)\n",
    "        g_opt = tf.train.AdamOptimizer(learning_rate=0.001).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    # create saver objects for the target model, generator, and discriminator\n",
    "    saver = tf.train.Saver(f_vars)\n",
    "    g_saver = tf.train.Saver(g_vars)\n",
    "    d_saver = tf.train.Saver(d_vars)\n",
    "\n",
    "    init  = tf.global_variables_initializer()\n",
    "\n",
    "    sess  = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # load the pretrained target model\n",
    "    try:\n",
    "        saver.restore(sess, \"./weights/target_model/model.ckpt\")\n",
    "    except:\n",
    "        print(\"make sure to train the target model first...\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    total_batches = int(X.shape[0] / batch_size)\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        X, y = shuffle(X, y)\n",
    "        loss_D_sum = 0.0\n",
    "        loss_G_fake_sum = 0.0\n",
    "        loss_perturb_sum = 0.0\n",
    "        loss_adv_sum = 0.0\n",
    "\n",
    "        for i in range(total_batches):\n",
    "\n",
    "            batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
    "\n",
    "            # if targeted, create one hot vectors of the target\n",
    "            if is_targeted:\n",
    "                targets = np.full((batch_y.shape[0],), target)\n",
    "                batch_y = np.eye(y.shape[-1])[targets]\n",
    "\n",
    "            # train the discriminator first n times\n",
    "            for _ in range(1):\n",
    "                _, loss_D_batch = sess.run([d_opt, d_loss], feed_dict={x_pl: batch_x, \\\n",
    "                                                                       is_training: True})\n",
    "\n",
    "            # train the generator n times\n",
    "            for _ in range(1):\n",
    "                _, loss_G_fake_batch, loss_adv_batch, loss_perturb_batch = \\\n",
    "                                    sess.run([g_opt, g_loss_fake, l_adv, l_perturb], \\\n",
    "                                                feed_dict={x_pl: batch_x, \\\n",
    "                                                           t: batch_y, \\\n",
    "                                                           is_training: True})\n",
    "            loss_D_sum += loss_D_batch\n",
    "            loss_G_fake_sum += loss_G_fake_batch\n",
    "            loss_perturb_sum += loss_perturb_batch\n",
    "            loss_adv_sum += loss_adv_batch\n",
    "\n",
    "        print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f, \\\n",
    "                \\nloss_perturb: %.3f, loss_adv: %.3f, \\n\" %\n",
    "                (epoch + 1, loss_D_sum/total_batches, loss_G_fake_sum/total_batches,\n",
    "                loss_perturb_sum/total_batches, loss_adv_sum/total_batches))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            g_saver.save(sess, \"weights/generator/gen.ckpt\")\n",
    "            d_saver.save(sess, \"weights/discriminator/disc.ckpt\")\n",
    "\n",
    "    # evaluate the test set\n",
    "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accs = []\n",
    "    total_batches_test = int(X_test.shape[0] / batch_size)\n",
    "    for i in range(total_batches_test):\n",
    "        batch_x, batch_y = next_batch(X_test, y_test, i, batch_size)\n",
    "        acc, x_pert = sess.run([accuracy, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
    "        accs.append(acc)\n",
    "\n",
    "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
    "\n",
    "    # plot some images and their perturbed counterparts\n",
    "    f, axarr = plt.subplots(2,2)\n",
    "    axarr[0,0].imshow(np.squeeze(batch_x[2]), cmap='Greys_r')\n",
    "    axarr[0,1].imshow(np.squeeze(x_pert[2]), cmap='Greys_r')\n",
    "    axarr[1,0].imshow(np.squeeze(batch_x[5]), cmap='Greys_r')\n",
    "    axarr[1,1].imshow(np.squeeze(x_pert[5]), cmap='Greys_r')\n",
    "    plt.show()\n",
    "\n",
    "    print('finished training, saving weights')\n",
    "    return batch_x, x_pert\n",
    "    #g_saver.save(sess, \"weights/generator/gen.ckpt\")\n",
    "    #d_saver.save(sess, \"weights/discriminator/disc.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''thresh adjusted from 0.3 to 0.2'''\n",
    "def attack(X, y, batch_size=64, thresh=0.3, target=-1):\n",
    "    x_pl = tf.placeholder(tf.float32, [None, X.shape[1], X.shape[2], X.shape[3]]) # image placeholder\n",
    "    t = tf.placeholder(tf.float32, [None, 10]) # target placeholder\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "    is_targeted = False\n",
    "    if target in range(0, y.shape[-1]):\n",
    "        is_targeted = True\n",
    "\n",
    "    perturb = tf.clip_by_value(generator(x_pl, is_training), -thresh, thresh)\n",
    "    x_perturbed = perturb + x_pl\n",
    "    x_perturbed = tf.clip_by_value(x_perturbed, 0, 1)\n",
    "\n",
    "    f = Target()\n",
    "    f_real_logits, f_real_probs = f.ModelC(x_pl)\n",
    "    f_fake_logits, f_fake_probs = f.ModelC(x_perturbed)\n",
    "\n",
    "    t_vars = tf.trainable_variables()\n",
    "    f_vars = [var for var in t_vars if 'ModelC' in var.name]\n",
    "    g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='g_weights')\n",
    "\n",
    "    sess = tf.Session()\n",
    "\n",
    "    f_saver = tf.train.Saver(f_vars)\n",
    "    g_saver = tf.train.Saver(g_vars)\n",
    "    f_saver.restore(sess, \"./weights/target_model/model.ckpt\")\n",
    "    g_saver.restore(sess, \"./weights/generator/gen.ckpt\")\n",
    "    #g_saver.restore(sess, tf.train.latest_checkpoint(\"./weights/generator/\"))\n",
    "\n",
    "    rawpert, pert, fake_l, real_l = sess.run([perturb, x_perturbed, f_fake_probs, f_real_probs], \\\n",
    "                                                feed_dict={x_pl: X[:32], \\\n",
    "                                                           is_training: False})\n",
    "    print('LA: ' + str(np.argmax(y[:32], axis=1)))\n",
    "    print('OG: ' + str(np.argmax(real_l, axis=1)))\n",
    "    print('PB: ' + str(np.argmax(fake_l, axis=1)))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(f_fake_probs, 1), tf.argmax(t, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    accs = []\n",
    "    total_batches_test = int(X.shape[0] / batch_size)\n",
    "    for i in range(total_batches_test):\n",
    "        batch_x, batch_y = next_batch(X, y, i, batch_size)\n",
    "\n",
    "        if is_targeted:\n",
    "            targets = np.full((batch_y.shape[0],), target)\n",
    "            batch_y = np.eye(y.shape[-1])[targets]\n",
    "\n",
    "        acc, fake_l, x_pert = sess.run([accuracy, f_fake_probs, x_perturbed], feed_dict={x_pl: batch_x, t: batch_y, is_training: False})\n",
    "        accs.append(acc)\n",
    "\n",
    "    print('accuracy of test set: {}'.format(sum(accs) / len(accs)))\n",
    "\n",
    "    f, axarr = plt.subplots(2,2)\n",
    "    axarr[0,0].imshow(np.squeeze(X[3]), cmap='Greys_r')\n",
    "    axarr[0,1].imshow(np.squeeze(pert[3]), cmap='Greys_r')\n",
    "    axarr[1,0].imshow(np.squeeze(X[4]), cmap='Greys_r')\n",
    "    axarr[1,1].imshow(np.squeeze(pert[4]), cmap='Greys_r')\n",
    "    plt.show()\n",
    "    \n",
    "    g_saver.save(sess, \"weights/generator/gen.ckpt\")\n",
    "    d_saver.save(sess, \"weights/discriminator/disc.ckpt\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pert,X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv2d_15 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [2, None, 32, 32, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-cea1ae574d5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpert_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdvGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-11bbc5a05f36>\u001b[0m in \u001b[0;36mAdvGAN\u001b[1;34m(X, y, X_test, y_test, epochs, batch_size, target)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# pass real and perturbed image to discriminator and the target model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0md_real_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_real_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0md_fake_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_fake_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_perturbed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# pass real and perturbed images to the model we are trying to fool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - Harvard University\\GitHub\\SCI-6338\\JustGAN\\discriminator.py\u001b[0m in \u001b[0;36mdiscriminator\u001b[1;34m(x, training)\u001b[0m\n\u001b[0;32m     17\u001b[0m                                                         \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                                         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"valid\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \t\t\t\t\t\t\tactivation=None)\n\u001b[0m\u001b[0;32m     20\u001b[0m                 \u001b[0mconv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[0;32m    422\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m       _scope=name)\n\u001b[1;32m--> 424\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1698\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m     \"\"\"\n\u001b[1;32m-> 1700\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[1;31m# are casted, not before.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[1;32m--> 819\u001b[1;33m                                               self.name)\n\u001b[0m\u001b[0;32m    820\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    175\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                          str(x.shape.as_list()))\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer conv2d_15 is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [2, None, 32, 32, 3]"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# read in mnist data\n",
    "#(X,y), (X_test,y_test) = mnist.load_data()\n",
    "(X, y), (X_test, y_test) = cifar10.load_data() #cifar100.load_data(label_mode='coarse')#(label_mode='coarse')\n",
    "X = np.divide(X, 255.0)\n",
    "X_test = np.divide(X_test, 255.0)\n",
    "X = X.reshape(X.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "y = to_categorical(y, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "batch_x, pert_x = AdvGAN(X, y, X_test, y_test, batch_size=32, epochs=5, target=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(3,50, figsize = (100,10))\n",
    "dif_x = (batch_x-pert_x)*100\n",
    "dif_x\n",
    "for i in range(0,50):\n",
    "    axarr[0,i].imshow(np.squeeze(batch_x[i]), cmap='Greys_r')\n",
    "    axarr[1,i].imshow(np.squeeze(pert_x[i]), cmap='Greys_r')\n",
    "    axarr[2,i].imshow(np.squeeze(dif_x[i]), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer cifar 10 attack to mobile net model\n",
    "def dataset():\n",
    "    images = []\n",
    "    labels = []\n",
    "    labels_dis = {}\n",
    "    people = [person for person in os.listdir(\"people/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_per, X_a = attack(X_test, y_test, target=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'os' from 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\os.py'>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pert.pkl','wb') as f:\n",
    "    pickle.dump(pert_x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Target at 0x237f43abba8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Target.ModelC of <__main__.Target object at 0x000002379CAF2438>>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.ModelC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
